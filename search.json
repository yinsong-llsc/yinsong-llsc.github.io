[{"title":"园林与草木溯游","path":"/2023/10/28/园林与草木溯游/","content":"记圆明园与植物园之行。 01 地图上看圆明园占地面积极广，虽然早在历史书上得知八国联军侵华火烧圆明园并尽数劫掠了所藏珍宝。但在真正走进，才知其荒凉残坯之状，地面建筑营造早已不存，只见一片荒丘离草。 进入园区看见一片荷池，开始第一次试镜，调了很久的光，终于将这株荷花对焦上。 王莲，形似一个巨大的盆子。 拍下这张照片的时候，想起来“月上柳梢头，人约黄昏后”又或者是“独立小桥风满袖，平林新月人归后”的画面。 紫薇，在家乡行道树种满了紫薇，花瓣颜色或深红、浅红、荼白、浅紫、淡蓝，树干无皮，怕痒。 水中树影自婆娑 与某人遇见，共行一路，各自形单，却为这一片热闹的观鱼所而共同驻足。 伤心桥下春波绿，曾是惊鸿照影来 玉簪花 02 圆明园中的莲花基地，里面种了许多品种的荷花，此刻开的正好，因此聚集了许多拍照的大爷大妈。 小伙子，你看，这是不是并蒂莲？我过去一看，真是并蒂莲。 想来花开并蒂并不常见，人生得遇知心人同样如此吧。 荷花又名莲花、芙蕖、菡萏，品种之众又有各种叫法。古人咏荷，如“接天莲叶无穷碧，映日荷花别样红”、“菡萏香消翠叶残，西风愁起绿波间”、“秋阴不散霜飞晚，留得残荷听雨声”、“藕风轻，莲露冷，断虹收，正红窗，初上帘钩，田田翠盖，趁斜阳鱼浪香浮”都是我喜欢的句子。 “并蒂不曾有，花期同，一笑一忘我” 03 “满纸荒唐言，一把辛酸泪，都云作者痴，谁解其中味” 曹雪芹在经历抄家之后，辗转来到北京西郊，开始著书黄叶村，披阅《风月宝鉴》，至今香山脚下植物园内有其纪念馆。 我是极爱红楼梦一书的，第一次看到书里的文字是在人教版高二的语文课本上，是《林黛玉进贾府》，看罢十分喜欢，书里描绘的情节与画面，那样美好的文字，读了叫人欣喜不已，后面便去买了全本，在高二的那个暑假通读完了一遍，之后也看了87版的影视，白先勇和蒋勋的讲述，特别是蒋勋老师的讲述，使我对生命与世情有了更深的体察与醒悟。自此红楼一书成为我生命里无法忘怀而又十分隐秘的慰藉。 来北京后第一次看见山楂树。 04 出了曹雪芹纪念馆后，一路去了卧佛寺、集秀园、木栈道、元宝石…… 看景亦看行人，大抵是沉浸于自然风光中而忘却自身的存在，看着往来的行人，快乐着他们的快乐。 一个小孩子给他妈妈拍照，就在这扇圆形门前。等他们拍完，我也拍了一张，很喜欢这样的圆门，总觉得人生少有圆满却总希望有所圆满。 你数一下树叶，一、二……七，七片，你知道这棵树就叫七叶树吗？ 过了山门，但没进去寺庙。 古木长新芽 枯木一枝与满山青翠 行到水穷处，坐看云起时 元宝石","categories":["山水行"]},{"title":"事如春梦了无痕","path":"/2023/10/28/事如春梦了无痕/","content":"“ 时间来到2022年的夏天，这个夏天初来时便热意逼人，在五月疫情肆虐的时候不出门到现在已有两月余，许多妄想的计划在现实中逐一落空，只听着此刻窗外的蝉鸣，回想起过去的这个春天，也是在北京的第一个春天，看过落雪与春花……” 春雪 .responsive-iframe { width: 100%; height: auto; } function resizeIframe() { var iframe = document.querySelector(\".responsive-iframe\"); var container = document.getElementById(\"iframe-container\"); var containerWidth = container.offsetWidth; var newHeight = (containerWidth / 16) * 9; // 根据宽度计算高度，这里假设宽高比为16:9 iframe.style.height = newHeight + \"px\"; } window.addEventListener(\"resize\", resizeIframe); resizeIframe(); 雪里始知春信至，山桃花开的时候迎来一场春雪。 年初的时候，那会儿过完年回到北京便赶上了一场大风雪，趁着落雪，去了颐和园，出门才一会儿便披了一身的雪。 到了颐和园东门，看见来来往往的人群，刚出门的那种清冷之感才忽然没有了，心里想着总有那么多和你一样的人，赶着风雪未歇的此刻来到这处精致的皇家园林一览雪中园景，于是看雪的心情便是热闹而欢快的了。 白皮松同样覆了一身的雪，在这处红木青砖的院落前。 我与许多人一样，信步穿行于馆苑间的廊檐下，驻看飞雪，俨然间只觉上下俱白，琉璃世界惟余池馆苍台。 喜欢这雪，飘在朱红漆木的屋前，朱红之热烈与白雪之清冷本是相反却也极熨帖的事物，一如孤僻少言的人会很容易喜欢上活泼热闹的。 谐趣园中的雪景。谐趣园在我觉着是一处构筑极为精美的院子，遥想古时的富贵人家，居住在这样集自然与人文美学于一体的园林中，锦衣玉食，事无巨细有婢女照顾，众人簇拥，会不会少有忧愁。 树枝上也缀满了一片晶莹雪，恍若人间仙境。 春花 在北京的第一个春天，向来喜欢植物花卉的我必然是要去领略一番京城的花事，早闻说玉渊潭的樱花、海棠花溪、故宫玉兰最负盛名，于是早早便定下了后续的看花行程，且所幸那时疫情还没有出现，使我得以一览京中春光。 园林博览园，去的时候花还未开，隐隐已有几枝向阳的山桃长出来花苞。 南植里的山茱萸。 去了圆明园，园子里山坡上、小河边开满了山桃花，白的似雪，红的如霞，每一棵桃花树下，满是前来拍照的人们。我行在其中，虽无缘再见当时的精致建筑，但偌大的园子，山石草木湖泊小径犹在，透过那些雅致的题名，可想见其当初的繁华与美丽。 小区里头的樱花 海棠花溪，种了很多品种的海棠，在此之前只知海棠有垂丝、西府、贴梗、木瓜之属。 去了玉渊潭，因为某些缘故去的很晚，樱花已经落了，大概也是我与她无缘，一如你遇见某个人，你以为会发生很美丽的故事，于是满心期盼，盼到没有后续便已收尾。","categories":["山水行"]},{"title":"与我有缘的花木","path":"/2023/10/28/与我有缘的花木/","content":"合欢 合欢，多么美好的名字呀，此花又名绒花树，豆科落叶乔木，夏季开花。你看她那绯红的羽扇般的花束，尽显轻盈灵动之态。叶子也是细细碎碎如含羞草的叶子，你若触上去她必然害羞的合拢。在家乡居住的小区里，便有好几棵高大的合欢花树，上面的图片便是那时拍的，还剪辑了一个视频，在bilibili发过，是阅读点赞评论最多的一条视频了，大概大家都喜欢合欢花，有过初遇此花时的欣喜。 纳兰容若说：“不见合欢花，空倚相思树”，是说不能见到心上人，心底满是无尽的相思。李渔说：“凡见此花者，无不解愠成欢，破涕为笑”，于是合欢被视为夫妻和睦、家庭幸福的象征，我喜欢合欢，爱其粉红轻盈之态，爱其雨后芳香之气，也向往此花寄托的美好情思。 曼珠沙华 凤岗河湿地公园 曼珠沙华，又名彼岸花，石蒜科多年生草本植物。相传此花开遍黄泉路上、忘川河畔、三生石旁，转世投胎之人路遇此花，可心生慰藉。且此花开时不见叶，花落之后才长出叶子，因此“花叶永不相见”，被赋予一种悲情意味。 我曾痴心种过此花，悉心呵护，两年来只见她如蒜苗一般的叶子葱葱郁郁又逐渐枯萎，复而又重新长出，但始终没能开花。后面在我本科毕业之后送给了一个朋友让她继续养着，可惜也是见叶不见花。 后来，是在2019年的夏天，在即将离家去往大连上学的前日，我去了离家不远常去的凤岗河湿地公园，只为离家远行前再看一眼我心心念念的事物，便惊喜的第一次瞧见曼珠沙华真真实实的冒出在那片草地上，亦不知是哪位有心人在此栽种，在发现一株后随即往旁边又看了，二、三、四……有好几株呢。忽然想到曾经种此花两载，不曾守得花开，今日遭逢，始得圆满，始知冥冥之中的因缘与眷顾。 还有一种叫做忽地笑的花儿，形状很像曼珠沙华，但是颜色是黄色的，在北京植物园的曹雪芹纪念馆有幸看过，只不如曼珠沙华那般焰烈的红。 紫茉莉 紫茉莉，紫茉莉科一年生草本植物。与此花结识，源于儿时乡居邻家门前种有此花，在夏天每天太阳落山的时候，紫茉莉的花儿便在一丝丝微风的吹拂下慢慢展开，至次日早晨谢去，花谢后会留下黑色的种子，于是我便拾得种子播于自家庭院，之后此花便也年年开在我家的院子。 紫茉莉花期虽然只有一个傍晚，但是开花极为频繁，且可持续一个漫长的暑假。于是一整个夏天，每天傍晚蹲在这儿看茉莉花开成了我的日常，也因此看到一朵花从花苞到盛开的过程。紫茉莉花不需要蜜蜂授粉，各自小昆虫也不近她的身，倒也落得素洁自在。 紫花地丁 紫花地丁，堇菜科多年生宿根草本植物。第一次见到此花，是逛了同学的校园她发现并指给我看的，小小的一株，花茎顶部低头而开，别有一番意味。在形色上查得名为“紫花地丁”，这名字倒也别致，是说的这花是园丁么，且看她的花形实在像一个“丁”字。 后面在自己的学校，在老家早春的地头边，数次遇见紫花地丁，甚至于在大连，在学校的草地上也发现了她的身影，当真是在我的生命中挥之不去了。 夹竹桃 夹竹桃，夹竹桃科常绿灌木。花期在夏天，花瓣有红白色。在老家的梦湖边，绿荫道旁，种满了夹竹桃。每每彳亍在那条林荫绿道上，便看见夹竹桃在烈日下的身影。 最早看见夹竹桃的花，是在上海嘉定母亲寄居的住处，露天的水池边，有一棵高大的夹竹桃，墨绿色的叶子间开着这样粉红色的花儿，那时只记住了花的形状却不知其名。 荷花 荷花，莲科多年生水生草本花卉，亦称莲花。古往今来，世人爱之，又有“水芙蓉、芙蕖、菡萏”之谓。我也喜欢莲花，爱其“出淤泥而不染，濯清涟而不妖，香远益清，亭亭净植，可远观而不可亵玩焉”。田田碧叶，好似擎雨之盖；蔓蔓花葶，宝藏清芬之雅质。 在我生长的乡村，便有荷塘，使我在很小的时候便见到荷花，那时觉得此花虽美，更多却是贪吃莲蓬中的莲子。后来在本科学校，也有一方荷池，荷花开的时候，我喜欢漫步在池塘边，在那个因为考研留校的夏天，晚饭后伫立在荷塘边看荷花成了我的日常，我看到了“小荷才露尖尖角，早有蜻蜓立上头”，看到了香樟树的浆果不时坠落水中点染成一圈一圈的涟漪，看到了花瓣浮于水面惹得鱼儿前来触碰，也喜欢滴水在荷叶上让它翻滚成“大珠小珠落玉盘”之状。同样的在研究生的校园，也有一片荷塘，多满足我啊。 在家的时候，凤岗河湿地公园、梦湖中的梦岛是我常去看荷花的地方。在北京的圆明园，也有一处看荷花的所在，虽然面积不大，但却是种满各式品种的荷花，叫法也是擅尽风雅，我和许多前来游玩的人给荷花拍写真，忽然有个大叔和我说话，“小伙子，你看这是不是并蒂莲？” 我凑近一看，哇真的是并蒂莲，并蒂莲可不常见啊！","categories":["花木记"]},{"title":"与君一醉一陶然","path":"/2023/10/28/与君一醉一陶然/","content":"那日早晨下了许多雨，睡梦中醒来听见窗户哗啦不绝于耳的雨声，依旧合眼半寐，继而雨声不再，天色明朗，复起身梳洗食讫，赴一场雨中陶然亭之行。 【一】 最早知道陶然亭之名，该是高中课本《故都的秋》所述“陶然亭的菊花”，后看《觉醒时代》，也知大风雪中酒炉正沸，有志之士曾在此为国家前途慷慨陈述。我赴陶然亭，无邀亦无会，但慕前人事迹与此亭陶然之风。 我是从北门入园的，第一眼看见水面便遇见此亭，知津？我不知。 芦花，繁繁匝匝的于水湄边铺成一片碧帐，等到秋天，芦花白茫茫，便是极美的意境了。 树木里最爱垂柳的枝条，花木里最爱荷花的清远 柳浪宜亭，记得扬州瘦西湖的五亭桥也是这般四面开圆门的形状。 陶然亭，故事发生的主角出现了，是一处有中庭信步有轩窗临景的所在。你看那个亭字，是不是很像亭子的模样？ “烟藏古寺无人到，榻倚深堂有月来” 古寺即是慈悲庵了，慈悲二字，读来便生虔诚欢喜心，人生于世，若对一切卵生、胎生、湿生、化生之物心生慈悲，该能保持此刻心下的清明吧。 紫迷魅啊！真喜欢这样紫色的珠子，后来回去查了一下，名字就叫紫珠。 雪松荫蔽下的玉簪花，在一片深碧中如积雪皎白。 不一样的海棠果 【二】 在陶然亭，另有一处中华名亭园，如琅琊山之醉翁亭、岳麓山之爱晚亭、杭州西湖之湖心亭，以及茶圣陆羽于惠山的品泉亭和王羲之流觞曲水的兰亭，在此亦有构筑。使我不必奔走四方，而得览诸亭之妙。 爱晚二字，最是极好。人皆喜早，我独爱晚。如晚照、晚花，正如有人爱春日之生机盎然，有人爱秋日之晚景萧疏。 天下第二泉者，乃无锡惠山泉 兰亭，在会稽山阴道上，尝读张岱《琅嬛文集》云“王右军卜居兹土，于千岩万壑中，独取兰亭一席地。其景物风华，定当妙绝千古。且余少时见兰亭墨刻，岩峦奇峭，亭榭巍峨，曲水流觞，浴鹅涤砚。开卷视之，不禁神往“ 流觞曲水 以前读《醉翁亭记》，最喜欢里面“觥筹交错，众宾欢乐”的样子，后来更喜欢“夕阳在山，人影散乱”的状态，如同生命在所有的热闹散尽，你会看到那个最落寞清醒的自己。 忽然天色暗淡，下起雨来，游人开始散去，我带了伞，并不慌忙，雨中游园想是难得。路过沧浪亭、杜甫草堂（皆仿建，原址在苏州及成都浣花溪），里面有避雨的游人。 雨中观瀑布","categories":["山水行"]},{"title":"看见海","path":"/2023/10/28/看见海/","content":"彼时只是一个生于内陆的孩童，第一次知道海也许是二年级人教版语文课本里的一首诗，每次讲诗，并不解其意，只是读起来朗朗上口，于是老师让我们读起来，是王之涣的《登鹳雀楼》： 白日依山尽，黄河入海流，欲穷千里目，更上一层楼。 印象中是第一次听闻海，知道了海大概是一片茫无际涯的大水。后来四年级的时候，学了巴金的《海上日出》，而且课文底页是附图的，对海就十分有了好感，再后来，初一下学期学到林海音的《爸爸的花儿落了》，是夹竹桃，后面看见夹竹桃总要想起来书中的主人公英子，那个因为下雨天赖床不起然后被爸爸拿起鸡毛掸子打起被宋妈抱上洋车的英子，校园里早读书声琅琅，窗外玉簪花在雨中莹润饱满。后面夹竹桃落了，爸爸的病愈加严重，那个齐肩发的女孩英子在台阶上泣不成声的落泪，而此刻弟弟妹妹还在打闹玩耍，她长大了，在校园礼堂为她们六年级毕业生唱完“长亭外，古道边，芳草碧连天”后，然后是一个小小的大人了，也曾在六年级国文课学到一篇文章《我们看海去》： 我们看海去！ 我们看海去！ 蓝色的大海上， 扬着白色的帆。 金红的太阳， 从海上升起来， 照到海面照到船头。 我们看海去！ 我们看海去！ 而萌生出强烈的想去看海的心愿。 后面，看的书越来越多，知道曹孟德观沧海后发出了“日月之行，若出其中。星汉灿烂，若出其里”的感叹。也知《庄子》里的河伯，在秋天百川灌河的时候以为天下之美尽在己，直至一路东出，得见大海，才兴叹见笑于大方之家…… 不知不觉间我的生命走过二十载，有那么一日忽然就来到了巍巍中华大地的黄渤海边，在一个叫大连的城市的星海广场，第一次看见了茫茫大海，碧波汹涌，水天相接。是一个阳光温暖的晴天，广场上聚满了人，一大群海鸥在投食的人群上空展翅翱翔。 那一刻，看见了地上自己短小的影子，看见了一片汪洋，看见了让人忘机的鸥鸟，与人群和谐的共处，那一刻的惊心，只呆呆的看着它们从身边掠过或停下，欢喜到不知身在何方。我心亦如这鸥鸟，如这碧波，荡荡悠悠不知斯生方长。 【一】 研究生的两年，在大工，在大连，一座海滨城市，看见海的时日便多起来。 在中秋节后的一个周末，和一个巧合遇见同爱摄影的朋友去了海之韵和棒棰岛，沿着山海步道从容前行，路边盛开着紫薇花。 想起来杜甫的一句诗：飘飘何所似，天地一沙鸥 在棒棰岛景区里徘徊许久，我们在路边坐着长谈，去看了古莲子长出的荷花，以及偶然一抬头望见梧桐树上的木板屋，不知是哪只鸟儿的家。 在路边板凳休息的时候，隔篱看见与花浑然一色的蝴蝶。 坐着海边看波浪不断的涌上来，有一个小孩子在那连连舀水。 【二】 大连的西北面则是渤海，原是比较偏僻的，只因去医院看病的缘故，离得比较近，就一个人步行到了渤海边，后面听人说去过夏家河子那冬天温度低的时候海面会结冰。 那边有一片礁石地，礁石地里有一个老妇人在捡着什么，我也在找寻着宝物，贝壳、好看的石头又或者幸运的看见海底冲上来的珍珠。 正是夕阳落山的时候，紫红色的天空倒映在海滩，海滩上静静的却有一人，水面也映着她的影子。 旁边是一座妈祖阁，那个福建莆田传说为庇佑出海打渔的人们而不幸淹入大海的林默娘，后来成为了沿海人们信奉的妈祖。 甚是喜欢这一从芦花，古人说“白马入芦花”，又有“芦花深处泊孤舟”。 【三】 金石滩边，赶上学期末，课程考试皆已结束，在寒假归家前来到金石滩边，微博上认识一位摄影博主便是这金石滩边鲁迅美术学院的，给我看过她拍的大雪时的海边。 沙石间清流汩汩，忽然从泥沙里蹦出来一只小螃蟹。 那时，看见了最蔚蓝的海，海面泊着无人的孤舟，有那么一刻，想着乘上一叶扁舟，然后江海寄余生。 那个午后，寂寂无人的海边，一个人呆坐了很久，和一个朋友说起这海。 【四】 去看海上落日，我是一个对落日十分痴迷的人，喜欢晚照、喜欢一切苍凉古旧的事物，我不能抑制心底那种与生俱来的悲凉与宿命之感。 藏人对神山圣水的敬重与朝谒是我十分崇敬的，于是，在海边捡石砾砌了一个玛尼堆。 不知怎的，蓦然垂下泪来，许是那日入夜海风太凉，这样的画面在看过的一部电影里也有这一幕，只是剧中人看的是繁星满天。 【五】 真正去看一场落日，是一个深秋的时节，我亦生于深秋。靠着栏杆，看着夕阳一点点的落下去，直至月色皎洁。 最爱这一缕夕阳照在建筑物是赭红色的光影 夕阳开始落山 渐沉 没入 余霞 新月皎洁 【六】 总不肯辜负这秋天，特别是在这北国，秋天一过，草木皆枯零，再难看到一点绿色。于是一个人去了山里，出山的时候又望见了海。 依山眺海 被海浪冲上来的海星","categories":["山水行"]},{"title":"暮春游富春江","path":"/2023/10/28/暮春游富春江/","content":"忽想暮春某日，与友自桐庐览富春江，吾辈纵舟江上，任之东西，一路落英缤纷，鸟声上下。抬眼处千峰万壑，碧色涟涟，花树自芳菲。时有烟村四五家，柴门小径，令人有隐逸之想。 吾与友相坐船尾，倏忽游鱼跃出水面，又潜入水中与吾舟并游，余大喜指之告友曰:“此鱼之乐也。”友欣然笑曰:“昔日庄子与惠子游于濠梁之上，见鱼儿悠然游弋，有此一对，今已千年，物是人非，而情之相似若此。”余欣然其言，肃然端坐取所藏素琴援琴应之。 日中，至乌龙，为兰江与新安江并流处，水湍清澈，山色竞秀，中有高峰一簇，丘峦环之。至其下，吾与友弃舟访岸，着屐而上。山石细碎，萦纡环回。半途歇于桐花树下，桐花扑嗒落肩上，叫人一怔。远处双鸟飞来，相随起落枝头，令人往羡。友自唱清歌，声环万籁。余执朱笔于一青岩处书曰: 山川静毓，行歌相答。 稍留片刻，复行，不觉至峰顶，眼界始阔，苍苍莽莽，榛榛葱葱，无穷无极。余二人渺立，呼啸山风。当是时，振臂一呼，仿佛纵身一赴，便染得，千寻碧。世间万种羁縻何所谓也，旋即泪下，是无由，也无因。","categories":["山水行"]},{"title":"会爱上桂花飘香的小城","path":"/2023/10/15/会爱上桂花飘香的小城/","content":"秋天有阳光的晴天，喜欢去居处附近的河边树林漫步。正是丹桂飘香的八月，抚州小城里的行道树、公园、小区里最不乏桂花树的身影。橙黄色的金桂香气最浓，直入肺腑。皎白色的淡些，然叶子更显碧绿。 柳永《望海潮》写的“三秋桂子，十里荷花”属实南国夏秋之际的盛景，我亦爱极。时常这个时候会想归老此间，恬淡安然。现实之中却是不可，只作片刻遐想。 喜欢一棵花枝缀满，体态丰盈的树，在它成长的岁月里，阴晴雨雪，开花结果。在年复一年的某个时刻，我殷勤探看，与这方花木两相伴。我知我与这山川卉木有着长长久久旦暮不歇的相思。万古江河、一夕风月，朝往北海，暮至苍梧。 人世多喧阗，若是有些会心好友，共赴良辰佳景，美食清欢，可多些欢乐。若不得，与这天地万物变化一道，顺应枯荣，励志进取，也很不错。 木芙蓉花谢了，未尝不比开的时候好看。","categories":["花木记"]},{"title":"华南秋树图鉴","path":"/2023/10/15/华南秋树图鉴/","content":"秋分之后，昼夜平分，夜晚凉气侵袭，渐露寒意，故物值于秋，如人生半百，经春萌发，历夏郁长，秋显其斑斓丰稔矣！ 桂花 每年中秋国庆时分，南方桂花逐渐步入佳境，树树小花盈盈缀满，几十步之外，便闻得甜香扑鼻，有橙黄、深红、乳白色之分，其中最香者，非深红色金桂莫属。每年回家，必漫步桂花树下，流连不去，真是坐卧桂花树下，得其所也。 木芙蓉 木芙蓉花，值秋而放。花未开时如棉桃，开时则流烟散霞，粉白可爱。红楼梦中，晴雯殁而化木芙蓉花神，宝玉作《芙蓉女儿诔》吊之，可谓 “公子情深，女儿命薄”，回想往日，撕扇为一乐，病补雀金裘，如今人走茶凉，真是“汝南泪血，斑斑洒向西风；梓泽馀衷，默默诉凭冷月。” 然余观此花，多有秋花不多，此开盛开的欣喜相逢，也有少时田间收稻谷时，遥见河边一丛丛木芙蓉花的童年回忆。 残荷 荷花至秋，渐渐萎落，荷叶也转为墨绿色，在阳光和冷雨之中枯黄，李义山说：“留得残荷听雨声”，点点滴滴，这次第，可以是愁人心上秋，也可以是岑寂心悄悄。那水中央的莲子，无人摘取，成为一擎独立的秋，待落入水中，来年发芽生根，又是一片翠绿荷叶。 乌桕 乌桕，在我们老家的小溪流旁边，是我最早看见乌桕树的地方。这种树的叶子夏天可招一种青色的扁虫，一被咬上便是刺痛的红痒，我们小孩子对其敬而远之。不过在秋天的时候，叶子红了，却极为好看，是南方秋日里难得的好看落叶，杜牧的“霜叶红于二月花”，少不了乌桕的红。 栾树 栾树，作为园林绿化树种南北随处可见，到了秋天，栾树挂满了灯笼一样的蒴果，在阳光下渐渐变红，极为好看，而且很神奇的是，它们的灯笼挂在树枝头，且不随叶子附生，这样果实更为显眼，观赏性极强。在北京、南昌、抚州，栾树毫不吝啬的挂满一树灯笼。 合欢 合欢，豆科合欢属落叶乔木，细碎的叶子在夏天会开满羽扇般红色的绒花，好看极了，夏天的傍晚，树下经过，可以闻到入肺的清香。这次很不错看到它的果实，证实豆科植物无疑了。 曼殊沙华 啥？这是曼殊沙华？对！没错哈哈！也许你见过她开花的时候，一茎花束，没有叶子，红的夺目。但是它不开花的时候，就是长着这样的叶子了，很有幸之前同样在这片土地看到它开花的样子。一种植物，两种形态，花开不见叶，长叶无有花，也是很神秘了。 葱兰 黄白色的搭配，有没有觉得很美？这些小葱般的植物，开出来的花就叫葱兰，在公园的草坪上，或大树的树下，不乏它们的身影。 革叶槭 远看以为是树上站满了小飞虫，这是一种鳞片叶，所以很像是羽翅。北京种的元宝枫长的果实也是这种，第一次在南方的公园看到，给大家科普一下。","categories":["花木记"]},{"title":"SQL语法基础","path":"/2023/10/14/SQL语法基础/","content":"SQL语法基础练习与掌握 1、一条 SQL 查询语句的执行顺序？ FROM：对 FROM 子句中的左表和右表执行笛卡儿积（Cartesianproduct），产生虚拟表 VT1。 ON：对虚拟表 VT1 应用 ON 筛选，只有那些符合的行才被插入虚拟表 VT2 中。 JOIN：如果指定了 OUTER JOIN（如 LEFT OUTER JOIN、RIGHT OUTER JOIN），那么保留表中未匹配的行作为外部行添加到虚拟表 VT2 中，产生虚拟表 VT3。如果 FROM 子句包含两个以上表，则对上一个连接生成的结果表 VT3 和下一个表重复执行步骤 1）～步骤 3），直到处理完所有的表为止。 WHERE：对虚拟表 VT3 应用 WHERE 过滤条件，只有符合的记录才被插入虚拟表 VT4 中 GROUP BY：根据 GROUP BY 子句中的列，对 VT4 中的记录进行分组操作，产生 VT5。 CUBE|ROLLUP：对表 VT5 进行 CUBE 或 ROLLUP 操作，产生表 VT6。 HAVING：对虚拟表 VT6 应用 HAVING 过滤器，只有符合的记录才被插入虚拟表 VT7 中。 SELECT：第二次执行 SELECT 操作，选择指定的列，插入到虚拟表 VT8 中。 DISTINCT：去除重复数据，产生虚拟表 VT9。 ORDER BY：将虚拟表 VT9 中的记录按照进行排序操作，产生虚拟表 VT10。 LIMIT：取出指定行的记录，产生虚拟表 VT11，并返回给查询用户。 2、检索数据（1）检索并列出所有已订购商品（prod_id）的去重后的清单。 12SELECT DISTINCT prod_idFROM OrderItems （2）从 Orders 表中检索顾客 ID（cust_id）和订单号（order_num），并先按顾客 ID 对结果进行排序，再按订单日期倒序排列。 12345# 根据列名排序SELECT cust_id, order_numFROM OrdersORDER BY cust_id,order_date DESC# order by 对多列排序的时候，先排序的列放前面，后排序的列放后面。 （3）显示 OrderItems 表中的数量（quantity）和价格（item_price），并按数量由多到少、价格由高到低排序。 1234SELECT quantity, item_priceFROM OrderItemsORDER BY quantity DESC,item_price DESC 3、过滤数据WHERE 可以过滤返回的数据。 下面的运算符可以在 WHERE 子句中使用： 运算符 描述 &#x3D; 等于 &lt;&gt; 不等于。 注释： 在 SQL 的一些版本中，该操作符可被写成 !&#x3D; &gt; 大于 &lt; 小于 &gt;&#x3D; 大于等于 &lt;&#x3D; 小于等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 IN 指定针对某个列的多个可能值 4、汇总数据汇总数据相关的函数： 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 5、分组数据HAVING vs WHERE： WHERE：过滤指定的行，后面不能加聚合函数（分组函数）。 HAVING：过滤分组，必须要与 GROUP BY 连用，不能单独使用 （1）返回订单数量总和不小于 100 的所有订单号，最后结果按照订单号升序排序。 12345SELECT order_numFROM OrderItemsGROUP BY order_numHAVING Sum(quantity) &gt;= 100ORDER BY order_num 6、使用子查询（1）使用子查询，返回购买价格为 10 美元或以上产品的顾客列表，结果无需排序。 12345SELECT cust_idFROM OrdersWHERE order_num IN (SELECT DISTINCT order_num FROM OrderItems where item_price &gt;= 10) 7、连接表连接表时需要在每个表中选择一个字段，并对这些字段的值进行比较，值相同的两条记录将合并为一条。连接表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。 使用 JOIN 连接两个表的基本语法如下： 1234SELECT table1.column1, table2.column2...FROM table1JOIN table2ON table1.common_column1 = table2.common_column2; SQL 允许在 JOIN 左边加上一些修饰性的关键词，从而形成不同类型的连接，如下表所示： 连接类型 说明 INNER JOIN 内连接 （默认连接方式）只有当两个表都存在满足条件的记录时才会返回行。 LEFT JOIN &#x2F; LEFT OUTER JOIN 左(外)连接 返回左表中的所有行，即使右表中没有满足条件的行也是如此。 RIGHT JOIN &#x2F; RIGHT OUTER JOIN 右(外)连接 返回右表中的所有行，即使左表中没有满足条件的行也是如此。 FULL JOIN &#x2F; FULL OUTER JOIN 全(外)连接 只要其中有一个表存在满足条件的记录，就返回行。 SELF JOIN 将一个表连接到自身，就像该表是两个表一样。为了区分两个表，在 SQL 语句中需要至少重命名一个表。 CROSS JOIN 交叉连接，从两个或者多个连接表中返回记录集的笛卡尔积。 如果不加任何修饰词，只写 JOIN，那么默认为 INNER JOIN。 8、组合查询UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。 UNION 基本规则： 所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询。 默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。 123SELECT column_name(s) FROM table1UNION ALLSELECT column_name(s) FROM table2; UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。 JOIN vs UNION： JOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。 UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。 9、增删改操作SQL 插入记录的方式汇总： 普通插入（全字段） ：INSERT INTO table_name VALUES (value1, value2, ...) 普通插入（限定字段） ：INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...) 多条一次性插入 ：INSERT INTO table_name (column1, column2, ...) VALUES (value1_1, value1_2, ...), (value2_1, value2_2, ...), ... 从另一个表导入 ：INSERT INTO table_name SELECT * FROM table_name2 [WHERE key=value] 带更新的插入 ：REPLACE INTO table_name VALUES (value1, value2, ...)（注意这种原理是检测到主键或唯一性索引键重复就删除原记录后重新插入） 更新记录： 1UPDATE examination_info SET tag = &#x27;Python&#x27; WHERE tag=&#x27;PYTHON&#x27; 或 1234UPDATE examination_infoSET tag = REPLACE(tag,&#x27;PYTHON&#x27;,&#x27;Python&#x27;)# REPLACE (目标字段，&quot;查找内容&quot;,&quot;替换内容&quot;) 删除记录： 1DELETE FROM exam_record WHERE MINUTE (TIMEDIFF(submit_time , start_time)) &lt; 5 AND score &lt; 60 10、表与索引操作创建表 123456789CREATE TABLE IF NOT EXISTS user_info_vip( id INT(11) PRIMARY KEY AUTO_INCREMENT COMMENT&#x27;自增ID&#x27;, uid INT(11) UNIQUE NOT NULL COMMENT &#x27;用户ID&#x27;, nick_name VARCHAR(64) COMMENT&#x27;昵称&#x27;, achievement INT(11) DEFAULT 0 COMMENT &#x27;成就值&#x27;, `level` INT(11) COMMENT &#x27;用户等级&#x27;, job VARCHAR(32) COMMENT &#x27;职业方向&#x27;, register_time DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;注册时间&#x27;)CHARACTER SET UTF8 修改表 1234ALTER TABLE user_info ADD school VARCHAR(15) AFTER level, CHANGE job profession VARCHAR(10), MODIFY achievement INT(11) DEFAULT 0; 删除表 1DROP TABLE IF EXISTS exam_record_2011; 创建索引 123456789101112-- 示例：-- 添加B-Tree索引：\tCREATE INDEX idx_name(索引名) ON 表名 (字段名); -- idx_name为索引名，以下都是-- 创建唯一索引：\tCREATE UNIQUE INDEX idx_name ON 表名 (字段名);-- 创建一个主键索引：\tALTER TABLE 表名 ADD PRIMARY KEY (字段名);-- 创建一个全文索引\tALTER TABLE 表名 ADD FULLTEXT INDEX idx_name (字段名);-- 通过以上示例，可以看出create 和 alter 都可以添加索引 示例： 1234ALTER TABLE examination_info ADD INDEX idx_duration(duration), ADD UNIQUE INDEX uniq_idx_exam_id(exam_id), ADD FULLTEXT INDEX full_idx_tag(tag); 删除索引 12345-- 使用 DROP INDEX 删除索引DROP INDEX idx_name ON 表名;-- 使用 ALTER TABLE 删除索引ALTER TABLE employees DROP INDEX idx_email; 注：在 MySQL 中，一次删除多个索引的操作是不支持的。每次删除索引时，只能指定一个索引名称进行删除。 示例： 12DROP INDEX uniq_idx_exam_id ON examination_info;DROP INDEX full_idx_tag ON examination_info;","categories":["数据库"]},{"title":"面试题精选（二）","path":"/2023/10/09/面试题精选（二）/","content":"Java后端面试题精选。 Java I&#x2F;O1、何为 I&#x2F;O?I&#x2F;O（Input&#x2F;Outpu） 即输入／输出 。 我们先从计算机结构的角度来解读一下 I&#x2F;O。 根据冯.诺依曼结构，计算机结构分为 5 大部分：运算器、控制器、存储器、输入设备、输出设备。 从计算机结构的视角来看的话， I&#x2F;O 描述了计算机系统与外部设备之间通信的过程。 我们再先从应用程序的角度来解读一下 I&#x2F;O。 为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 用户空间（User space） 和 内核空间（Kernel space ） 。 像我们平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源有关的操作，比如文件管理、进程通信、内存管理等等。也就是说，我们想要进行 IO 操作，一定是要依赖内核空间的能力。并且，用户空间的程序不能直接访问内核空间。 当想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成。因此，用户进程想要执行 IO 操作的话，必须通过 系统调用 来间接访问内核空间。 我们在平常开发过程中接触最多的就是 磁盘 IO（读写文件） 和 网络 IO（网络请求和响应）。 从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。 当应用程序发起 I&#x2F;O 调用后，会经历两个步骤： 内核等待 I&#x2F;O 设备准备好数据。 内核将数据从内核空间拷贝到用户空间。 2、有哪些常见的 IO 模型?UNIX 系统下， IO 模型一共有 5 种：同步阻塞 I&#x2F;O、同步非阻塞 I&#x2F;O、I&#x2F;O 多路复用、信号驱动 I&#x2F;O 和异步 I&#x2F;O。 （1）BIO (Blocking I&#x2F;O)BIO 属于同步阻塞 IO 模型 。 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。 在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I&#x2F;O 处理模型来应对更高的并发量。 （2）NIO (Non-blocking I&#x2F;O)Java 中的 NIO 提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它是支持面向缓冲的，基于通道的 I&#x2F;O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。 Java 中的 NIO 可以看作是 I&#x2F;O 多路复用模型。也有很多人认为，Java 中的 NIO 属于同步非阻塞 IO 模型。 我们先来看看 同步非阻塞 IO 模型。 同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。 相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。 但是，这种 IO 模型同样存在问题：应用程序不断进行 I&#x2F;O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 这个时候，I&#x2F;O 多路复用模型 就上场了。 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -&gt; 用户空间）还是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。 select 调用：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 （3）AIO (Asynchronous I&#x2F;O)AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。 异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 Netty1、Netty 是什么？ Netty 是⼀个 基于 NIO 的 client-server(客户端服务器)框架，使用它可以快速简单地开发网络应用程序。 它极大地简化并优化了 TCP 和 UDP 套接字服务器等网络编程,并且性能以及安全性等很多方面甚至都要更好。 支持多种协议 如 FTP，SMTP，HTTP 以及各种二进制和基于文本的传统协议。 2、为什么要用 Netty？Netty 具有下面这些优点，并且相比于直接使用 JDK 自带的 NIO 相关的 API 来说更加易用。 统一的 API，支持多种传输类型，阻塞和非阻塞的。 简单而强大的线程模型。 自带编解码器解决 TCP 粘包&#x2F;拆包问题。 自带各种协议栈。 真正的无连接数据包套接字支持。 比直接使用 Java 核心 API 有更高的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制。 安全性不错，有完整的 SSL&#x2F;TLS 以及 StartTLS 支持。 社区活跃 成熟稳定，经历了大型项目的使用和考验，而且很多开源项目都使用到了 Netty， 比如我们经常接触的 Dubbo、RocketMQ 等等。 3、Netty 应用场景了解么？Netty 主要用来做网络通信 : 作为 RPC 框架的网络通信工具 ：我们在分布式系统中，不同服务节点之间经常需要相互调用，这个时候就需要 RPC 框架了。不同服务节点之间的通信是如何做的呢？可以使用 Netty 来做。比如我调用另外一个节点的方法的话，至少是要让对方知道我调用的是哪个类中的哪个方法以及相关参数吧！ 实现一个自己的 HTTP 服务器 ：通过 Netty 我们可以自己实现一个简单的 HTTP 服务器，这个大家应该不陌生。说到 HTTP 服务器的话，作为 Java 后端开发，我们一般使用 Tomcat 比较多。一个最基本的 HTTP 服务器可要以处理常见的 HTTP Method 的请求，比如 POST 请求、GET 请求等等。 实现一个即时通讯系统 ：使用 Netty 我们可以实现一个可以聊天类似微信的即时通讯系统，这方面的开源项目还蛮多的，可以自行去 Github 找一找。 实现消息推送系统 ：市面上有很多消息推送系统都是基于 Netty 来做的。 4、Netty 核心组件与作用（1）Bytebuf（字节容器） 网络通信最终都是通过字节流进行传输的。 ByteBuf 就是 Netty 提供的⼀个字节容器，其内部是⼀个字节数组。 当我们通过 Netty 传输数据的时候，就是通过 ByteBuf 进行的。我们可以将 ByteBuf 看作是 Netty 对 Java NIO 提供了 ByteBuffer 字节容器的封装和抽象。 （2）Bootstrap 和 ServerBootstrap（启动引导类） Bootstrap 是客户端的启动引导类&#x2F;辅助类 ，ServerBootstrap 是服务端的启动引导类&#x2F;辅助类 。 Bootstrap 通常使用 connect() 方法连接到远程的主机和端口，作为⼀个 Netty TCP 协议通信中的客户端。另外， Bootstrap 也可以通过 bind()方法绑定本地的⼀个端口，作为 UDP 协议通信中的⼀端。 ServerBootstrap 通常使用bind() 方法绑定本地的端口上，然后等待客户端的连接。 Bootstrap 只需要配置⼀个线程组EventLoopGroup , 而 ServerBootstrap 需要配置两个线程组EventLoopGroup ，⼀个用于接收连接，⼀个用于具体的 IO 处理。 （3）Channel Channel 接口是 Netty 对网络操作的抽象类。通过 Channel 我们可以进行 I&#x2F;O 操作。⼀旦客户端成功连接服务端，就会新建⼀个 Channel 同该用户端进行绑定。 比较常用的 Channel 接口实现类是 ：NioServerSocketChannel （服务端）、NioSocketChannel （客户端），这两个 Channel 可以和 BIO 编程模型中的 ServerSocket 以及 Socket 两个概念对应上。 （4）EventLoop（事件循环） EventLoop 的主要作用实际就是责监听网络事件并调用事件处理器进行相关 I&#x2F;O 操作（读写）的处理。 Channel 和 EventLoop 的关系？ Channel 为 Netty 网络操作(读写等操作)抽象类， EventLoop 负责处理注册到其上的 Channel 的 I&#x2F;O 操作，两者配合进行 I&#x2F;O 操作。 EventloopGroup 和 EventLoop 的关系？ EventLoopGroup 包含多个 EventLoop （每⼀个 EventLoop 通常内部包含⼀个线程），它管理着所有的 EventLoop 的生命周期。并且， EventLoop 处理的 I&#x2F;O 事件都将在它专有的 Thread 上被处理，即 Thread 和 EventLoop 属于 1 : 1 的关系，从而保证线程安全。 （5）ChannelHandler（消息处理器） 和 ChannelPipeline（ChannelHandler 对象链表） ChannelHandler 是消息的具体处理器，主要负责处理客户端&#x2F;服务端接收和发送的数据。 当Channel被创建时，它会自动地分配到它专属的ChannelPipeline。一个Channel包含一个ChannelPipeline。ChannelPipeline是ChannelHandler的链，一个pipeline上可以有很多的ChannelHandler。 可以在ChannelPipeline上通过addLast（）方法添加一个或者多个ChannelHandler（一个数据或者事件可能会被多个Handler处理）。当一个ChannelHandler处理完之后就会将数据交给下一个ChannelHandler。 当ChannelHandler被添加到ChannelPipeline，它会有一个CHannelHandlerContext，代表一个ChannelHandler和ChannelPipeline之间的“绑定”。ChannelPipeline通过ChannelHandlerContext来间接管理ChannelHandler。 （6）ChannelFuture操作执行结果 Netty中所有的IO操作都为异步的，我们不能立刻得到操作是否执行成功。 但是可以通过ChannelFuture接口的addListener（）方法注册一个ChannelFutureListener，当操作执行成功或者失败时，监听就会自动触发返回结果。 并且，你还可以通过 ChannelFuture 的 channel() 方法获取连接相关联的 Channel 。 另外，我们还可以通过 ChannelFuture 接口的 sync() 方法让异步的操作编程同步的。 （7）NioEventLoopGroup 默认的构造函数会起多少线程？ NioEventLoopGroup 默认的构造函数实际会起的线程数为 CPU核心数*2。 另外，如果你继续深入下去看构造函数的话，你会发现每个 NioEventLoopGroup 对象内部都会分配⼀组 NioEventLoop ，其大小是 nThreads , 这样就构成了⼀个线程池， ⼀个 NIOEventLoop 和⼀个线程相对应，这和我们上面说的 EventloopGroup 和 EventLoop 关系这部分内容相对应。 5、Reactor线程模型Reactor是一种经典的线程模型，Reactor模式基于事件驱动，特别适合海量的IO事件。 Reactor线程模型分为单线程模型、多线程模型以及主从多线程模型。 （1）单线程Reactor 所有的 IO 操作都由同⼀个 NIO 线程处理。 单线程 Reactor 的优点是对系统资源消耗特别小，但是，没办法支撑大量请求的应用场景并且处理请求的时间可能非常慢，项目中一般不使用 。 （2）多线程Reactor ⼀个线程负责接受请求，⼀组 NIO 线程处理 IO 操作。 大部分场景下多线程 Reactor 模型是没有问题的，但是在⼀些并发连接数比较多（如百万并发）的场景下，⼀个线程负责接受客户端请求就存在性能问题了。 （3）主从多线程Reactor 一组NIO线程负责接受请求，一组NIO线程处理IO操作。 6、什么是TCP粘包、拆包？1）粘包拆包发生场景 因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。 如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。 如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。 2）常见的解决方案 对于粘包和拆包问题，常见的解决方案有四种： 发送端将每个包都封装成固定的长度，比如100字节大小。如果不足100字节可通过补0或空等进行填充到指定长度； 发送端在每个包的末尾使用固定的分隔符，例如\\r 。如果发生拆包需等待多个包发送过来之后再找到其中的\\r 进行合并；例如，FTP协议； 将消息分为头部和消息体，头部中保存整个消息的长度，只有读取到足够长度的消息之后才算是读到了一个完整的消息； 通过自定义协议进行粘包和拆包的处理。 3）Netty对粘包和拆包问题的处理 Netty对解决粘包和拆包的方案做了抽象，提供了一些解码器（Decoder）来解决粘包和拆包的问题。如： LineBasedFrameDecoder：以行为单位进行数据包的解码； DelimiterBasedFrameDecoder：以特殊的符号作为分隔来进行数据包的解码； FixedLengthFrameDecoder：以固定长度进行数据包的解码； LenghtFieldBasedFrameDecode：适用于消息头包含消息长度的协议（最常用）； 基于Netty进行网络读写的程序，可以直接使用这些Decoder来完成数据包的解码。对于高并发、大流量的系统来说，每个数据包都不应该传输多余的数据（所以补齐的方式不可取），LenghtFieldBasedFrameDecode更适合这样的场景。 7、Netty的长连接和心跳机制（1）TCP长连接和短连接 TCP在进行读写之前，server与client之间必须提前建立一个连接。建立连接的过程，需要三次握手，释放连接时需要四次挥手。这个过程是比较消耗网络资源并且有时间延迟的。 短连接：server端与client端建立连接之后，读写完成之后就关闭掉连接，如果下一次再要互相发送消息，就要重新连接。 优点：管理和实现比较简单， 缺点：每一次的读写都要建立连接必然会带来大量网络资源消耗，并且连接的建立也需要耗费时间。 长连接：client向server双方建立连接之后，即使client与server完成一次读写，他们之间的连接也不会主动关闭，后续的读写操作会继续使用这个连接。 长连接可以省去较多的TCP建立和关闭的操作，降低对网络资源的依赖，节约时间。对于拼房请求资源的客户来说，非常适合长连接。 （2）心跳机制 在TCP保持长连接的过程中，可能会出现断网等网络异常出现，异常发生的时候，client与server之间如果没有交互的话，他们是无法发现对方已经掉线的，为了解决这个问题，引入了心跳机制。 心跳机制的工作原理：在client与server之间在一定时间内没有数据交互时，即处于idle状态时，客户端或服务端就会发送一个特殊的数据包给对方，当接收方收到这个数据报文后，也立即发送一个特殊的数据报文回应给对方，这就是一个PING+PONG交互。所以当某一端收到心跳信息后，就知道对方仍然在线，这就确保了TCP连接的有效性。 TCP实际上自带的就有长连接选项，本身也有心跳包机制，也就是TCP的选项：SO_KEEP_ALIVE。但是，TCP协议层面的长连接灵活性不够，所以，一般情况下我们都是在应用层协议之上实现自定义心跳机制，也就是在Netty层面通过编码实现。通过Netty实现心跳机制，核心类时IdleStateHandler。 8、Netty的零拷贝在 OS 层⾯上的 Zero-copy 通常指避免在用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据。而在 Netty 层面 ，零拷贝主要体现在对于数据操作的优化。 零拷贝技术的核心思想是将数据从内核空间直接传输到网络适配器的缓冲区，避免了数据在内核空间和用户空间之间的复制。在使用零拷贝技术时，应用程序通过调用操作系统提供的API（如sendfile、mmap等）将数据直接映射到网络适配器的缓冲区，然后通过网络传输到远程主机。这样，数据只需要经过一次复制操作，从而大大提高了数据传输的效率和性能。 Netty 中的零拷贝体现在以下几个方面： 使用 Netty 提供的 CompositeByteBuf 类，可以将多个 ByteBuf 合并为⼀个逻辑上的 ByteBuf ，避免了各个 ByteBuf 之间的拷贝。 ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同⼀个存储区域的 ByteBuf ，避免了内存的拷贝。 通过 FileRegion 包装的 FileChannel.tranferTo 实现文件传输，可以直接将文件缓冲区的数据发送到目标 Channel ，避免了传统通过循环 write 方式导致的内存拷贝问题。 Dubbo1、什么是RPC？RPC就是远程方法调用，和本地方法调用不同，本地方法调用指的是进程内部的方法调用，而远程方法调用指的是两个进程内的方法互相调用。 要实现远程方法调用，必然通过网络进行数据传输，于是就有了： RPC &amp; HTTP：通过HTTP协议传输数据； RPC &amp; TCP：通过TCP协议传输数据； 有了 HTTP 协议，为什么还要有 RPC ？ 2、什么是Dubbo？目前，官网上是这么介绍的：Apache Dubbo 是⼀款⾼性能、轻量级的开源 Java 服务框架。在几个月前，官网的介绍是：Apache Dubbo 是⼀款⾼性能、轻量级的开源 Java RPC框架。 为什么会将RPC改为服务？ Dubbo⼀开始的定位就是RPC，专注于两个服务之间的调用。但随着微服务的盛行，除了服务调用之外，Dubbo也在逐步的涉猎服务治理、服务监控、服务网关等等，所以现在的Dubbo目标已经不止是RPC框架了，而是和Spring Cloud类似想成为了⼀个服务框架。 Dubbo 内置支持 Dubbo2、Triple 两款高性能通信协议。其中 Dubbo2 是基于 TCP 传输协议之上构建的二进制私有 RPC 通信协议，是一款非常简单、紧凑、高效的通信协议。 Triple 是基于 HTTP&#x2F;2 的新一代 RPC 通信协议，在网关穿透性、通用性以及 Streaming 通信上具备优势，Triple 完全兼容 gRPC 协议。 3、原理 4、服务发现Dubbo 提供的是一种 Client-Based 的服务发现机制，依赖第三方注册中心组件来协调服务发现过程，支持常用的注册中心如 Nacos、Consul、Zookeeper 等。 以下是 Dubbo 服务发现机制的基本工作原理图： 服务发现包含提供者、消费者和注册中心三个参与角色，其中，Dubbo 提供者实例注册 URL 地址到注册中心，注册中心负责对数据进行聚合，Dubbo 消费者从注册中心读取地址列表并订阅变更，每当地址列表发生变化，注册中心将最新的列表通知到所有订阅的消费者实例。 配置Nacos注册中心： Nacos ：Nacos 注册中心的基本使用和工作原理。 5、负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 weighted random 基于权重的随机负载均衡策略。 具体实现上，Dubbo 提供的是客户端负载均衡，即由 Consumer 通过负载均衡算法得出需要将请求提交到哪个 Provider 实例。 负载均衡策略 目前 Dubbo 内置了如下负载均衡算法，可通过调整配置项启用。 使用方式 只需要调整 loadbalance 相应取值即可，每种负载均衡策略取值请参见文档最上方表格。 服务端服务级别 1&lt;dubbo:service interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 客户端服务级别 1&lt;dubbo:reference interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 服务端方法级别 123&lt;dubbo:service interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:service&gt; 客户端方法级别 123&lt;dubbo:reference interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:reference&gt; 6、流量管控Dubbo 的流量管控规则可以基于应用、服务、方法、参数等粒度精准的控制流量走向，根据请求的目标服务、方法以及请求体中的其他附加参数进行匹配，符合匹配条件的流量会进一步的按照特定规则转发到一个地址子集。流量管控规则有以下几种： 条件路由规则 标签路由规则 脚本路由规则 动态配置规则 具体见文档：https://cn.dubbo.apache.org/zh-cn/overview/core-features/traffic/ 7、通信协议Dubbo 框架提供了自定义的高性能 RPC 通信协议：基于 HTTP&#x2F;2 的 Triple 协议 和 基于 TCP 的 Dubbo2 协议。除此之外，Dubbo 框架支持任意第三方通信协议，如官方支持的 gRPC、Thrift、REST、JsonRPC、Hessian2 等，更多协议可以通过自定义扩展实现。这对于微服务实践中经常要处理的多协议通信场景非常有用。 具体见文档：https://cn.dubbo.apache.org/zh-cn/overview/core-features/protocols/ 8、SPI机制spi， 简单来说， 就是 service provider interface， 说白了是什么意思呢， 比如你有个接口， 现在这个接口有 3 个实现类， 那么在系统运行的时候对这个接口到底选择哪个实现类呢？ 这就需要 spi 了， 需要根据指定的配置或者是默认的配置， 去找到对应的实现类加载进来， 然后用这个实现类的实例对象。 举个栗子。你有一个接口 A。 A1&#x2F;A2&#x2F;A3 分别是接口 A 的不同实现。 你通过配置 接口 A &#x3D; 实现 A2， 那么在系统实际运行的时候， 会加载你的配置， 用实现 A2 实例化一个对象来提供服务。 spi 机制一般用在哪儿？ 插件扩展的场景， 比如说你开发了一个给别人使用的开源框架， 如果你想让别人自己写个插件， 插到你的开源框架里面， 从而扩展某个功能， 这个时候 spi 思想就用上了。 9、使用示例Dubbo使用的具体操作步骤如下： 定义接口： 首先，你需要定义服务接口，即提供给其他模块调用的方法和参数。 实现接口： 编写接口的具体实现类，实现接口中定义的方法。 配置提供者： 创建一个Dubbo的配置文件，例如dubbo-provider.xml。 在配置文件中配置服务提供者的相关信息，包括注册中心地址、端口号、服务接口、实现类等。 启动提供者： 在提供者端启动应用程序，加载Dubbo的配置文件。 这样提供者就可以将自己注册到注册中心，并提供服务。 配置消费者： 创建一个Dubbo的配置文件，例如dubbo-consumer.xml。 在配置文件中配置服务消费者的相关信息，包括注册中心地址、超时时间、服务接口等。 引用服务： 在消费者端，通过Dubbo的引用机制，引用提供者提供的服务。 在消费者的配置文件中，配置引用的服务接口、版本号、负载均衡策略等。 调用服务： 在消费者端，通过引用的服务接口，调用提供者提供的方法。 监控和管理： Dubbo提供了控制台用于监控和管理服务。 配置控制台的相关参数，例如注册中心地址、端口号等。 在控制台中，你可以查看服务的调用情况、性能指标等。 以上是Dubbo的基本使用步骤。你可以根据实际需求和具体情况，进行相应的配置和调整。同时，你也可以参考Dubbo的官方文档和示例代码，以获取更详细的操作指南。 Dubbo两小时快速上手教程 负载均衡1、什么是负载均衡？负载均衡 指的是将用户请求分摊到不同的服务器上处理，以提高系统整体的并发处理能力以及可靠性。负载均衡服务可以有由专门的软件或者硬件来完成，一般情况下，硬件的性能更好，软件的价格更便宜。 2、负载均衡分为哪几种？负载均衡可以简单分为 服务端负载均衡 和 客户端负载均衡 这两种。 （1）服务端负载均衡服务端负载均衡 主要应用在 系统外部请求 和 网关层 之间，可以使用 软件 或者 硬件 实现。 下图是我画的一个简单的基于 Nginx 的服务端负载均衡示意图： 根据 OSI 模型，服务端负载均衡还可以分为： 二层负载均衡 三层负载均衡 四层负载均衡 七层负载均衡 最常见的是四层和七层负载均衡。 四层负载均衡 工作在 OSI 模型第四层，也就是传输层，这一层的主要协议是 TCP&#x2F;UDP，负载均衡器在这一层能够看到数据包里的源端口地址以及目的端口地址，会基于这些信息通过一定的负载均衡算法将数据包转发到后端真实服务器。也就是说，四层负载均衡的核心就是 IP+端口层面的负载均衡，不涉及具体的报文内容。 七层负载均衡 工作在 OSI 模型第七层，也就是应用层，这一层的主要协议是 HTTP 。这一层的负载均衡比四层负载均衡路由网络请求的方式更加复杂，它会读取报文的数据部分（比如说我们的 HTTP 部分的报文），然后根据读取到的数据内容（如 URL、Cookie）做出负载均衡决策。也就是说，七层负载均衡器的核心是报文内容（如 URL、Cookie）层面的负载均衡，执行第七层负载均衡的设备通常被称为 反向代理服务器 。 （2）客户端负载均衡客户端负载均衡 主要应用于系统内部的不同的服务之间，可以使用现成的负载均衡组件来实现。 在客户端负载均衡中，客户端会自己维护一份服务器的地址列表，发送请求之前，客户端会根据对应的负载均衡算法来选择具体某一台服务器处理请求。 3、负载均衡常见的算法有哪些？（1）随机法随机法 是最简单粗暴的负载均衡算法。 如果没有配置权重的话，所有的服务器被访问到的概率都是相同的。如果配置权重的话，权重越高的服务器被访问的概率就越大。 （2）轮询法轮询法是挨个轮询服务器处理，也可以设置权重。 如果没有配置权重的话，每个请求按时间顺序逐一分配到不同的服务器处理。如果配置权重的话，权重越高的服务器被访问的次数就越多。 （3）一致性 Hash 法相同参数的请求总是发到同一台服务器处理，比如同个 IP 的请求。 （4）最小连接法当有新的请求出现时，遍历服务器节点列表并选取其中活动连接数最小的一台服务器来响应当前请求。活动连接数可以理解为当前正在处理的请求数。 最小连接法可以尽可能最大地使请求分配更加合理化，提高服务器的利用率。 4、七层负载均衡可以怎么做？简单介绍两种项目中常用的七层负载均衡解决方案：DNS 解析和反向代理。 （1）DNS解析DNS 解析实现负载均衡的原理是这样的：在 DNS 服务器中为同一个主机记录配置多个 IP 地址，这些 IP 地址对应不同的服务器。当用户请求域名的时候，DNS 服务器采用轮询算法返回 IP 地址，这样就实现了轮询版负载均衡。 （2）反向代理客户端将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器，获取数据后再返回给客户端。对外暴露的是反向代理服务器地址，隐藏了真实服务器 IP 地址。 数据库优化1、读写分离（1）什么是读写分离？见名思意，根据读写分离的名字，我们就可以知道：读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。 这样的话，就能够小幅提升写性能，大幅提升读性能。 一般情况下，我们都会选择一主多从，也就是一台主数据库负责写，其他的从数据库负责读。主库和从库之间会进行数据同步，以保证从库中数据的准确性。 （2）读写分离会带来什么问题？读写分离对于提升数据库的并发非常有效，但是，同时也会引来一个问题：主库和从库的数据存在延迟，比如你写完主库之后，主库的数据同步到从库是需要时间的，这个时间差就导致了主库和从库的数据不一致性问题。这也就是我们经常说的 主从同步延迟 。 解决方案： 1、强制将读请求路由到主库处理 既然从库的数据过期了，那我就直接从主库读取嘛！这种方案虽然会增加主库的压力，但是，实现起来比较简单。 2、延迟读取 对于一些对数据比较敏感的场景，你可以在完成写请求之后，避免立即进行请求操作。比如你支付成功之后，跳转到一个支付成功的页面，当你点击返回之后才返回自己的账户。 （3）如何实现读写分离？1、代理方式 我们可以在应用和数据中间加了一个代理层。应用程序所有的数据请求都交给代理层处理，代理层负责分离读写请求，将它们路由到对应的数据库中。 提供类似功能的中间件有 MySQL Router（官方）、Atlas（基于 MySQL Proxy）、MaxScale、MyCat。 2、组件方式 在这种方式中，我们可以通过引入第三方组件来帮助我们读写请求。 这也是我比较推荐的一种方式。这种方式目前在各种互联网公司中用的最多的，相关的实际的案例也非常多。如果你要采用这种方式的话，推荐使用 sharding-jdbc ，直接引入 jar 包即可使用，非常方便。同时，也节省了很多运维的成本。 （4）主从复制原理是什么？ 主库将数据库中数据的变化写入到 binlog 从库连接主库 从库会创建一个 I&#x2F;O 线程向主库请求更新的 binlog 主库会创建一个 binlog dump 线程来发送 binlog ，从库中的 I&#x2F;O 线程负责接收 从库的 I&#x2F;O 线程将接收的 binlog 写入到 relay log 中。 从库的 SQL 线程读取 relay log 同步数据本地（也就是再执行一遍 SQL ）。 MySQL 主从复制是依赖于 binlog 。另外，常见的一些同步 MySQL 数据到其他数据源的工具（比如 canal）的底层一般也是依赖 binlog。 2、分库分表读写分离主要应对的是数据库读并发，没有解决数据库存储问题。试想一下：如果 MySQL 一张表的数据量过大怎么办?换言之，我们该如何解决 MySQL 的存储压力呢？ 答案之一就是 分库分表。 （1）什么是分库？分库 就是将数据库中的数据分散到不同的数据库上，可以垂直分库，也可以水平分库。 垂直分库 就是把单一数据库按照业务进行划分，不同的业务使用不同的数据库，进而将一个数据库的压力分担到多个数据库。 举个例子：将数据库中的用户表、订单表和商品表分别单独拆分为用户数据库、订单数据库和商品数据库。 水平分库 是把同一个表按一定规则拆分到不同的数据库中，每个库可以位于不同的服务器上，这样就实现了水平扩展，解决了单表的存储和性能瓶颈的问题。 举个例子：订单表数据量太大，你对订单表进行了水平切分（水平分表），然后将切分后的 2 张订单表分别放在两个不同的数据库。 （2）什么是分表？分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。 垂直分表 是对数据表列的拆分，把一张列比较多的表拆分为多张表。 举个例子：我们可以将用户信息表中的一些列单独抽出来作为一个表。 水平分表 是对数据表行的拆分，把一张行比较多的表拆分为多张表，可以解决单一表数据量过大的问题。 举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分只能解决单表数据量大的问题，为了提升性能，我们通常会选择将拆分后的多张表放在不同的数据库中。也就是说，水平分表通常和水平分库同时出现。 （3）什么情况下需要分库分表？遇到下面几种场景可以考虑分库分表： 单表的数据达到千万级别以上，数据库读写速度比较缓慢。 数据库中的数据占用的空间越来越大，备份时间越来越长。 应用的并发量太大。 （4）常见的分片算法有哪些？分片算法主要解决了数据被水平分片之后，数据究竟该存放在哪个表的问题。 哈希分片：求指定 key（比如 id） 的哈希，然后根据哈希值确定数据应被放置在哪个表中。哈希分片比较适合随机读写的场景，不太适合经常需要范围查询的场景。 范围分片：按照特性的范围区间（比如时间区间、ID 区间）来分配数据，比如 将 id 为 1~299999 的记录分到第一个库， 300000~599999 的分到第二个库。范围分片适合需要经常进行范围查找的场景，不太适合随机读写的场景（数据未被分散，容易出现热点数据的问题）。 地理位置分片：很多 NewSQL 数据库都支持地理位置分片算法，也就是根据地理位置（如城市、地域）来分配数据。 融合算法：灵活组合多种分片算法，比如将哈希分片和范围分片组合。 （5）分库分表会带来什么问题呢？join 操作：同一个数据库中的表分布在了不同的数据库中，导致无法使用 join 操作。这样就导致我们需要手动进行数据的封装，比如你在一个数据库中查询到一个数据之后，再根据这个数据去另外一个数据库中找对应的数据。不过，很多大厂的资深 DBA 都是建议尽量不要使用 join 操作。因为 join 的效率低，并且会对分库分表造成影响。 事务问题：同一个数据库中的表分布在了不同的数据库中，如果单个操作涉及到多个数据库，那么数据库自带的事务就无法满足我们的要求了。这个时候，我们就需要引入分布式事务了。 分布式 ID：分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 ID 了。 跨库聚合查询问题：分库分表会导致常规聚合查询操作，如 group by，order by 等变得异常复杂。这是因为这些操作需要在多个分片上进行数据汇总和排序，而不是在单个数据库上进行。为了实现这些操作，需要编写复杂的业务代码，或者使用中间件来协调分片间的通信和数据传输。 （6）分库分表比较推荐的方案？Apache ShardingSphere 是一款分布式的数据库生态系统， 可以将任意数据库转换为分布式数据库，并通过数据分片、弹性伸缩、加密等能力对原有数据库进行增强。 分库分表的实战文章：《芋道 Spring Boot 分库分表入门》 mycat基于 cobar 改造的， 属于 proxy 层方案， 支持的功能非常完善， 而且目前应该是非常火的而且不断流行的数据库中间件， 社区很活跃， 也有一些公司开始在用了。 但是确实相比于 sharding jdbc 来说， 年轻一些， 经历的锤炼少一些。 总结： sharding-jdbc 这种 client 层方案的优点在于不用部署， 运维成本低， 不需要代理层的二次转发请求， 性能很高， 但是如果遇到升级啥的需要各个系统都重新升级版本再发布， 各个系统都需要耦合 sharding-jdbc 的依赖； mycat 这种 proxy 层方案的缺点在于需要部署， 自己运维一套中间件， 运维成本高， 但是好处在于对于各个项目是透明的， 如果遇到升级之类的都是自己中间件那里搞就行了。 （7）分库分表后，数据怎么迁移呢？分库分表之后，我们如何将老库（单库单表）的数据迁移到新库（分库分表后的数据库系统）呢？ 比较简单同时也是非常常用的方案就是停机迁移，写个脚本老库的数据写到新库中。比如你在凌晨 2 点，系统使用的人数非常少的时候，挂一个公告说系统要维护升级预计 1 小时。然后，你写一个脚本将老库的数据都同步到新库中。 如果你不想停机迁移数据的话，也可以考虑双写方案。双写方案是针对那种不能停机迁移的场景，实现起来要稍微麻烦一些。具体原理是这样的： 我们对老库的更新操作（增删改），同时也要写入新库（双写）。如果操作的数据不存在于新库的话，需要插入到新库中。 这样就能保证，咱们新库里的数据是最新的。 在迁移过程，双写只会让被更新操作过的老库中的数据同步到新库，我们还需要自己写脚本将老库中的数据和新库的数据做比对。如果新库中没有，那咱们就把数据插入到新库。如果新库有，旧库没有，就把新库对应的数据删除（冗余数据清理）。 重复上一步的操作，直到老库和新库的数据一致为止。 想要在项目中实施双写还是比较麻烦的，很容易会出现问题。我们可以借助上面提到的数据库同步工具 Canal 做增量数据迁移（还是依赖 binlog，开发和维护成本较低）。 消息队列1、消息队列选型Kafka、 ActiveMQ、 RabbitMQ、 RocketMQ 有什么优缺点？ 2、Kafka 核心概念（1）什么是 Producer、Consumer、Broker、Topic、Partition？Kafka 将生产者发布的消息发送到 Topic（主题） 中，需要这些消息的消费者可以订阅这些 Topic（主题），如下图所示： Producer（生产者） : 生产消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。 Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。 （2）Kafka 的多副本机制Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。 带来的好处： Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。 Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。 3、Kafka 消费顺序、消息丢失和重复消费（1）Kafka 如何保证消息的消费顺序？我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。 每次添加消息到 Partition(分区) 的时候都会采用尾加法。 Kafka 只能为我们保证 Partition(分区) 中的消息有序。 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。 所以，我们就有一种很简单的保证消息消费顺序的方法：1 个 Topic 只对应一个 Partition。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。 Kafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表&#x2F;对象的 id 来作为 key 。 总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key&#x2F;Partition，然后对于 N 个消费方线程，每个线程分别消费一个Partition 即可，这样就能保证顺序性。 （2）Kafka 如何保证消息不丢失？1、生产者丢失消息的情况 生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 为了确定消息是发送成功，我们要判断消息发送的结果。我们可以通过 get()方法获取调用结果。 123ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o); future.addCallback(result -&gt; logger.info(&quot;生产者成功发送消息到topic:&#123;&#125; partition:&#123;&#125;的消息&quot;, result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -&gt; logger.error(&quot;生产者发送消失败，原因：&#123;&#125;&quot;, ex.getMessage())); 如果消息发送失败的话，我们检查失败的原因之后重新发送即可！ 解决办法： 设置acks&#x3D;all， 一定不会丢， 要求是， 你的 leader 接收到消息， 所有的 follower都同步到了消息之后， 才认为本次写成功了。 如果没满足这个条件， 生产者会自动不断的重试， 重试无限次。 2、消费者丢失消息的情况 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。 3、Kafka 弄丢了消息 我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。 试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 设置 acks &#x3D; all，解决办法就是我们设置 acks &#x3D; all。 设置 replication.factor &gt;&#x3D; 3 为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 replication.factor &gt;&#x3D; 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。 设置 min.insync.replicas &gt; 1 一般情况下我们还需要设置 min.insync.replicas&gt; 1 ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。另外需要确保需要确保 replication.factor &gt; min.insync.replicas。 （3）Kafka 如何保证消息不重复消费？kafka 出现消息重复消费的原因： 服务端侧已经消费的数据没有成功提交 offset（根本原因）。 Kafka 侧 由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务假死，触发了分区 rebalance。 1、kafka的rebalance机制：consumer group中的消费者与topic下的partion重新匹配的过程 2、何时会产生rebalance：consumer group中的成员个数发生变化consumer消费超时group订阅的topic个数发生变化group订阅的topic的分区数发生变化 解决方案： 消费消息服务做幂等校验，比如 Redis 的 set、MySQL 的主键等天然的幂等功能。这种方法最有效。 将 enable.auto.commit参数设置为 false，关闭自动提交，开发者在代码中手动提交 offset。那么这里会有个问题： 什么时候提交 offset 合适？ 处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底 （4）大量消息在 MQ 里长时间积压，该如何解决？一般这个时候， 只能临时紧急扩容了， 具体操作步骤和思路如下： 先修复 consumer 的问题， 确保其恢复消费速度， 然后将现有 consumer 都停掉。 新建一个 topic， partition 是原来的 10 倍， 临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序， 这个程序部署上去消费积压的数据， 消费之后不做耗时的处理， 直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer， 每一批 consumer 消费一个临时 queue 的数据。 这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍， 以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后， 得恢复原先部署的架构， 重新用原先的 consumer 机器来消费消息。 海量数据处理1、Bitmapbitmap是什么？ 在计算机中一个字节(byte) &#x3D; 8位(bit), 这里的bit就是位，数据的最小表示单位，map一般是表示地图或者映射，加一起叫作位图。 简单回顾一下二进制的一些知识： 1byte &#x3D; 8bit 一个bit有2种状态，0 或者 1 所以1个byte可以表示0000 0000 -&gt; 1111 1111, 也就是十进制的 0 到 255。 有10亿个不重复的无序的数字，如何快速排序？ 在大部分编程语言里面，int类型一般的都是占4个byte，也是32位，不管你这个数字是1 或者是1亿你都得占32位，所以如果你现在有10亿数字需要存放在内存里面，需要多少内存呢？ 1000000000 * 4 &#x2F; 1024 &#x2F; 1024 &#x3D; 3800MB，大概需要3800MB内存。 为了解决这个问题，bitmap采用了一种映射机制，举个例子，假如有 1，3, 7，2, 5 这5个数字需要存放，正常情况下你需要5*4&#x3D;20byte，但bitmap只需要1byte，它是咋做到呢？ 假设下面是1byte，首先将所有位置为0： 0 0 0 0 0 0 0 从第一个0开始数数，把对应数字的位置置为1，比如说第一个1那就是第2个位置置为1，第二个3就是把第4个位置置为1，依此论推… 123451 =&gt; 0 1 0 0 0 0 0 03 =&gt; 0 0 0 1 0 0 0 07 =&gt; 0 0 0 0 0 0 0 12 =&gt; 0 0 1 0 0 0 0 05 =&gt; 0 0 0 0 0 1 0 0 叠加起来最终的串就是： 10 1 1 1 0 1 0 1 其实最终的数字和二进制没有什么关系，纯粹是数数，这个串就可以代表最大到7的数字，然后我们就开始数数，从0开始： 12345比如第1个位置是1，那就记个1比如第2个位置是1，那就记个2比如第3个位置是1，那就记个3比如第5个位置是1，那就记个5比如第7个位置是1，那就记个7 结果就是 1 2 3 5 7，不仅仅排序了，而且还去重了！ 2、问题一：统计不同号码的个数这类题目其实是求解数据重复的问题。对于这类问题，可以使用位图法处理 8位电话号码可以表示的范围为00000000～99999999。如果用 bit表示一个号码，那么总共需要1亿个bit，总共需要大约10MB的内存。 创建一个长度为10^8的位图（Bitmap），每个位代表一个电话号码是否出现过。初始时，所有位都设置为0 读取文件中的每个电话号码，将其转换为整数。 对于每个电话号码，检查对应的位图位置是否为0。如果为0，则将该位置的位图设置为1，表示该号码已经出现过。 继续读取下一个电话号码，重复步骤3，直到读取完所有号码。 统计位图中值为1的位的个数，即为不同号码的个数。 3、问题二：出现频率最高的100个词假如有一个1G大小的文件，文件里每一行是一个词，每个词的大小不超过16byte，要求返回出现频率最高的100个词。内存大小限制是10M 方案： 由于内存限制，我们无法直接将大文件的所有词一次性读到内存中。 可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于10M，进而直接将单个小文件读取到内存中进行处理。 第一步，首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 500，将结果为i的词存放到文件f(i)中，遍历结束后，可以得到500个小文件，每个小文件的大小为2M左右； 第二步，接着统计每个小文件中出现频数最高的100个词。可以使用HashMap来实现，其中key为词，value为该词出现的频率。 第三步，在第二步中找出了每个文件出现频率最高的100个词之后，通过维护一个小顶堆来找出所有小文件中出现频率最高的100个词。 4、问题三：查找两个大文件共同的URL给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，找出 a、b 两个文件共同的 URL。内存限制是 4G。 方案： 每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。5,000,000,000 * 64B ≈ 320GB 由于内存大小只有 4G，因此，不可能一次性把所有 URL 加载到内存中处理。 可以采用分治策略，也就是把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。 首先遍历文件a，对遍历到的 URL 进行哈希取余 hash(URL) % 1000，根据计算结果把遍历到的 URL 存储到 a0, a1,a2, …, a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, …, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, …, a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。 接着遍历 ai( i∈[0,999])，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。 最后总结一下： 分而治之，进行哈希取余； 对每个子文件进行 HashSet 统计。 更多：数据中 TopK 问题的常用套路 Maven1、Maven依赖冲突及解决Maven是一个常用的构建和项目管理工具，它可以帮助我们管理项目的依赖关系。在使用Maven构建项目时，可能会遇到依赖冲突的问题，即不同的依赖项引入了同一个类或版本不一致的类。 以下是几种常见的解决依赖冲突的方法： 排除冲突依赖：在pom.xml文件中，可以通过 &lt;exclusions&gt; 标签排除某个依赖项的特定传递依赖。例如： 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;example-artifact&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;conflicting-group&lt;/groupId&gt; &lt;artifactId&gt;conflicting-artifact&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 引入统一版本：如果项目中引入了多个依赖项，且它们引用了相同的库但版本不同，可以通过在pom.xml中显式指定一个统一的版本来解决冲突。例如： 12345678910&lt;dependency&gt; &lt;groupId&gt;conflicting-group&lt;/groupId&gt; &lt;artifactId&gt;conflicting-artifact&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;another-group&lt;/groupId&gt; &lt;artifactId&gt;another-artifact&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 使用dependencyManagement：在父项目的pom.xml中，可以使用 &lt;dependencyManagement&gt; 标签来集中管理依赖项的版本。子项目可以继承父项目的依赖管理，确保所有子项目使用相同的依赖版本。 使用Maven插件：Maven提供了一些插件来解决依赖冲突问题，如Maven Dependency Plugin和Maven Enforcer Plugin。这些插件可以帮助分析和解决依赖冲突，提供冲突报告和冲突解决策略。 手动调整依赖：在某些情况下，可能需要手动调整项目的依赖关系，例如移除冲突的依赖项或选择合适的替代依赖项。 解决依赖冲突问题需要根据具体情况进行分析和调整。在解决冲突时，建议使用最新的稳定版本，并确保依赖项的版本兼容性。同时，及时更新和维护项目的依赖关系，以避免潜在的冲突问题。 场景问题设计1、对于高并发数据量，10亿级，怎么实现一个redis排行榜对于高并发数据量，10亿级的情况下，实现一个Redis排行榜可以采取以下策略： 使用有序集合（Sorted Set）：Redis的有序集合数据结构非常适合实现排行榜。可以将每个元素作为有序集合的成员，分数作为排序依据。分数可以是用户的得分、点击量、浏览量等。 分片存储：将数据按照一定的规则进行分片存储，将不同的排行榜数据分散到不同的Redis节点上。这样可以降低单个Redis节点的负载压力，提高并发处理能力。 合理设置过期时间：根据业务需求，合理设置排行榜数据的过期时间，避免长时间无效数据的积累。可以使用Redis的过期时间功能，自动清理过期的排行榜数据。 使用批量操作：对于大量的数据插入或更新操作，可以使用Redis的批量操作命令（如 ZADD ）来一次性处理多个元素，减少网络开销和提高效率。 使用Redis集群或分布式部署：当数据量非常大时，可以考虑使用Redis集群或将数据分布到多个Redis实例中。这样可以水平扩展Redis的处理能力，提高并发处理和数据存储的能力。 合理选择数据结构和算法：根据具体的排行榜需求，选择合适的数据结构和算法。例如，如果需要支持按照时间范围查询排行榜，可以使用Redis的有序集合结合时间戳作为分数，使用 ZREVRANGEBYSCORE 命令来查询指定时间范围内的排行榜数据。 监控和优化：定期监控Redis的性能指标，如QPS（Queries Per Second）、内存使用情况等，并进行优化。可以使用Redis的性能监控工具或第三方监控工具来帮助发现性能瓶颈和优化点。 需要根据具体的业务需求和系统架构选择适合的实现方式，并进行性能测试和调优，以确保Redis排行榜的高并发处理能力和稳定性。 2、实现用户多设备同时登录要实现用户多设备同时登录的功能，可以使用Java中的会话管理和并发控制机制来实现。以下是一个基本的实现方案： 用户登录时，生成一个唯一的会话ID，并将该会话ID与用户绑定。可以使用UUID类生成唯一的会话ID。 将会话ID存储在后端服务器的会话管理器中，可以使用内存存储、数据库或缓存等方式。 每次用户在新设备上登录时，生成一个新的会话ID，并将其与用户绑定。 在用户每次请求时，将会话ID作为请求的一部分发送到后端服务器。 后端服务器接收到请求后，根据会话ID验证用户的身份，并进行相应的处理。 对于并发登录控制，可以使用读写锁（ReentrantReadWriteLock）来实现。在用户登录时，获取写锁，防止其他线程同时修改会话信息。在用户请求时，获取读锁，允许多个线程同时读取会话信息。 可以使用定时任务或心跳机制来定期清理过期的会话，以释放资源并保持会话的有效性。 需要根据具体的业务需求和系统架构选择适合的实现方式。同时，为了保证用户登录的安全性，还需考虑加密、防护攻击等安全措施。","categories":["Java面试"]},{"title":"LeetCode算法刷题录","path":"/2023/10/07/LeetCode算法刷题录/","content":"常用数据结构与算法，大厂笔试刷题记录。 一、数组1、最大子序和给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。子数组 是数组中的一个连续部分。 12345678910public int maxSubArray(int[] nums) &#123; int res = nums[0]; for (int i = 1; i &lt; nums.length; i++) &#123; // 遍历数组，将元素相加，若&gt;0加入结果集，否则重置。 nums[i] += Math.max(nums[i - 1], 0); // 将最新和值赋值nums数组，减少空间消费。 res = Math.max(res, nums[i]); &#125; return res; &#125; 2、两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。 123456789101112public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map=new HashMap&lt;&gt;(); for(int i=0;i&lt;nums.length;i++)&#123; if(map.containsKey(target-nums[i]))&#123; //本来想着应该先遍历一次存储nums数组元素到map中，其实不必要，一次遍历即可， //不存在再push， 否则判断另一元素是否也在，在遍历完一遍后，必然可以所有元素都存在map中。 return new int[]&#123;i,map.get(target-nums[i])&#125;; &#125; map.put(nums[i],i); &#125; return new int[0]; &#125; 3、合并两个有序数组给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。 请你 合并 nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。 123456789public void merge(int[] nums1, int m, int[] nums2, int n) &#123; int len = m + n - 1; int len1 = m - 1, len2 = n - 1; while (len1 &gt;= 0 &amp;&amp; len2 &gt;= 0) &#123; nums1[len--] = nums1[len1] &gt; nums2[len2] ? nums1[len1--] : nums2[len2--]; &#125; // 表示将nums2数组从下标0位置开始，拷贝到nums1数组中，从下标0位置开始，长度为len2+1 System.arraycopy(nums2, 0, nums1, 0, len2 + 1); &#125; 4、二分查找给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 1234567891011121314public int search(int[] nums, int target) &#123; int i = 0, j = nums.length - 1; while (i &lt;= j) &#123; int mid = i + (j - i) / 2; if (nums[mid] &lt; target) &#123; i = mid + 1; &#125; else if (nums[mid] &gt; target) &#123; j = mid - 1; &#125; else &#123; return mid; &#125; &#125; return -1; &#125; 5、移动零给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。 请注意 ，必须在不复制数组的情况下原地对数组进行操作。 1234567891011public void moveZeroes(int[] nums) &#123; int j = 0; for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] != 0) &#123; nums[j++] = nums[i]; &#125; &#125; while (j &lt; nums.length) &#123; nums[j++] = 0; &#125; &#125; 6、删除排序数组中的重复项给你一个 非严格递增排列 的数组 nums ，请你** 原地** 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。 123456789101112public int removeDuplicates(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; int index = 0; for (int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] != nums[index]) &#123; nums[++index] = nums[i]; &#125; &#125; return index + 1; &#125; 7、圆圈中最后剩下的数字从 0 号成员起开始计数，排在第 target 位的成员离开圆桌，且成员离开后从下一个成员开始计数。请返回游戏结束时最后一位成员的编号。 12345678910111213public int lastRemaining(int n, int m) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(n); for (int i = 0; i &lt; n; i++) &#123; list.add(i); &#125; int idx = 0; while (n &gt; 1) &#123; idx = (idx + m - 1) % n; list.remove(idx); n--; &#125; return list.get(0); &#125; 8、调整数组使奇数位于偶数前面123456789101112131415public int[] exchange(int[] nums) &#123; int i = 0, j = nums.length - 1; while (i &lt; j) &#123; while (i &lt; j &amp;&amp; nums[i] % 2 == 1) &#123; i++; &#125; while (i &lt; j &amp;&amp; nums[j] % 2 == 0) &#123; j--; &#125; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125; return nums; &#125; 9、三数之和给你一个整数数组 nums ，判断是否存在三元组 [nums[i], nums[j], nums[k]] 满足 i != j、i != k 且 j != k ，同时还满足 nums[i] + nums[j] + nums[k] == 0 。 12345678910111213141516171819202122232425262728293031public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Arrays.sort(nums); for (int i = 0; i &lt; nums.length - 2; i++) &#123; if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1]) &#123; continue; &#125; int left = i + 1; int right = nums.length - 1; while (left &lt; right) &#123; int sum = nums[i] + nums[left] + nums[right]; if (sum == 0) &#123; res.add(Arrays.asList(nums[i], nums[left], nums[right])); while (left &lt; right &amp;&amp; nums[left] == nums[left + 1]) &#123; left++; &#125; while (left &lt; right &amp;&amp; nums[right] == nums[right - 1]) &#123; right--; &#125; left++; right--; &#125; else if (sum &lt; 0) &#123; left++; &#125; else &#123; right--; &#125; &#125; &#125; return res; &#125; 10、快速排序手撕快速排序 12345678910111213141516171819202122232425262728293031323334// quickSort 方法是快速排序的入口。它接受一个整数数组 nums、一个起始索引 low 和一个结束索引 high。 // 如果 low 小于 high，则选择一个基准元素（通常选择最后一个元素）， // 将数组划分为两部分，使得左边的元素都小于基准元素，右边的元素都大于基准元素。然后递归地对左右两部分进行快速排序。 public void quickSort(int[] nums, int low, int high) &#123; if (low &lt; high) &#123; int pivotIndex = partition(nums, low, high); quickSort(nums, low, pivotIndex - 1); quickSort(nums, pivotIndex + 1, high); &#125; &#125; // partition 方法用于划分数组。它选择基准元素 pivot，并使用两个指针 i 和 j 遍历数组。 // 如果 nums[j] 小于 pivot，则将 nums[j] 与 nums[i] 交换， // 并将 i 向后移动一位。最后，将基准元素 pivot 与 nums[i+1] 交换， // 使得基准元素位于正确的位置，并返回基准元素的索引。 private int partition(int[] nums, int low, int high) &#123; int pivot = nums[high]; int i = low - 1; for (int j = low; j &lt; high; j++) &#123; if (nums[j] &lt; pivot) &#123; i++; swap(nums, i, j); &#125; &#125; swap(nums, i + 1, high); return i + 1; &#125; private void swap(int[] nums, int i, int j) &#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125; 11、合并区间以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间。 12345678910111213141516171819public int[][] merge(int[][] intervals) &#123; if(intervals==null||intervals.length&lt;=1)&#123; return intervals; &#125; Arrays.sort(intervals,(a,b)-&gt;Integer.compare(a[0],b[0])); List&lt;int[]&gt; merged=new ArrayList&lt;&gt;(); int[] currentInterval=intervals[0]; for (int i = 1; i &lt; intervals.length; i++) &#123; if(intervals[i][0]&lt;=currentInterval[1])&#123; currentInterval[1]=Math.max(currentInterval[1],intervals[i][1]); &#125;else&#123; merged.add(currentInterval); currentInterval=intervals[i]; &#125; &#125; merged.add(currentInterval); return merged.toArray(new int[merged.size()][]); &#125; 12、在排序数组中查找元素的左右边界给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。 如果数组中不存在目标值 target，返回 [-1, -1]。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public int[] searchRange(int[] nums, int target) &#123; int[] result = &#123;-1, -1&#125;; // 查找第一个位置 int first = findFirst(nums, target); if (first == -1) &#123; return result; // 未找到目标元素，返回初始结果 &#125; // 查找最后一个位置 int last = findLast(nums, target); result[0] = first; result[1] = last; return result; &#125; private int findFirst(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; int first = -1; while (left &lt;= right) &#123; int mid = left + (right - left) / 2; if (nums[mid] &gt;= target) &#123; right = mid - 1; if (nums[mid] == target) &#123; first = mid; // 更新第一个位置 &#125; &#125; else &#123; left = mid + 1; &#125; &#125; return first; &#125; private int findLast(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; int last = -1; while (left &lt;= right) &#123; int mid = left + (right - left) / 2; if (nums[mid] &lt;= target) &#123; left = mid + 1; if (nums[mid] == target) &#123; last = mid; // 更新最后一个位置 &#125; &#125; else &#123; right = mid - 1; &#125; &#125; return last; &#125; 13、下一个排列整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。 例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。 整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。 123456789101112131415161718192021222324252627282930313233343536373839404142434445461. 从右到左遍历给定的排列，找到第一个破坏升序的相邻元素对，记为i-1和i。即找到满足nums[i-1] &lt; nums[i]的i值。 2. 如果找不到这样的相邻元素对，说明给定排列已经是字典序最大的排列，将其翻转为最小排列即可。 3. 如果找到了相邻元素对，从右到左找到第一个大于nums[i-1]的元素，记为j。 4. 交换nums[i-1]和nums[j]。 5. 将从i开始到最右边的元素进行翻转，以得到下一个字典序更大的排列。 public void nextPermutation(int[] nums) &#123; int i = nums.length - 2; // 从右到左找到第一个破坏升序的相邻元素对 while (i &gt;= 0 &amp;&amp; nums[i] &gt;= nums[i + 1]) &#123; i--; &#125; if (i &gt;= 0) &#123; int j = nums.length - 1; // 从右到左找到第一个大于nums[i]的元素 while (j &gt;= 0 &amp;&amp; nums[j] &lt;= nums[i]) &#123; j--; &#125; // 交换nums[i]和nums[j] swap(nums, i, j); &#125; // 翻转从i+1到最右边的元素 reverse(nums, i + 1); &#125; private void swap(int[] nums, int i, int j) &#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125; private void reverse(int[] nums, int start) &#123; int i = start; int j = nums.length - 1; while (i &lt; j) &#123; swap(nums, i, j); i++; j--; &#125; &#125; 14、乘积最大子数组给你一个整数数组 nums ，请你找出数组中乘积最大的非空连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。 子数组 是数组的连续子序列。 1234567891011121314151617181920212223public int maxProduct(int[] nums) &#123; int maxProduct = nums[0]; //currentMax 和 currentMin 来分别记录以当前元素结尾的子数组的最大乘积和最小乘积 int currentMax = nums[0]; int currentMin = nums[0]; for (int i = 1; i &lt; nums.length; i++) &#123; //如果遇到负数，我们交换 currentMax 和 currentMin 的值，以保证在乘积计算中能够得到最大的乘积 if (nums[i] &lt; 0) &#123; int temp = currentMax; currentMax = currentMin; currentMin = temp; &#125; currentMax = Math.max(nums[i], currentMax * nums[i]); currentMin = Math.min(nums[i], currentMin * nums[i]); maxProduct = Math.max(maxProduct, currentMax); &#125; return maxProduct; &#125; 15、和为 K 的子数组给你一个整数数组 nums 和一个整数 k ，请你统计并返回 该数组中和为 k 的连续子数组的个数。 前缀和是一种常用于解决数组或序列问题的技巧，它通常用于快速计算某个区间内元素的和。前缀和的基本思想是将原数组的前缀和（或累积和）存储在一个辅助数组中，以便在常数时间内获取任意区间的和。 下面是前缀和的基本概念： 前缀和数组：前缀和数组是一个与原始数组具有相同长度的数组，其中每个元素存储了原始数组从开头到当前位置的所有元素的累积和。通常，前缀和数组的第一个元素为0，然后依次计算后续元素的值。 计算前缀和数组：计算前缀和数组的一种常见方法是使用迭代。从数组的第一个元素开始，依次计算前缀和数组中的每个元素。例如，如果原数组为nums，前缀和数组prefixSum可以如下计算： 1234prefixSum[0] = 0; // 第一个元素为0for (int i = 1; i &lt; nums.length; i++) &#123; prefixSum[i] = prefixSum[i - 1] + nums[i];&#125; 计算区间和：一旦有了前缀和数组，你可以用它来快速计算任意区间 [i, j] 内元素的和，其中 i 和 j 是数组的索引。区间 [i, j] 的和等于 prefixSum[j] - prefixSum[i - 1]。需要注意，如果 i 等于0，那么 prefixSum[i - 1] 通常被定义为0。 通过遍历数组并计算累积和，同时使用哈希表来记录每个前缀和出现的次数。当找到一个前缀和与当前和的差值等于k时，就说明存在一个子数组的和等于k，因此增加计数器count。最后返回count即可。 123456789101112131415public int subarraySum(int[] nums, int k) &#123; int count = 0; int sum = 0; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); map.put(0, 1); // 初始化前缀和为0的次数为1，用于处理数组中某个子数组的和正好等于k的情况 for (int i = 0; i &lt; nums.length; i++) &#123; sum += nums[i]; // 如果最新的前缀和-k的结果在map中，说明存在j-i这个连续区间和为k。 if (map.containsKey(sum - k)) &#123; count += map.get(sum - k); &#125; map.put(sum, map.getOrDefault(sum, 0) + 1); &#125; return count; &#125; 16、长度最小的子数组给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其总和大于等于 target 的长度最小的 连续子数，如果不存在，返回0。 12345678910111213141516public int minSubArrayLen(int target, int[] nums) &#123; int left = 0, right = 0; int sum = 0; int count = Integer.MAX_VALUE; while (right &lt; nums.length) &#123; sum += nums[right]; // 此处还得是while while (sum &gt;= target) &#123; count = Math.min(count, right - left + 1); sum -= nums[left]; left++; &#125; right++; &#125; return count == Integer.MAX_VALUE ? 0 : count; &#125; 二、字符串1、字符串相加给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和并同样以字符串形式返回。 123456789101112131415161718public String addStrings(String num1, String num2) &#123; StringBuilder res = new StringBuilder(); int m = num1.length() - 1, n = num2.length() - 1; int carry = 0; while (m &gt;= 0 || n &gt;= 0) &#123; int n1 = m &gt;= 0 ? num1.charAt(m) - &#x27;0&#x27; : 0; int n2 = n &gt;= 0 ? num2.charAt(n) - &#x27;0&#x27; : 0; int ans = n1 + n2 + carry; carry = ans / 10; res.append(ans % 10); m--; n--; &#125; if (carry == 1) &#123; res.append(carry); &#125; return res.reverse().toString(); &#125; 2、最长公共前缀编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 &quot;&quot;。 123456789101112131415public String longestCommonPrefix(String[] strs) &#123; if (strs == null || strs.length == 0) &#123; return &quot;&quot;; &#125; String prefix = strs[0]; for (String s : strs) &#123; while (s.indexOf(prefix) != 0) &#123; prefix = prefix.substring(0, prefix.length() - 1); if (prefix.isEmpty()) &#123; return &quot;&quot;; &#125; &#125; &#125; return prefix; &#125; 3、无重复字符的最长子串给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。 12345678910111213141516public int lengthOfLongestSubstring(String s) &#123; int maxLength = 0; int left = 0, right = 0; Set&lt;Character&gt; set = new HashSet&lt;&gt;(); while (right &lt; s.length()) &#123; if (!set.contains(s.charAt(right))) &#123; set.add(s.charAt(right)); maxLength = Math.max(maxLength, right - left + 1); right++; &#125; else &#123; set.remove(s.charAt(left)); left++; &#125; &#125; return maxLength; &#125; 4、最长回文子串给你一个字符串 s，找到 s 中最长的回文子串。 12345678910111213141516171819202122232425public String longestPalindrome(String s) &#123; if(s==null||s.length()&lt;2)&#123; return s; &#125; int start=0,end=0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); int len2 = expandAroundCenter(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; end - start) &#123; start = i - (len-1) / 2;; end = i + len / 2; &#125; &#125; return s.substring(start,end+1); &#125; private int expandAroundCenter(String s, int i, int j) &#123; while (i &gt;= 0 &amp;&amp; j &lt; s.length() &amp;&amp; s.charAt(i) == s.charAt(j)) &#123; i--; j++; &#125; return j - i - 1; &#125; 5、字符串相乘给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 1234567891011121314151617181920212223242526public String multiply(String num1, String num2) &#123; int m = num1.length(), n = num2.length(); int[] res = new int[m + n]; for (int i = m - 1; i &gt;= 0; i--) &#123; for (int j = n - 1; j &gt;= 0; j--) &#123; int mul = (num1.charAt(i) - &#x27;0&#x27;) * (num2.charAt(j) - &#x27;0&#x27;); // num1[i] 和 num2[j] 的乘积对应的就是 res[i+j] 和 res[i+j+1] 这两个位置. int p1 = i + j; // 进位 int p2 = i + j + 1; //当前位 int sum = mul + res[p2]; res[p1] += sum / 10; res[p2] = sum % 10; &#125; &#125; StringBuilder str = new StringBuilder(); for (int i = 0; i &lt; res.length; i++) &#123; if (i == 0 &amp;&amp; res[i] == 0) &#123; continue; &#125; str.append(res[i]); &#125; return str.toString(); &#125; 6、回文子串给你一个字符串 s ，请你统计并返回这个字符串中 回文子串 的数目。 12345678910111213141516171819202122public int countSubstrings(String s) &#123; int count = 0; for (int i = 0; i &lt; s.length(); i++) &#123; count += expandAroundCenter(s, i, i); // 以单个字符为中心扩展 count += expandAroundCenter(s, i, i + 1); // 以两个字符之间为中心扩展 &#125; return count; &#125; private int expandAroundCenter(String s, int left, int right) &#123; int count = 0; while (left &gt;= 0 &amp;&amp; right &lt; s.length() &amp;&amp; s.charAt(left) == s.charAt(right)) &#123; count++; left--; right++; &#125; return count; &#125; 三、栈1、有效的括号12345678910111213141516171819202122232425public boolean isValid(String s) &#123; //如果为奇数，肯定无效 if(s.length()%2==1)&#123; return false; &#125; Map&lt;Character,Character&gt; map=new HashMap&lt;&gt;(); map.put(&#x27;)&#x27;,&#x27;(&#x27;); map.put(&#x27;]&#x27;,&#x27;[&#x27;); map.put(&#x27;&#125;&#x27;,&#x27;&#123;&#x27;); Deque&lt;Character&gt; stack=new LinkedList&lt;&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char c=s.charAt(i); if(map.containsKey(c))&#123; if(stack.isEmpty()||stack.peek()!=map.get(c))&#123; return false; &#125; stack.pop(); &#125;else&#123; stack.push(c); &#125; &#125; return stack.isEmpty(); &#125; 2、用栈实现队列请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty） 1234567891011121314151617181920212223242526272829303132333435class MyQueue &#123; Stack&lt;Integer&gt; stack1; Stack&lt;Integer&gt; stack2; public MyQueue() &#123; stack1 = new Stack&lt;&gt;(); stack2 = new Stack&lt;&gt;(); &#125; public void push(int x) &#123; stack1.push(x); &#125; public int pop() &#123; if (stack2.isEmpty()) &#123; while (!stack1.isEmpty()) &#123; stack2.push(stack1.pop()); &#125; &#125; return stack2.pop(); &#125; public int peek() &#123; if(stack2.isEmpty())&#123; while (!stack1.isEmpty()) &#123; stack2.push(stack1.pop()); &#125; &#125; return stack2.peek(); &#125; public boolean empty() &#123; return stack1.isEmpty() &amp;&amp; stack2.isEmpty(); &#125; &#125; 3、最小栈设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。 实现 MinStack 类: MinStack() 初始化堆栈对象。 void push(int val) 将元素val推入堆栈。 void pop() 删除堆栈顶部的元素。 int top() 获取堆栈顶部的元素。 int getMin() 获取堆栈中的最小元素。 1234567891011121314151617181920212223242526272829class MinStack &#123; Stack&lt;Integer&gt; stack1; Stack&lt;Integer&gt; stack2; public MinStack() &#123; stack1=new Stack&lt;&gt;(); stack2=new Stack&lt;&gt;(); &#125; public void push(int val) &#123; stack1.push(val); if (stack2.isEmpty() || stack2.peek() &gt;= val) &#123; stack2.push(val); &#125; &#125; public void pop() &#123; if(stack2.peek().equals(stack1.pop()))&#123; //此处同时进行了stack1的pop操作 stack2.pop(); &#125; &#125; public int top() &#123; return stack1.peek(); &#125; public int getMin() &#123; return stack2.peek(); &#125; &#125; 4、验证出栈序列合法性给定 pushed 和 popped 两个序列，每个序列中的 值都不重复，只有当它们可能是在最初空栈上进行的推入 push 和弹出 pop 操作序列的结果时，返回 true；否则，返回 false 。 思路：（1）遍历入栈数组，并将其放入栈中；（2）栈不空，循环判断栈顶元素是否和出栈数组第一个元素相同，相同则出栈，并将指向出栈数组的索引+1；（3）最后判断栈是否为空，空则为合法出栈序列。 123456789101112public boolean validateStackSequences(int[] pushed, int[] popped) &#123; Stack&lt;Integer&gt; stack=new Stack&lt;&gt;(); int i=0; for(int c:pushed)&#123; stack.push(c); while(!stack.isEmpty()&amp;&amp;stack.peek()==popped[i])&#123; stack.pop(); i++; &#125; &#125; return stack.isEmpty(); &#125; 5、字符串解码给定一个经过编码的字符串，返回它解码后的字符串。 输入：s &#x3D; “3[a2[c]]”输出：”accaccacc” 输入：s &#x3D; “2[abc]3[cd]ef”输出：”abcabccdcdcdef” 123456789101112131415161718192021222324252627282930313233343536373839public String decodeString(String s) &#123; Stack&lt;Integer&gt; countStack = new Stack&lt;&gt;(); Stack&lt;String&gt; strStack = new Stack&lt;&gt;(); StringBuilder res = new StringBuilder(); int index = 0; while (index &lt; s.length()) &#123; char c = s.charAt(index); if (Character.isDigit(c)) &#123; int count = 0; // 此处获取字符前的重复数，可能&gt;10 while (Character.isDigit(s.charAt(index))) &#123; count = count * 10 + (s.charAt(index) - &#x27;0&#x27;); index++; &#125; countStack.push(count); &#125; else if (c == &#x27;[&#x27;) &#123; // 如果遇到左括号 [ ，则将当前解码的字符串压入 stringStack ，并重置 result 为一个新的空字符串 strStack.push(res.toString()); res.setLength(0); index++; &#125; else if (c == &#x27;]&#x27;) &#123; // 如果遇到右括号 ] ，则从 stringStack 中弹出一个字符串，并根据重复次数将 result 追加到弹出的字符串中 StringBuilder temp = new StringBuilder(strStack.pop()); // 此处由于定义的Stack&lt;String&gt;，因此直接弹出一次得到字符串。 int repeatTimes = countStack.pop(); for (int i = 0; i &lt; repeatTimes; i++) &#123; temp.append(res); &#125; res = temp; index++; &#125; else &#123; // 如果遇到其他字符，则直接将其追加到 result 中。 res.append(c); index++; &#125; &#125; return res.toString(); &#125; 6、每日温度给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。 思路：辅助栈存储数组下标 （1）遍历温度数组，入栈，并循环判断“栈不空，且当前数组元素是否大于栈顶元素，得到栈中元素到下一个更大元素的下标步长，即i-stack.pop()。 1234567891011121314public int[] dailyTemperatures(int[] T) &#123; int[] result = new int[T.length]; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for (int i = 0; i &lt; T.length; i++) &#123; while (!stack.isEmpty() &amp;&amp; T[i] &gt; T[stack.peek()]) &#123; int index = stack.pop(); result[index] = i - index; &#125; stack.push(i); &#125; return result;&#125; 7、旋转数组查找123456789101112131415161718192021222324252627282930public static int rotatedBinarySearch(int[] array, int target)&#123; int start = 0, end = array.length-1; while(start&lt;=end) &#123; int mid = start + (end-start)/2; if(array[mid]==target)&#123; return mid; &#125; //情况A：旋转点在中位数右侧 if(array[mid]&gt;=array[start]) &#123; //最左侧元素 &lt;= 查找目标 &lt; 中位数 if(array[mid]&gt;target &amp;&amp; array[start]&lt;=target)&#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; //情况B：旋转点在中位数左侧，或与中位数重合 else &#123; //中位数 &lt; 查找目标 &lt;= 最右侧元素 if(array[mid]&lt;target &amp;&amp; target&lt;=array[end])&#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; &#125; return -1;\t&#125; 四、链表1、反转链表给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 1234567891011public ListNode reverseList(ListNode head) &#123; ListNode pre=null; ListNode cur=head; while (cur!=null)&#123; ListNode temp=cur.next; cur.next=pre; pre=cur; cur=temp; &#125; return pre; &#125; 2、合并两个有序链表将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 1234567891011121314151617public ListNode mergeTwoLists(ListNode list1, ListNode list2) &#123; ListNode mergeList = new ListNode(-1); ListNode cur=mergeList; while (list1 != null &amp;&amp; list2 != null) &#123; if (list1.val &lt; list2.val) &#123; cur.next = list1; list1 = list1.next; &#125; else &#123; cur.next = list2; list2 = list2.next; &#125; cur = cur.next; &#125; cur.next = list1 != null ? list1 : list2; return mergeList.next; &#125; 3、环形链表给你一个链表的头节点 head ，判断链表中是否有环。 123456789101112public boolean hasCycle(ListNode head) &#123; ListNode fast = head, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next; if (fast == slow) &#123; return true; &#125; &#125; return false; &#125; 4、相交链表给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。 12345678public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode first = headA, second = headB; while (first != second) &#123; first = first != null ? first.next : headB; second = second != null ? second.next : headA; &#125; return first; &#125; 5、链表中倒数第k个结点获取链表中倒数第k个结点。 1234567891011public ListNode kthReverseNode(ListNode head, int k) &#123; ListNode fast = head, slow = head; while (k-- != 0) &#123; fast = fast.next; &#125; while (fast != null) &#123; fast = fast.next; slow = slow.next; &#125; return slow; &#125; 6、回文链表给你一个单链表的头节点 head ，请你判断该链表是否为回文链表。如果是，返回 true ；否则返回 false 。 123456789101112131415161718192021222324252627public boolean isPalindrome(ListNode head) &#123; if (head == null || head.next == null) &#123; return true; &#125; ListNode fast = head, slow = head; ListNode cur = head, pre = null; while (fast != null &amp;&amp; fast.next != null) &#123; //快慢指针寻找中点并反转前半部分 cur = slow; fast = fast.next.next; slow = slow.next; cur.next = pre; pre = cur; &#125; if (fast != null) &#123; slow = slow.next; //slow指向后半部分 &#125; while (slow != null &amp;&amp; pre != null) &#123; // 比较 if (slow.val != pre.val) &#123; return false; &#125; slow = slow.next; pre = pre.next; &#125; return true; &#125; 7、删除排序链表中的重复元素给定一个已排序的链表的头 head ， 删除所有重复的元素，使每个元素只出现一次 。返回 已排序的链表 。 1234567891011public ListNode deleteDuplicates(ListNode head) &#123; ListNode cur = head; while (cur != null &amp;&amp; cur.next != null) &#123; if (cur.val == cur.next.val) &#123; cur.next = cur.next.next; &#125; else &#123; cur = cur.next; &#125; &#125; return head; &#125; 8、指定区间反转链表 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left &lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。 1234567891011121314151617181920public ListNode reverseBetween(ListNode head, int left, int right) &#123; ListNode dummy=new ListNode(-1); dummy.next=head; ListNode pre=dummy; for(int i=1;i&lt;left;i++)&#123; pre=pre.next; &#125; // 在需要反转的区间里，每遍历到一个节点，让这个新节点来到反转部分的起始位置。 // //pre-cur-node-node.next ListNode cur=pre.next; for(int i=left;i&lt;right;i++)&#123; ListNode next=cur.next; cur.next=next.next; next.next=pre.next; pre.next=next; &#125; return dummy.next; &#125; 9、环形链表的入环结点给定一个链表的头节点 head ，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 123456789101112131415161718public ListNode detectCycle(ListNode head) &#123; ListNode fast = head, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; fast=fast.next.next; slow=slow.next; if (fast == slow) &#123; ListNode index1 = fast; ListNode index2 = head; while (index1 != index2) &#123; index1 = index1.next; index2 = index2.next; &#125; return index1; &#125; &#125; return null; &#125; 10、重排链表给定一个单链表 L 的头节点 head ，单链表 L 表示为： 1L0 → L1 → … → Ln - 1 → Ln 请将其重新排列后变为： 1L0 → Ln → L1 → Ln - 1 → L2 → Ln - 2 → … 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 10、重排链表 * 【思路】：1、找到中间结点，反转后半部分链表，然后交替合并。 */ public void reorderList(ListNode head) &#123; if (head == null) &#123; return; &#125; // 1、获得中间结点 ListNode mid = findMid(head); ListNode head2=mid.next; // 2、获得前半部分链表 mid.next=null; // 3、反转后半部分链表 head2=reverse(head2); // 4、合并 ListNode head1=head; mergeLists(head1,head2); &#125; private void mergeLists(ListNode head1, ListNode head2) &#123; while (head1 != null &amp;&amp; head2 != null) &#123; ListNode temp1 = head1.next; ListNode temp2 = head2.next; // 此处需要改变head1.next的值，因此需要上面的暂存操作。 head1.next = head2; head1 = temp1; head2.next = head1; head2 = temp2; &#125; &#125; private ListNode findMid(ListNode head) &#123; ListNode fast = head, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next; &#125; return slow; &#125; private ListNode reverse(ListNode head)&#123; ListNode pre=null; ListNode cur=head; while (cur!=null)&#123; ListNode temp=cur.next; cur.next=pre; pre=cur; cur=temp; &#125; return pre; &#125; 11、排序链表给你链表的头结点 head ，请将其按 升序 排列并返回 排序后的链表 。 123456789101112131415161718192021222324252627282930313233343536373839public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) &#123; return head; &#125; // 找到链表中点，将链表拆分为两个子链表 ListNode slow = head; ListNode fast = head; ListNode pre = null; while (fast != null &amp;&amp; fast.next != null) &#123; pre = slow; fast = fast.next.next; slow = slow.next; &#125; pre.next = null; // 递归对两个子链表进行排序 ListNode left = sortList(head); ListNode right = sortList(slow); // 合并两个有序链表 return merge(left, right); &#125; private ListNode merge(ListNode left, ListNode right) &#123; ListNode dummy = new ListNode(0); ListNode cur = dummy; while (left != null &amp;&amp; right != null) &#123; if (left.val &lt; right.val) &#123; cur.next = left; left = left.next; &#125; else &#123; cur.next = right; right = right.next; &#125; cur = cur.next; &#125; cur.next = left == null ? right : left; return dummy.next; &#125; 12、K 个一组翻转链表给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。 123456789101112131415161718192021222324252627282930public ListNode reverseKGroup(ListNode head, int k) &#123; if (head == null || head.next == null) &#123; return head; &#125; ListNode tail = head; for (int i = 0; i &lt; k; i++) &#123; if (tail == null) &#123; return head; &#125; tail = tail.next; &#125; ListNode newHead = reverse(head, tail); head.next = reverseKGroup(tail, k); return newHead; &#125; private ListNode reverse(ListNode head, ListNode tail) &#123; ListNode pre = null; ListNode cur = head; while (cur != tail) &#123; ListNode temp = cur.next; cur.next = pre; pre = cur; cur = temp; &#125; return pre; &#125; 13、奇偶链表给定单链表的头节点 head ，将所有索引为奇数的节点和索引为偶数的节点分别组合在一起，然后返回重新排序的列表。 1234567891011121314151617181920212223public ListNode oddEvenList(ListNode head) &#123; if (head == null || head.next == null) &#123; return head; &#125; // 分别定义奇偶结点 ListNode odd = head; ListNode even = head.next; // 暂存偶结点头部 ListNode temp = even; // 循环，even更快到尾部 while (even != null &amp;&amp; even.next != null) &#123; odd.next = even.next; odd = odd.next; even.next = odd.next; even = even.next; &#125; // 将所有奇结点的末尾接上偶结点的头 odd.next = temp; return head; &#125; 14、旋转链表给你一个链表的头节点 head ，旋转链表，将链表每个节点向右移动 k 个位置。 123456789101112131415161718192021222324252627public ListNode rotateRight(ListNode head, int k) &#123; if (head == null || head.next == null || k == 0) &#123; return head; &#125; // 计算链表长度并将尾节点与头节点相连 int length = 1; ListNode tail = head; while (tail.next != null) &#123; tail = tail.next; length++; &#125; tail.next = head; // 计算实际需要右移的步数 int steps = length - k % length; // 找到新的头节点和尾节点 ListNode newTail = tail; while (steps &gt; 0) &#123; newTail = newTail.next; head = head.next; steps--; &#125; newTail.next = null; return head; &#125; 15、分隔链表给你一个链表的头节点 head 和一个特定值 x ，请你对链表进行分隔，使得所有 小于 x 的节点都出现在 大于或等于 x 的节点之前。 你应当 保留 两个分区中每个节点的初始相对位置。 12345678910111213141516171819202122public ListNode partition(ListNode head, int x) &#123; ListNode smallerHead = new ListNode(0); ListNode greaterHead = new ListNode(0); ListNode smaller = smallerHead; ListNode greater = greaterHead; while (head != null) &#123; if (head.val &lt; x) &#123; smaller.next = head; smaller = smaller.next; &#125; else &#123; greater.next = head; greater = greater.next; &#125; head = head.next; &#125; greater.next = null; smaller.next = greaterHead.next; return smallerHead.next; &#125; 16、两数相加给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 2—&gt;4—&gt;6 1—&gt;2—&gt;5 结果： 3—&gt;6—&gt;1—&gt;1 1234567891011121314151617181920212223242526public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode dummy = new ListNode(-1); ListNode cur = dummy; int carry = 0; while (l1 != null || l2 != null) &#123; int val1 = l1 == null ? 0 : l1.val; int val2 = l2 == null ? 0 : l2.val; int sum = val1 + val2 + carry; carry = sum / 10; cur.next = new ListNode(sum % 10); cur = cur.next; if (l1 != null) &#123; l1 = l1.next; &#125; if (l2 != null) &#123; l2 = l2.next; &#125; &#125; if (carry == 1) &#123; cur.next = new ListNode(1); &#125; return dummy.next; &#125; 17、两两交换链表中的节点给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。 12345678910public ListNode swapPairs(ListNode head) &#123; if(head==null||head.next==null)&#123; return head; &#125; ListNode temp=head.next; head.next=swapPairs(temp.next); temp.next=head; return temp; &#125; 18、删除链表中的倒数第K个结点思路：需要定义一个哑结点，如删除第一个元素。 123456789101112131415161718192021public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode dummy = new ListNode(0); dummy.next = head; ListNode fast = dummy, slow = dummy; // 将快指针向前移动n步 for (int i = 0; i &lt;= n; i++) &#123; fast = fast.next; &#125; // 快慢指针一起向前移动，直到快指针到达链表末尾 while (fast != null) &#123; fast = fast.next; slow = slow.next; &#125; // 删除倒数第k个节点 slow.next = slow.next.next; return dummy.next; &#125; 五、二叉树1、二叉树的中序遍历给定一个二叉树的根节点 root ，返回 它的 中序 遍历 。 递归： 12345678910111213public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); inOrder(root,res); return res; &#125; private void inOrder(TreeNode root, List&lt;Integer&gt; res) &#123; if(root==null)&#123; return; &#125; inOrder(root.left,res); res.add(root.val); inOrder(root.right,res); &#125; 迭代： 1234567891011121314public List&lt;Integer&gt; inorderTraversalByIterator(TreeNode root) &#123; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); Deque&lt;TreeNode&gt; stack=new LinkedList&lt;&gt;(); while (root!=null||!stack.isEmpty())&#123; while (root!=null)&#123; stack.push(root); root=root.left; &#125; root=stack.pop(); res.add(root.val); root=root.right; &#125; return res; &#125; 2、二叉树的前序遍历给你二叉树的根节点 root ，返回它节点值的 前序 遍历。 递归： 1234567891011121314public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); preorder(root, result); return result; &#125; public void preorder(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) &#123; return; &#125; result.add(root.val); preorder(root.left, result); preorder(root.right, result); &#125; 迭代： 123456789101112131415161718192021public List&lt;Integer&gt; preOrderTraversalByIterator(TreeNode root) &#123; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); if(root==null)&#123; return res; &#125; Stack&lt;TreeNode&gt; stack=new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty())&#123; // 入栈，根右左，那么出栈就是根左右（前序） TreeNode node=stack.pop(); res.add(node.val); while (node.right!=null)&#123; stack.push(node.right); &#125; while (node.left!=null)&#123; stack.push(node.left); &#125; &#125; return res; &#125; 3、二叉树的后序遍历递归： 1234567891011121314public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); postorder(root, res); return res; &#125; void postorder(TreeNode root, List&lt;Integer&gt; list) &#123; if (root == null) &#123; return; &#125; postorder(root.left, list); postorder(root.right, list); list.add(root.val); &#125; 迭代： 123456789101112131415161718192021public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null)&#123; return result; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty())&#123; //入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果。 TreeNode node = stack.pop(); result.add(node.val); if (node.left != null)&#123; stack.push(node.left); &#125; if (node.right != null)&#123; stack.push(node.right); &#125; &#125; Collections.reverse(result); return result; &#125; 4、二叉树的层序遍历给你二叉树的根节点 root ，返回其节点值的 层序遍历 。 （即逐层地，从左到右访问所有节点）。 12345678910111213141516171819202122232425262728293031public static List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; int levelSize = queue.size(); List&lt;Integer&gt; currentLevel = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; levelSize; i++) &#123; TreeNode node = queue.poll(); currentLevel.add(node.val); if (node.left != null) &#123; queue.offer(node.left); &#125; if (node.right != null) &#123; queue.offer(node.right); &#125; &#125; result.add(currentLevel); &#125; return result; &#125; 5、二叉树的最大深度给定一个二叉树 root ，返回其最大深度。 二叉树的 最大深度 是指从根节点到最远叶子节点的最长路径上的节点数。 12345678public int maxDepth(TreeNode root) &#123; if(root==null)&#123; return 0; &#125; int left=maxDepth(root.left); int right=maxDepth(root.right); return Math.max(left,right)+1; &#125; 6、平衡二叉树给定一个二叉树，判断它是否是高度平衡的二叉树。 123456789public boolean isBalanced(TreeNode root) &#123; if (root == null) return true; return Math.abs(depth(root.left) - depth(root.right)) &lt;= 1 &amp;&amp; isBalanced(root.left) &amp;&amp; isBalanced(root.right); &#125; private int depth(TreeNode root) &#123; if (root == null) return 0; return Math.max(depth(root.left), depth(root.right)) + 1; &#125; 7、对称二叉树给你一个二叉树的根节点 root ， 检查它是否轴对称。 12345678910111213141516public boolean isSymmetric(TreeNode root) &#123; if (root == null) &#123; return true; &#125; return symmetric(root.left, root.right); &#125; private boolean symmetric(TreeNode left, TreeNode right) &#123; if (left == null &amp;&amp; right == null) &#123; return true; &#125; if (left == null || right == null || left.val != right.val) &#123; return false; &#125; return symmetric(left.left, right.right) &amp;&amp; symmetric(left.right, right.left); &#125; 8、二叉树的直径给你一棵二叉树的根节点，返回该树的 直径 。 二叉树的 直径 是指树中任意两个节点之间最长路径的 长度 。这条路径可能经过也可能不经过根节点 root 。 两节点之间路径的 长度 由它们之间边数表示。 1234567891011121314public int diameterOfBinaryTree(TreeNode root) &#123; getHeight(root); return res; &#125; private int getHeight(TreeNode root)&#123; if(root==null)&#123; return 0; &#125; int left=getHeight(root.left); int right=getHeight(root.right); res=Math.max(res,left+right); return Math.max(left,right)+1; &#125; 9、路径总和给你二叉树的根节点 root 和一个表示目标和的整数 targetSum 。判断该树中是否存在 根节点到叶子节点 的路径，这条路径上所有节点值相加等于目标和 targetSum 。 123456789public boolean hasPathSum(TreeNode root, int targetSum) &#123; if (root == null) &#123; return false; &#125; if (targetSum - root.val == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) &#123; return true; &#125; return hasPathSum(root.left, targetSum - root.val) || hasPathSum(root.right, targetSum - root.val); &#125; 10、翻转二叉树给你一棵二叉树的根节点 root ，翻转这棵二叉树，并返回其根节点。 相当于对每个结点，交换其左右结点。 1234567891011121314151617181920public TreeNode invertTree(TreeNode root) &#123; if (root == null) &#123; return null; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; TreeNode node = queue.poll(); if (node.left != null) &#123; queue.offer(node.left); &#125; if (node.right != null) &#123; queue.offer(node.right); &#125; TreeNode temp = node.left; node.left = node.right; node.right = temp; &#125; return root; &#125; 11、另一个树的子树给你两棵二叉树 root 和 subRoot 。检验 root 中是否包含和 subRoot 具有相同结构和节点值的子树。 12345678910111213141516public boolean isSubtree(TreeNode root, TreeNode subRoot) &#123; if (root == null) &#123; return false; &#125; return isSubtree(root.left,subRoot)||isSubtree(root.right,subRoot)||dfs(root,subRoot); &#125; private boolean dfs(TreeNode root, TreeNode subRoot) &#123; if(subRoot==null&amp;&amp;root==null)&#123; return true; &#125; if(root==null||subRoot==null||subRoot.val!=root.val)&#123; return false; &#125; return dfs(root.left,subRoot.left)&amp;&amp;dfs(root.right,subRoot.right); &#125; 11、二叉树的所有路径给你一个二叉树的根节点 root ，按 任意顺序 ，返回所有从根节点到叶子节点的路径。 1234567891011121314151617181920212223public List&lt;List&lt;Integer&gt;&gt; binaryTreePaths(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; paths = new ArrayList&lt;&gt;(); if (root == null) &#123; return paths; &#125; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); backTrack(root, paths, list); return paths; &#125; private void backTrack(TreeNode root, List&lt;List&lt;Integer&gt;&gt; path, List&lt;Integer&gt; list) &#123; if (root == null) &#123; return; &#125; list.add(root.val); if (root.left == null &amp;&amp; root.right == null) &#123; path.add(new ArrayList&lt;&gt;(list)); &#125; else &#123; backTrack(root.left, path, list); backTrack(root.right, path, list); &#125; list.remove(list.size() - 1); &#125; 12、二叉树的右视图给定一个二叉树的 根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 12345678910111213141516171819202122232425262728public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; int size = queue.size(); for (int i = 0; i &lt; size; i++) &#123; TreeNode node = queue.poll(); if (node.left != null) &#123; queue.add(node.left); &#125; if (node.right != null) &#123; queue.add(node.right); &#125; if (i == size - 1) &#123; result.add(node.val); &#125; &#125; &#125; return result; &#125; 13、验证二叉搜索树给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树。 1234567891011121314151617long pre = Long.MIN_VALUE;public boolean isValidBST(TreeNode root) &#123; if (root == null) &#123; return true; &#125; // 访问左子树 if (!isValidBST(root.left)) &#123; return false; &#125; // 访问当前节点：如果当前节点小于等于中序遍历的前一个节点，说明不满足BST，返回 false；否则继续遍历。 if (root.val &lt;= pre) &#123; return false; &#125; pre = root.val; // 访问右子树 return isValidBST(root.right);&#125; 14、二叉搜索树的最近公共祖先给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 123456789101112public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; while(root!=null)&#123; if(p.val&lt;root.val&amp;&amp;q.val&lt;root.val)&#123; root=root.left; &#125;else if(p.val&gt;root.val&amp;&amp;q.val&gt;root.val)&#123; root=root.right; &#125;else&#123; return root; &#125; &#125; return root; &#125; 15、二叉树的最近公共祖先给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 12345678public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left == null) return right; if(right == null) return left; return root; &#125; 16、序列化与反序列化二叉树请实现两个函数，分别用来序列化和反序列化二叉树，不对序列化之后的字符串进行约束，但要求能够根据序列化之后的字符串重新构造出一棵与原二叉树相同的树。 二叉树的序列化(Serialize)是指：把一棵二叉树按照某种遍历方式的结果以某种格式保存为字符串。 二叉树的反序列化(Deserialize)是指：根据某种遍历顺序得到的序列化字符串结果str，重构二叉树。 123456789101112131415161718192021222324int index=-1;String Serialize(TreeNode root) &#123; if(root==null)&#123; return &quot;#&quot;; &#125;else&#123; return root.val+&quot;,&quot;+Serialize(root.left)+&quot;,&quot;+Serialize(root.right); &#125; &#125;TreeNode Deserialize(String str) &#123; String[] s=str.split(&quot;,&quot;); index++; int len=s.length; if(index&gt;len)&#123; return null; &#125; TreeNode node=null; if(!s[index].equals(&quot;#&quot;))&#123; node=new TreeNode(Integer.parseInt(s[index])); node.left=Deserialize(str); node.right=Deserialize(str); &#125; return node;&#125; 17、构造二叉树（1）从前序与中序遍历序列构造二叉树 1234567891011121314151617181920212223public TreeNode buildTree(int[] preorder, int[] inorder) &#123; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; map.put(inorder[i], i); &#125; return buildTreeHelper(preorder, 0, preorder.length, inorder, 0, inorder.length, map); &#125; private TreeNode buildTreeHelper(int[] preorder, int preStart, int preEnd, int[] inorder, int inStart, int inEnd, HashMap&lt;Integer, Integer&gt; map) &#123; if (preStart &gt; preEnd || inStart &gt; inEnd) &#123; return null; &#125; int rootVal = preorder[preStart]; TreeNode root = new TreeNode(rootVal); //获取根结点在中序中的索引 int rootIndex = map.get(rootVal); int leftSize = rootIndex - inStart; root.left = buildTreeHelper(preorder, preStart + 1, preStart + leftSize, inorder, inStart, rootIndex - 1, map); root.right = buildTreeHelper(preorder, preStart + leftSize + 1, preEnd, inorder, rootIndex + 1, inEnd, map); return root; &#125; （2）从后序与中序遍历序列构造二叉树 12345678910111213141516171819202122public TreeNode buildTree(int[] inorder, int[] postorder) &#123; HashMap&lt;Integer,Integer&gt; map=new HashMap&lt;&gt;(); for(int i=0;i&lt;inorder.length;i++)&#123; map.put(inorder[i],i); &#125; return buildTreeHelper(inorder,0,inorder.length-1,postorder,0,postorder.length-1,map); &#125; public TreeNode buildTreeHelper(int[] inorder,int i_start,int i_end,int[] postorder,int p_start,int p_end,HashMap&lt;Integer,Integer&gt; map)&#123; if(p_start&gt;p_end||i_start&gt;i_end)&#123; return null; &#125; int root_val=postorder[p_end]; TreeNode root=new TreeNode(root_val); int i_root_index=map.get(root_val); int leftnum=i_root_index-i_start; root.left=buildTreeHelper(inorder,i_start,i_root_index-1,postorder,p_start,p_start+leftnum-1,map); root.right=buildTreeHelper(inorder,i_root_index+1,i_end,postorder,p_start+leftnum,p_end-1,map); return root; &#125; 18、有序数组转换为二叉搜索树给你一个整数数组 nums ，其中元素已经按 升序 排列，请你将其转换为一棵 高度平衡 二叉搜索树。 123456789101112131415161718public TreeNode sortedArrayToBST(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return null; &#125; return dfs(nums, 0, nums.length - 1); &#125; private TreeNode dfs(int[] nums, int i, int j) &#123; if (i &gt; j) &#123; return null; &#125; int mid = i + (j - i) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = dfs(nums, i, mid - 1); root.right = dfs(nums, mid + 1, j); return root; &#125; 19、求根节点到叶节点数字之和给你一个二叉树的根节点 root ，树中每个节点都存放有一个 0 到 9 之间的数字。 每条从根节点到叶节点的路径都代表一个数字： 例如，从根节点到叶节点的路径 1 -&gt; 2 -&gt; 3 表示数字 123 。 计算从根节点到叶节点生成的 所有数字之和 。 12345678910111213141516171819public int sumNumbers(TreeNode root) &#123; return dfs(root, 0); &#125; private int dfs(TreeNode node, int sum) &#123; if (node == null) &#123; return 0; &#125; int currentSum = sum * 10 + node.val; if (node.left == null &amp;&amp; node.right == null) &#123; return currentSum; &#125; int leftSum = dfs(node.left, currentSum); int rightSum = dfs(node.right, currentSum); return leftSum + rightSum; &#125; 20、路径总和 II给你二叉树的根节点 root 和一个整数目标和 targetSum ，找出所有 从根节点到叶子节点 路径总和等于给定目标和的路径。 1234567891011121314151617181920212223public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int targetSum) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); backtrack(res, list, root, targetSum); return res; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; res, List&lt;Integer&gt; list, TreeNode root, int targetSum) &#123; if (root == null) &#123; return; &#125; list.add(root.val); targetSum -= root.val; if (targetSum == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) &#123; res.add(new ArrayList&lt;&gt;(list)); &#125; backtrack(res, list, root.left, targetSum);//上面有判空，此处省略 backtrack(res, list, root.right, targetSum); list.remove(list.size() - 1); &#125; 21、不同的二叉搜索树给你一个整数 n ，求恰由 n 个节点组成且节点值从 1 到 n 互不相同的 二叉搜索树 有多少种？返回满足题意的二叉搜索树的种数。 123456789101112131415public int numTrees(int n) &#123; //初始化 dp 数组 int[] dp = new int[n + 1]; //初始化0个节点和1个节点的情况 dp[0] = 1; dp[1] = 1; for (int i = 2; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= i; j++) &#123; //对于第i个节点，需要考虑1作为根节点直到i作为根节点的情况，所以需要累加 //一共i个节点，对于根节点j时,左子树的节点个数为j-1，右子树的节点个数为i-j dp[i] += dp[j - 1] * dp[i - j]; &#125; &#125; return dp[n]; &#125; 六、贪心算法1、买卖股票的最佳时机 I给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 12345678public int maxProfit(int[] prices) &#123; int cost = prices[0], profit = 0; for (int price : prices) &#123; cost = Math.min(cost, price); profit = Math.max(profit, price - cost); &#125; return profit; &#125; 2、买卖股票的最佳时机 II给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。 在每一天，你可以决定是否购买和&#x2F;或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。返回 你能获得的 最大 利润。 12345678910public int maxProfit(int[] prices) &#123; int profit = 0; for (int i = 1; i &lt; prices.length; i++) &#123; int temp = prices[i] - prices[i - 1]; if (temp &gt; 0) &#123; profit += temp; &#125; &#125; return profit; &#125; 七、动态规划1、爬楼梯假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 123456789public int climbStairs(int n) &#123; int[] dp=new int[n+1]; dp[0]=1; dp[1]=1; for(int i=2;i&lt;=n;i++)&#123; dp[i]=dp[i-1]+dp[i-2]; &#125; return dp[n]; &#125; 2、最长递增子序列给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列 是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 思路：动态规划，初始化dp数组默认值1。 （1）双层遍历，i从1到nums.length，j从0到i，如果nums[j] &lt; nums[i]，更新dp[i] = Math.max(dp[i], dp[j] + 1) （2）外层循环更新maxLen = Math.max(maxLen, dp[i]) 12345678910111213141516171819public int lengthOfLIS(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; int[] dp = new int[nums.length]; dp[0] = 1; Arrays.fill(dp, 1); int maxLen = 1; for (int i = 1; i &lt; nums.length; i++) &#123; for (int j = 0; j &lt; i; j++) &#123; if (nums[j] &lt; nums[i]) &#123; dp[i] = Math.max(dp[i], dp[j] + 1); &#125; &#125; maxLen = Math.max(maxLen, dp[i]); &#125; return maxLen; &#125; 3、最长公共子序列给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。 1234567891011121314151617public int longestCommonSubsequence(String text1, String text2) &#123; int m = text1.length(), n = text2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i &lt;= m; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; char c1 = text1.charAt(i-1); char c2 = text2.charAt(j-1); if (c1 == c2) &#123; dp[i][j] = dp[i - 1][j - 1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); &#125; &#125; &#125; return dp[m][n]; &#125; 4、打家劫舍（1）一排 12345678910111213public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) return 0; if (nums.length == 1) return nums[0]; int[] dp = new int[nums.length]; dp[0] = nums[0]; dp[1] = Math.max(dp[0], nums[1]); for (int i = 2; i &lt; nums.length; i++) &#123; dp[i] = Math.max(dp[i - 1], dp[i - 2] + nums[i]); &#125; return dp[nums.length - 1];\t&#125; （2）圆圈 123456789101112131415161718192021222324public int rob(int[] nums) &#123; if (nums == null || nums.length == 0) &#123; return 0; &#125; if (nums.length == 1) &#123; return nums[0]; &#125; // 分别计算从第一间房屋到倒数第二间房屋和从第二间房屋到最后一间房屋的最大金额 int max1 = robRange(nums, 0, nums.length - 2); int max2 = robRange(nums, 1, nums.length - 1); // 返回两种情况下的最大金额 return Math.max(max1, max2); &#125; private int robRange(int[] nums, int start, int end) &#123; int prevMax = 0; // 前一间房屋的最大金额 int currMax = 0; // 当前房屋的最大金额 for (int i = start; i &lt;= end; i++) &#123; int temp = currMax; currMax = Math.max(prevMax + nums[i], currMax); prevMax = temp; &#125; return currMax; &#125; 5、最小路径和给定一个包含非负整数的 *m* x *n* 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 1234567891011121314151617181920212223public int minPathSum(int[][] grid) &#123; if (grid == null || grid.length == 0) &#123; return 0; &#125; int m = grid.length, n = grid[0].length; int[][] dp = new int[m][n]; dp[0][0] = grid[0][0]; for (int i = 1; i &lt; m; i++) &#123; dp[i][0] = dp[i - 1][0] + grid[i][0]; &#125; for (int j = 1; j &lt; n; j++) &#123; dp[0][j] = dp[0][j - 1] + grid[0][j]; &#125; for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; dp[i][j] = Math.min(dp[i - 1][j], dp[i][j - 1]) + grid[i][j]; &#125; &#125; return dp[m - 1][n - 1]; &#125; 6、最大正方形在一个由 &#39;0&#39; 和 &#39;1&#39; 组成的二维矩阵内，找到只包含 &#39;1&#39; 的最大正方形，并返回其面积。 123456789101112131415161718192021222324// 思路：我们用 dp(i, j)表示以 (i, j) 为右下角，且只包含 1 的正方形的边长最大值。// 如果该位置的值是 1，则 dp(i, j) 的值由其上方、左方和左上方的三个相邻位置的 dp值决定。具体而言，当前位置的元素值等于三个相邻位置的元素中的最小值加 1public int maximalSquare(char[][] matrix) &#123; int row = matrix.length, col = matrix[0].length; if (matrix == null || row == 0 || col == 0) &#123; return 0; &#125; int[][] dp = new int[row][col]; int maxSide = 0; for (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; col; j++) &#123; if (matrix[i][j] == &#x27;1&#x27;) &#123; if (i == 0 || j == 0) &#123; dp[i][j] = 1; &#125; else &#123; dp[i][j] = Math.min(Math.min(dp[i - 1][j], dp[i][j - 1]), dp[i - 1][j - 1]) + 1; &#125; maxSide = Math.max(maxSide, dp[i][j]); &#125; &#125; &#125; return maxSide * maxSide; &#125; 7、最长重复子数组给两个整数数组 nums1 和 nums2 ，返回 两个数组中 公共的 、长度最长的子数组的长度 。 12345678910111213public int findLength(int[] nums1, int[] nums2) &#123; int m = nums1.length, n = nums2.length; int[][] dp = new int[m + 1][n + 1]; int res = 0; for (int i = m - 1; i &gt;= 0; i--) &#123; for (int j = n - 1; j &gt;= 0; j--) &#123; dp[i][j] = nums1[i] == nums2[j] ? dp[i + 1][j + 1] + 1 : 0; res = Math.max(res, dp[i][j]); &#125; &#125; return res; &#125; 8、零钱兑换给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数。 123456789101112131415161718public int coinChange(int[] coins, int amount) &#123; //创建一个长度为 amount + 1 的 dp 数组，初始值都设置为 amount + 1 （表示不可能的情况） int[] dp=new int[amount+1]; Arrays.fill(dp,amount+1); //将 dp[0] 设置为0，表示兑换金额为0时不需要任何硬币。 dp[0]=0; //遍历金额从1到 amount ，并尝试使用每种硬币来兑换。 for(int i=1;i&lt;=amount;i++)&#123; for(int coin:coins)&#123; if(coin&lt;=i)&#123; dp[i]=Math.min(dp[i],dp[i-coin]+1); &#125; &#125; &#125; //如果其值大于 amount ，则表示无法兑换，返回-1。 return dp[amount]&gt;amount?-1:dp[amount]; &#125; 9、完全平方数给你一个整数 n ，返回和为 n 的完全平方数的最少数量。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数。 12345678910111213public int numSquares(int n) &#123; int dp[] = new int[n + 1]; Arrays.fill(dp, Integer.MAX_VALUE); dp[0] = 0; //依次求出 1, 2... 直到 n 的解 for (int i = 1; i &lt;= n; i++) &#123; //依次减去一个平方数 for (int j = 1; j * j &lt;= i; j++) &#123; dp[i] = Math.min(dp[i], dp[i - j * j] + 1); &#125; &#125; return dp[n];&#125; 八、深度&#x2F;广度优先遍历1、岛屿数量给你一个由 &#39;1&#39;（陆地）和 &#39;0&#39;（水）组成的的二维网格，请你计算网格中岛屿的数量。 123456789101112131415161718192021222324public int numIslands(char[][] grid) &#123; int count = 0; for (int i = 0; i &lt; grid.length; i++) &#123; for (int j = 0; j &lt; grid[0].length; j++) &#123; if (grid[i][j] == &#x27;1&#x27;) &#123; dfs(grid, i, j); count++; &#125; &#125; &#125; return count; &#125; private void dfs(char[][] grid, int i, int j) &#123; if (i &lt; 0 || i &gt;= grid.length || j &lt; 0 || j &gt;= grid[0].length || grid[i][j] == &#x27;0&#x27;) &#123; return; &#125; //将已经访问过的岛屿单元格标记为0，以避免重复计算。 grid[i][j] = &#x27;0&#x27;; dfs(grid, i + 1, j); dfs(grid, i, j + 1); dfs(grid, i - 1, j); dfs(grid, i, j - 1); &#125; 2、括号生成数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。 123456789101112131415161718public List&lt;String&gt; generateParenthesis(int n) &#123; List&lt;String&gt; res=new ArrayList&lt;&gt;(); dfs(n,n,res,&quot;&quot;); return res; &#125; private void dfs(int left, int right, List&lt;String&gt; res, String cur) &#123; if (left == 0 &amp;&amp; right == 0) &#123; res.add(cur); return; &#125; if (left &gt; 0) &#123; dfs(left - 1, right, res, cur + &quot;(&quot;); &#125; if (right &gt; left) &#123; dfs(left, right - 1, res, cur + &quot;)&quot;); &#125; &#125; 3、太平洋大西洋水流问题有一个 m × n 的矩形岛屿，与 太平洋 和 大西洋 相邻。 “太平洋” 处于大陆的左边界和上边界，而 “大西洋” 处于大陆的右边界和下边界。 返回既能流向太平洋，也能流向大西洋的数字下标list。 12345678910111213141516171819202122232425262728293031323334353637383940public List&lt;List&lt;Integer&gt;&gt; pacificAndAtlantic(int[][] matrix)&#123; if(matrix==null||matrix.length==0)&#123; return new ArrayList&lt;&gt;(); &#125; int m=matrix.length; int n=matrix[0].length; int[][] pacific=new int[m][n]; int[][] atlantic=new int[m][n]; for(int i=0;i&lt;m;i++)&#123; for(int j=0;j&lt;n;j++)&#123; if(i==0||j==0)&#123; dfs(matrix,pacific,i,j,matrix[i][j]); &#125; if(i==m-1||j==n-1)&#123; dfs(matrix,atlantic,i,j,matrix[i][j]); &#125; &#125; &#125; List&lt;List&lt;Integer&gt;&gt; res=new ArrayList&lt;&gt;(); for(int i=0;i&lt;m;i++)&#123; for(int j=0;j&lt;n;j++)&#123; if(pacific[i][j]==1&amp;&amp;atlantic[i][j]==1)&#123; res.add(Arrays.asList(i,j)); &#125; &#125; &#125; return res; &#125; public void dfs(int[][] matrix,int[][] ocean,int i,int j,int pre)&#123; if(i&lt;0 || i&gt;=matrix.length || j&lt;0 || j&gt;=matrix[0].length || ocean[i][j]==1 || matrix[i][j]&lt;pre)&#123; return; &#125; ocean[i][j]=1; dfs(matrix, ocean, i - 1, j, matrix[i][j]); dfs(matrix, ocean, i + 1, j, matrix[i][j]); dfs(matrix, ocean, i, j - 1, matrix[i][j]); dfs(matrix, ocean, i, j + 1, matrix[i][j]); &#125; 4、被围绕的区域给你一个 m x n 的矩阵 board ，由若干字符 &#39;X&#39; 和 &#39;O&#39; ，找到所有被 &#39;X&#39; 围绕的区域，并将这些区域里所有的 &#39;O&#39; 用 &#39;X&#39; 填充。 12345678910111213141516171819202122232425262728293031323334353637public void sounded(char[][] board)&#123; if(board==null||board.length==0)&#123;//非常重要 return; &#125; int m=board.length; int n=board[0].length; for(int i=0;i&lt;m;i++)&#123; for(int j=0;j&lt;n;j++)&#123; boolean isEdge=(i==0||i==m-1||j==0||j==n-1); if(isEdge&amp;&amp;board[i][j]==&#x27;O&#x27;)&#123; dfs(board,i,j); &#125; &#125; &#125; for(int i=0;i&lt;m;i++)&#123; for(int j=0;j&lt;n;j++)&#123; if(board[i][j]==&#x27;O&#x27;)&#123; board[i][j]=&#x27;X&#x27;; &#125; if(board[i][j]==&#x27;#&#x27;)&#123; board[i][j]=&#x27;O&#x27;; &#125; &#125; &#125; &#125; public void dfs(char[][] board,int i,int j)&#123; //注意此处多个或运算符连接需空隔好。 if(i &lt; 0 || j &lt; 0 || i &gt;= board.length || j &gt;= board[0].length || board[i][j] == &#x27;X&#x27; || board[i][j] == &#x27;#&#x27;)&#123; return; &#125; // 将边界上的O临时替换为# board[i][j]=&#x27;#&#x27;; dfs(board,i+1,j); dfs(board,i-1,j); dfs(board,i,j+1); dfs(board,i,j-1); &#125; 九、回溯算法1、全排列（1）给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列。你可以 按任意顺序 返回答案。 思路：回溯 （1）list的大小和nums数组长度相同时，加入list到结果集res。 （2）for循环遍历数组，如果list存在nums[i]，跳过该数，并进行常规添加-回溯下一层-撤销操作。 1234567891011121314151617181920public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); backTrack(nums, res, list); return res; &#125; private void backTrack(int[] nums, List&lt;List&lt;Integer&gt;&gt; res, List&lt;Integer&gt; list) &#123; if (list.size() == nums.length) &#123; res.add(new ArrayList&lt;&gt;(list)); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; if(list.contains(nums[i]))&#123; continue; &#125; list.add(nums[i]); backTrack(nums, res, list); list.remove(list.size() - 1); &#125; &#125; （2）给定一个可包含重复数字的序列 nums ，按任意顺序 返回所有不重复的全排列。 思路：由于存在重复元素，添加boolean[] used数组记录当前数字是否用过。 （1）为方便去重，先对nums进行排序。 （2）在数组循环中，跳过重复元素或已使用的元素 12345678910111213141516171819202122232425262728public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); boolean[] used = new boolean[nums.length]; Arrays.sort(nums); backtrack(res, list, nums, used); return res; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; res, List&lt;Integer&gt; list, int[] nums, boolean[] used) &#123; if (list.size() == nums.length) &#123; res.add(new ArrayList&lt;&gt;(list)); return; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; //如果当前数字已经被使用过，或者前一个相同的数字未被使用，则跳过当前数字。 if (used[i] || (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i - 1])) &#123; continue; &#125; used[i] = true; list.add(nums[i]); backtrack(res, list, nums, used); used[i] = false; list.remove(list.size() - 1); &#125; &#125; 2、子集（1）给定一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集 12345678910111213141516public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); List&lt;Integer&gt; currentSubset = new ArrayList&lt;&gt;(); // 子集，不需要包括所有元素，因此传入一个起始索引 backtrack(nums, 0, currentSubset, result); return result;&#125;private void backtrack(int[] nums, int start, List&lt;Integer&gt; currentSubset, List&lt;List&lt;Integer&gt;&gt; result) &#123; result.add(new ArrayList&lt;&gt;(currentSubset)); for (int i = start; i &lt; nums.length; i++) &#123; currentSubset.add(nums[i]); backtrack(nums, i + 1, currentSubset, result); currentSubset.remove(currentSubset.size() - 1); &#125;&#125; （2）给你一个整数数组 nums ，其中可能包含重复元素，请你返回该数组所有可能的子集。 123456789101112131415161718public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); Arrays.sort(nums); // 对数组进行排序，以便处理重复元素 backtrack(nums, 0, new ArrayList&lt;&gt;(), result); return result;&#125;private void backtrack(int[] nums, int start, List&lt;Integer&gt; currentSubset, List&lt;List&lt;Integer&gt;&gt; result) &#123; result.add(new ArrayList&lt;&gt;(currentSubset)); for (int i = start; i &lt; nums.length; i++) &#123; if (i &gt; start &amp;&amp; nums[i] == nums[i - 1]) &#123; continue; // 跳过重复元素，避免生成重复的子集 &#125; currentSubset.add(nums[i]); backtrack(nums, i + 1, currentSubset, result); currentSubset.remove(currentSubset.size() - 1); &#125;&#125; 3、单词搜索给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。 1234567891011121314151617181920212223242526272829303132333435363738394041public boolean exist(char[][] board, String word) &#123; if (board == null || board.length == 0 || word == null || word.length() == 0) &#123; return false; &#125; int rows = board.length; int cols = board[0].length; boolean[][] visited = new boolean[rows][cols]; for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; if (board[i][j] == word.charAt(0) &amp;&amp; backtrack(board, word, i, j, 0, visited)) &#123; return true; &#125; &#125; &#125; return false; &#125; private boolean backtrack(char[][] board, String word, int row, int col, int index, boolean[][] visited) &#123; if (index == word.length()) &#123; return true; &#125; if (row &lt; 0 || col &lt; 0 || row &gt;= board.length || col &gt;= board[0].length || visited[row][col] || board[row][col] != word.charAt(index)) &#123; return false; &#125; visited[row][col] = true; boolean result = backtrack(board, word, row + 1, col, index + 1, visited) || backtrack(board, word, row - 1, col, index + 1, visited) || backtrack(board, word, row, col + 1, index + 1, visited) || backtrack(board, word, row, col - 1, index + 1, visited); visited[row][col] = false; return result; &#125; 4、组合（1）给你一个 无重复元素 的整数数组 candidates 和一个目标整数 target ，找出 candidates 中可以使数字和为目标数 target 的 所有 不同组合。 candidates 中的 同一个 数字可以 无限制重复被选取 。 1234567891011121314151617181920public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; List&lt;List&lt;Integer&gt;&gt; combinations=new ArrayList&lt;&gt;(); List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); backtrack(combinations,res,candidates,target,0); return combinations; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; combinations,List&lt;Integer&gt; res,int[] candidates,int target,int start)&#123; if(target==0)&#123; combinations.add(new ArrayList&lt;&gt;(res)); return; &#125; for(int i=start;i&lt;candidates.length;i++)&#123; if(candidates[i]&lt;=target)&#123; res.add(candidates[i]); backtrack(combinations,res,candidates,target-candidates[i],i); res.remove(res.size()-1); &#125; &#125; &#125; （2）给定两个整数 n 和 k，返回范围 [1, n] 中所有可能的 k 个数的组合。 12345678910111213141516171819public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); List&lt;Integer&gt; current = new ArrayList&lt;&gt;(); backtrack(result, current, n, k, 1); return result; &#125; private void backtrack(List&lt;List&lt;Integer&gt;&gt; result, List&lt;Integer&gt; current, int n, int k, int start) &#123; if (k == 0) &#123; result.add(new ArrayList&lt;&gt;(current)); return; &#125; for (int i = start; i &lt;= n; i++) &#123; current.add(i); backtrack(result, current, n, k - 1, i + 1); current.remove(current.size() - 1); &#125; &#125; 5、分割回文串给定一个字符串 s ，请将 s 分割成一些子串，使每个子串都是 回文串 ，返回 s 所有可能的分割方案。 示例 1： 12输入：s = &quot;aab&quot;输出：[[&quot;a&quot;,&quot;a&quot;,&quot;b&quot;],[&quot;aa&quot;,&quot;b&quot;]] 123456789101112131415161718192021222324252627282930public List&lt;List&lt;String&gt;&gt; partition(String s) &#123; List&lt;List&lt;String&gt;&gt; partitions=new ArrayList&lt;&gt;(); List&lt;String&gt; curpartion=new ArrayList&lt;&gt;(); doPartition(s,partitions,curpartion); return partitions; &#125; private void doPartition(String s,List&lt;List&lt;String&gt;&gt; partitions,List&lt;String&gt; curpartion)&#123; // 字符串遍历结束，添加本轮分割结果 if(s.length()==0)&#123; partitions.add(new ArrayList&lt;&gt;(curpartion)); return; &#125; for(int i=0;i&lt;s.length();i++)&#123; // 不断截取字符串前i个字符判断是否回文 if(isPalindrome(s,0,i))&#123; curpartion.add(s.substring(0,i+1)); doPartition(s.substring(i+1),partitions,curpartion); curpartion.remove(curpartion.size()-1); &#125; &#125; &#125; private boolean isPalindrome(String s,int i,int j)&#123; while(i&lt;j)&#123; if(s.charAt(i++)!=s.charAt(j--))&#123; return false; &#125; &#125; return true; &#125; 十、多线程1、线程a、b、c顺序执行要求线程a执行完才开始线程b，线程b执行完才开始线程c。 1234567891011121314151617181920212223242526272829303132333435public class ThreadExecutionOrder &#123; public static void main(String[] args) &#123; Thread threadA = new Thread(new Task(&quot;Thread A&quot;)); Thread threadB = new Thread(new Task(&quot;Thread B&quot;)); Thread threadC = new Thread(new Task(&quot;Thread C&quot;)); try &#123; threadA.start(); threadA.join(); // 等待线程A执行完毕 threadB.start(); threadB.join(); // 等待线程B执行完毕 threadC.start(); threadC.join(); // 等待线程C执行完毕 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; static class Task implements Runnable &#123; private final String name; public Task(String name) &#123; this.name = name; &#125; @Override public void run() &#123; System.out.println(name + &quot; is running.&quot;); // 执行任务逻辑 System.out.println(name + &quot; is done.&quot;); &#125; &#125;&#125; 2、两个线程轮流打印数字两个线程轮流打印数字，从1到100。 12345678910111213141516171819202122232425262728293031323334353637383940public class PrintNumbers &#123; private static final int MAX_NUMBER = 100; private static int number = 1; private static final Object lock = new Object(); public static void main(String[] args) &#123; Thread thread1 = new Thread(new PrintNumberTask(1)); Thread thread2 = new Thread(new PrintNumberTask(2)); thread1.start(); thread2.start(); &#125; static class PrintNumberTask implements Runnable &#123; private final int threadId; public PrintNumberTask(int threadId) &#123; this.threadId = threadId; &#125; @Override public void run() &#123; while (number &lt;= MAX_NUMBER) &#123; synchronized (lock) &#123; if (number % 2 == threadId - 1) &#123; System.out.println(&quot;Thread &quot; + threadId + &quot;: &quot; + number); number++; lock.notifyAll(); &#125; else &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125;&#125; 解释： 判断 number % 2 == threadId - 1 的目的是为了确定当前线程是否应该打印当前的数字。 在这里， number 表示当前要打印的数字， threadId 表示线程的ID（1或2）。根据这个判断条件，两个线程将交替打印数字。 具体解释如下： 当 threadId 为1时，判断条件为 number % 2 == 0 。这意味着只有当 number 为偶数时，线程1才会打印该数字。 当 threadId 为2时，判断条件为 number % 2 == 1 。这意味着只有当 number 为奇数时，线程2才会打印该数字。 通过这种方式，两个线程在获取到锁后会进行条件判断，只有符合条件的线程才会执行打印操作，并将 number 加1。否则，线程会通过 wait() 方法释放锁并等待，直到被其他线程通过 notifyAll() 方法唤醒。 3、两个线程按序轮流打印写两个线程，一个线程打印1~ 52，另一个线程打印A~Z，打印顺序是12A34B…5152Z。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class PrintNumbersAndLetters &#123; private static final Object lock = new Object(); private static int number = 1; private static char letter = &#x27;A&#x27;; public static void main(String[] args) &#123; Thread numberThread = new Thread(new PrintNumberTask()); Thread letterThread = new Thread(new PrintLetterTask()); numberThread.start(); letterThread.start(); &#125; static class PrintNumberTask implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; while (number &lt;= 52) &#123; if (number % 2 == 1) &#123; System.out.print(number); number++; System.out.print(number); number++; lock.notify(); &#125; else &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125; static class PrintLetterTask implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; while (letter &lt;= &#x27;Z&#x27;) &#123; System.out.print(letter); letter++; lock.notify(); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 解释： 在 PrintNumberTask 中，使用 while 循环判断 number 是否小于等于52。如果是奇数，则打印当前数字和下一个数字，并将 number 加2。然后通过 notify() 方法唤醒等待的线程，并自己进入等待状态。如果是偶数，则通过 wait() 方法释放锁并等待。 在 PrintLetterTask 中，使用 while 循环判断 letter 是否小于等于字母’Z’。每次循环打印当前字母，并将 letter 递增。然后通过 notify() 方法唤醒等待的线程，并自己进入等待状态。 4、三个线程按次序轮流打印编写一个程序，启动三个线程，三个线程的ID分别是A，B，C；，每个线程将自己的ID值在屏幕上打印5遍，打印顺序是ABCABC… 12345678910111213141516171819202122232425262728293031323334353637383940414243public class PrintABC &#123; private static final Object lock = new Object(); private static int currentThread = 0; private static final int MAX_COUNT = 5; public static void main(String[] args) &#123; Thread threadA = new Thread(new PrintTask(&quot;A&quot;, 0)); Thread threadB = new Thread(new PrintTask(&quot;B&quot;, 1)); Thread threadC = new Thread(new PrintTask(&quot;C&quot;, 2)); threadA.start(); threadB.start(); threadC.start(); &#125; static class PrintTask implements Runnable &#123; private final String threadId; private final int id; public PrintTask(String threadId, int id) &#123; this.threadId = threadId; this.id = id; &#125; @Override public void run() &#123; for (int i = 0; i &lt; MAX_COUNT; i++) &#123; synchronized (lock) &#123; while (currentThread != id) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.print(threadId); currentThread = (currentThread + 1) % 3; lock.notifyAll(); &#125; &#125; &#125; &#125;&#125; 解释： 在 PrintTask 中，使用 for 循环来打印ID值。在每次循环中，线程首先通过 synchronized 块获取到锁，并检查 currentThread 的值是否等于自己的ID值。如果不是，则线程进入等待状态，释放锁并等待被唤醒。如果是，则打印自己的ID值，并更新 currentThread 的值为下一个线程的ID。最后，通过 notifyAll() 方法唤醒其他等待的线程，并释放锁。 5、十个线程打印求和编写10个线程，第一个线程从1加到10，第二个线程从11加20…第十个线程从91加到100，最后再把10个线程结果相加。 12345678910111213141516171819202122232425262728293031323334353637383940/** * 编写10个线程，第一个线程从1加到10，第二个线程从11加20…第十个线程从91加到100，最后再把10个线程结果相加 */public class TenThreadSum &#123; public static class SumThread extends Thread&#123; int forct = 0; int sum = 0; SumThread(int forct)&#123; this.forct = forct; &#125; @Override public void run() &#123; for(int i = 1; i &lt;= 10; i++)&#123; sum += i + forct * 10; &#125; System.out.println(getName() + &quot; &quot; + sum); &#125; &#125; public static void main(String[] args) &#123; int result = 0; for(int i = 0; i &lt; 10; i++)&#123; SumThread sumThread = new SumThread(i); sumThread.start(); try &#123; sumThread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; result = result + sumThread.sum; &#125; System.out.println(&quot;result &quot; + result); &#125;&#125; 6、三个窗口同时卖票1234567891011121314151617181920212223242526272829public class TicketSeller &#123; private static int tickets = 100; // 总票数，假设为100张 public static void main(String[] args) &#123; Thread seller1 = new Thread(new Seller(), &quot;窗口1&quot;); Thread seller2 = new Thread(new Seller(), &quot;窗口2&quot;); Thread seller3 = new Thread(new Seller(), &quot;窗口3&quot;); seller1.start(); seller2.start(); seller3.start(); &#125; static class Seller implements Runnable &#123; @Override public void run() &#123; while (true) &#123; synchronized (TicketSeller.class) &#123; if (tickets &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;售出票号：&quot; + tickets); tickets--; &#125; else &#123; break; // 所有票已售完，退出循环 &#125; &#125; &#125; &#125; &#125;&#125; 7、交替打印两个数组1234567891011121314151617181920212223242526272829303132333435363738394041public class AlternatePrint &#123; private static final Object lock = new Object(); private static boolean printFirstArray = true; public static void main(String[] args) &#123; int[] array1 = &#123;1, 3, 5, 7, 9&#125;; int[] array2 = &#123;2, 4, 6, 8, 10&#125;; Thread thread1 = new Thread(new PrintTask(array1)); Thread thread2 = new Thread(new PrintTask(array2)); thread1.start(); thread2.start(); &#125; static class PrintTask implements Runnable &#123; private final int[] array; public PrintTask(int[] array) &#123; this.array = array; &#125; @Override public void run() &#123; for (int num : array) &#123; synchronized (lock) &#123; while ((printFirstArray &amp;&amp; array != array1) || (!printFirstArray &amp;&amp; array != array2)) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(num); printFirstArray = !printFirstArray; lock.notifyAll(); &#125; &#125; &#125; &#125;&#125; 解释： 我们创建了两个线程 thread1 和 thread2 ，它们分别负责打印 array1 和 array2 的元素。每个线程通过循环遍历数组，并使用 synchronized 关键字获取到共享的锁对象 lock 。 在每次循环中，线程首先通过 synchronized 块获取到锁，并检查当前应该打印的数组是否与自己负责打印的数组相符。如果不相符，则线程进入等待状态，释放锁并等待被唤醒。如果相符，则打印当前元素，并将 printFirstArray 的值取反，以便下一个线程可以打印另一个数组的元素。最后，通过 notifyAll() 方法唤醒其他等待的线程，并释放锁。 8、生产者消费者模式1、BlockingQueue 实现生产者消费者模式 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; BlockingQueue&lt;Object&gt; queue=new ArrayBlockingQueue&lt;&gt;(10); Runnable producer=()-&gt;&#123; while (true)&#123; try &#123; queue.put(new Object()); System.out.println(&quot;producing...&quot;); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; &#125;; new Thread(producer).start(); Runnable consumer=()-&gt;&#123; while (true)&#123; try &#123; queue.take(); System.out.println(&quot;taking...&quot;); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; &#125;; new Thread(consumer).start(); &#125; 2、Condition 实现生产者消费者模式 123456789101112131415161718192021222324252627282930313233343536private Queue queue; private int size; private ReentrantLock lock=new ReentrantLock(); private Condition notFull=lock.newCondition(); private Condition notEmpty=lock.newCondition(); public ProducerAndConsumerMode(int size)&#123; this.size=size; queue=new LinkedList(); &#125; public void put(Object o) throws InterruptedException&#123; lock.lock(); try&#123; while (queue.size()==size)&#123; notFull.await(); &#125; queue.add(o); notEmpty.signalAll(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void take(Object o) throws InterruptedException&#123; lock.lock(); try &#123; while (queue.size()==0)&#123; notEmpty.await(); &#125; queue.remove(o); notFull.signalAll(); &#125;finally &#123; lock.unlock(); &#125; &#125; 3、wait&#x2F;notify方法实现生产者消费者 123456789101112131415161718192021222324252627282930313233343536373839private int maxSize; private LinkedList&lt;Object&gt; storage=new LinkedList&lt;&gt;(); public ProducerAndConsumerMode(int size, LinkedList&lt;Object&gt; storage) &#123; this.maxSize = size; this.storage = storage; &#125; public synchronized void put() throws InterruptedException &#123; while (storage.size() == maxSize) &#123; wait(); &#125; storage.add(new Object()); notifyAll(); &#125; public synchronized void take() throws InterruptedException &#123; while (storage.size() == 0) &#123; wait(); &#125; System.out.println(storage.remove()); notifyAll(); &#125;","categories":["数据结构与算法"]},{"title":"面试题精选（一）","path":"/2023/10/06/面试题精选（一）/","content":"Java后端面试题精选。 一、Java基础重载和重写的区别重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为private则子类就不能重写该方法。 接口和抽象类的区别 抽象类可以存在普通成员函数，而接口中只能存在public abstract 方法。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的。 抽象类只能继承一个，接口可以实现多个。 使用场景：当你关注一个事物的本质的时候，用抽象类；当你关注一个操作的时候，用接口。 CopyOnWriteArrayList的底层原理 首先CopyOnWriteArrayList内部是通过数组来实现的，在向CopyOnWriteArrayList添加元素时，会复制⼀个新的数组，写操作在新数组上进行，读操作在原数组上进行。 并且，写操作会加锁，防止出现并发写入丢失数据的问题。 写操作结束之后会把原数组指向新数组。 CopyOnWriteArrayList允许在写操作时来读取数据，大大提高了读的性能，因此适合读多写少的应用场景，但是CopyOnWriteArrayList会比较占内存，同时可能读到的数据不是实时最新的数据，所以不适合实时性要求很高的场景。 HashMap详解（1） Put流程 （2）为什么HashMap的容量是2的倍数？ 第一个原因是为了方便哈希取余： 将元素放在table数组上面，是用hash值%数组大小定位位置，而HashMap是用hash值&amp;(数组大小-1)，却能和前面达到一样的效果，这就得益于HashMap的大小是2的倍数，2的倍数意味着该数的二进制位只有一位为1，而该数-1就可以得到二进制位上1变成0，后面的0变成1，再通过&amp;运算，就可以得到和%一样的效果，并且位运算比%的效率高得多。 第二个方面是在扩容时，利用扩容后的大小也是2的倍数，将已经产生hash碰撞的元素完美的转移到新的table中去。 （3）HashMap 是线程安全的吗？ HashMap不是线程安全的，可能会发生这些问题： 多线程下扩容死循环。JDK1.7 中的 HashMap 使用头插法插入元素，在多线程的环境下，扩容的时候有可能导致环形链表的出现，形成死循环。因此，JDK1.8 使用尾插法插入元素，在扩容时会保持链表元素原本的顺序，不会出现环形链表的问题。 多线程的 put 可能导致元素的丢失。多线程同时执行 put 操作，如果计算出来的索引位置是相同的，那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。 put 和 get 并发时，可能导致 get 为 null。线程 1 执行 put 时，因为元素个数超出 threshold 而导致 rehash，线程 2 此时执行 get，有可能导致这个问题。 （4）线程安全的Map Java 中有 HashTable、Collections.synchronizedMap、以及 ConcurrentHashMap 可以实现线程安全的 Map。 HashTable 是直接在操作方法上加 synchronized 关键字，锁住整个table数组，粒度比较大； Collections.synchronizedMap 是使用 Collections 集合工具的内部类，通过传入 Map 封装出一个 SynchronizedMap 对象，内部定义了一个对象锁，方法内通过对象锁实现； ConcurrentHashMap 在jdk1.7中使用分段锁，在jdk1.8中使用CAS+synchronized实现。 反射 反射的原理？ 我们都知道 Java 程序的执行分为编译和运行两步，编译之后会生成字节码(.class)文件，JVM 进行类加载的时候，会加载字节码文件，将类型相关的所有信息加载进方法区，反射就是去获取这些信息，然后进行各种操作。 序列化序列化：将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程 使用场景： 对象在进行网络传输（比如远程方法调用 RPC 的时候）之前需要先被序列化，接收到序列化的对象之后需要再进行反序列化； 将对象存储到文件之前需要进行序列化，将对象从文件中读取出来需要进行反序列化； 将对象存储到数据库（如 Redis）之前需要用到序列化，将对象从缓存数据库中读取出来需要反序列化； 将对象存储到内存之前需要进行序列化，从内存中读取出来之后需要进行反序列化。 序列化协议对应于 TCP&#x2F;IP 4 层模型的哪一层？ 表示层（数据处理、编解码、压缩解压缩、加密解密） 如果有些字段不想进行序列化怎么办？ 对于不想进行序列化的变量，使用 transient 关键字修饰。 关于 transient 几点注意： transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 常见序列化协议有哪些？ JDK 自带的序列化方式一般不会用 ，因为序列化效率低并且存在安全问题。比较常用的序列化协议有 Hessian、Kryo、Protobuf、ProtoStuff，这些都是基于二进制的序列化协议。 像 JSON 和 XML 这种属于文本类序列化方式。虽然可读性比较好，但是性能较差，一般不会选择。 二、并发编程线程的生命周期 线程通常有五种状态，创建，就绪，运行、阻塞和死亡状态。 阻塞的情况又分为三种： (1)、等待阻塞：运行的线程执行wait方法，该线程会释放占用的所有资源，JVM会把该线程放入“等待池”中。进入这个状态后，是不能自动唤醒的，必须依靠其他线程调用notify或notifyAll方法才能被唤醒，wait是object类的方法。 (2)、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入“锁池”中。 (3)、其他阻塞：运行的线程执行sleep或join方法，或者发出了I&#x2F;O请求时，JVM会把该线程置为阻塞状态。当sleep状态超时、join等待线程终止或者超时、或者I&#x2F;O处理完毕时，线程重新转入就绪状态。sleep是Thread类的方法。 新建状态（New）：新创建了一个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。 运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。 死亡状态（Dead）：线程执行完了或者因异常退出了run方法，该线程结束生命周期。 yield（）执行后线程直接进入就绪状态，马上释放了cpu的执行权，但是依然保留了cpu的执行资格，所以有可能cpu下次进行线程调度还会让这个线程获取到执行权继续执行。join（）执行后线程进入阻塞状态，例如在线程B中调用线程A的join（），那线程B会进入到阻塞队列，直到线程A结束或中断线程。 ThreadLocal的底层原理 ThreadLocal是Java中所提供的线程本地存储机制，可以利用该机制将数据缓存在某个线程内部，该线程可以在任意时刻、任意方法中获取缓存的数据。 ThreadLocal底层是通过ThreadLocalMap来实现的，每个Thread对象（注意不是ThreadLocal对象）中都存在⼀个ThreadLocalMap，Map的key为ThreadLocal对象，Map的value为需要缓存的值。 如果在线程池中使用ThreadLocal会造成内存泄漏，因为当ThreadLocal对象使用完之后，应该要把设置的key，value，也就是Entry对象进行回收，但线程池中的线程不会回收，而线程对象是通过强引⽤指向ThreadLocalMap，ThreadLocalMap也是通过强引用指向Entry对象，线程不被回收，Entry对象也就不会被回收，从而出现内存泄漏，解决办法是，在使用了ThreadLocal对象之后，手动调用ThreadLocal的remove方法，手动清楚Entry对象。 ThreadLocal经典的应用场景就是连接管理（⼀个线程持有⼀个连接，该连接对象可以在不同的方法之间进行传递，线程之间不共享同⼀个连接）。 线程上下文传递：ThreadLocal 可以用于在多个方法之间传递线程上下文信息，避免显式地传递参数。例如，在一个 web 请求处理过程中，可以将用户信息存储在 ThreadLocal 中，各个方法可以直接从 ThreadLocal 中获取用户信息，而不需要每个方法都传递用户信息参数。 ThreadLocal内存泄露原因及避免： 内存泄露为程序在申请内存后，无法释放已申请的内存空间。 强引用：使用最普遍的引用(new)，一个对象具有强引用，不会被垃圾回收器回收。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不回收这种对象。 如果想取消强引用和某个对象之间的关联，可以显式地将引用赋值为null，这样可以使JVM在合适的时间就会回收该对象。 弱引用：JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。可以在缓存中使用弱引用。 ThreadLocal的实现原理，每一个Thread维护一个ThreadLocalMap，key为使用弱引用的ThreadLocal实例，value为线程变量的副本。 ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal不存在外部强引用时，Key(ThreadLocal)势必会被GC回收，这样就会导致ThreadLocalMap中key为null， 而value还存在着强引用，只有thead线程退出以后,value的强引用链条才会断掉，但如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链（红色链条） key 使用强引用 当ThreadLocalMap的key为强引用回收ThreadLocal时，因为ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。 key 使用弱引用 当ThreadLocalMap的key为弱引用回收ThreadLocal时，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。当key为null，在下一次ThreadLocalMap调用set(),get()，remove()方法的时候会被清除value值。 因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 ThreadLocal正确的使用方法: 每次使用完ThreadLocal都调用它的remove()方法清除数据。 将ThreadLocal变量定义成private static，这样就一直存在ThreadLocal的强引用，也就能保证任何时候都能通过ThreadLocal的弱引用访问到Entry的value值，进而清除掉 。 final的可见性 final修饰的属性，在运行期间是不允许修改的，这样一来，就间接的保证了可见性，所有多线程读取final属性，值肯定是一样。 final并不是说每次取数据从主内存读取，他没有这个必要，而且final和volatile是不允许同时修饰一个属性的。 final修饰的内容已经不允许再次被写了，而volatile是保证每次读写数据去主内存读取，并且volatile会影响一定的性能，就不需要同时修饰。 Sychronized的锁升级过程 偏向锁：在锁对象的对象头中记录⼀下当前获取到该锁的线程ID，该线程下次如果又来获取该锁就可以直接获取到了。 轻量级锁：由偏向锁升级而来，当⼀个线程获取到锁后，此时这把锁是偏向锁，此时如果有第二个线程来竞争锁，偏向锁就会升级为轻量级锁，之所以叫轻量级锁，是为了和重量级锁区分开来，轻量级锁底层是通过自旋来实现的，并不会阻塞线程。 重量级锁：如果自旋次数过多仍然没有获取到锁，则会升级为重量级锁，重量级锁会导致线程阻塞。 自旋锁：自旋锁就是线程在获取锁的过程中，不会去阻塞线程，也就无所谓唤醒线程，阻塞和唤醒这两个步骤都是需要操作系统去进行的，比较消耗时间，自旋锁是线程通过CAS获取预期的⼀个标记，如果没有获取到，则继续循环获取，如果获取到了则表示获取到了锁，这个过程线程⼀直在运行中，相对而言没有使用太多的操作系统资源，比较轻量。 线程池原理1、为什么使用线程池？ 降低资源消耗；提高线程利用率，降低创建和销毁线程的消耗。 提高响应速度；任务来了，直接有线程可用可执行，而不是先创建线程，再执行。 提高线程的可管理性；线程是稀缺资源，使用线程池可以统一分配调优监控。 2、线程池参数 corePoolSize 代表核心线程数，也就是正常情况下创建工作的线程数，这些线程创建后并不会消除，而是一种常驻线程。 maxinumPoolSize 代表的是最大线程数，它与核心线程数相对应，表示最大允许被创建的线程数，比如当前任务较多，将核心线程数都用完了，还无法满足需求时，此时就会创建新的线程，但是线程池内线程总数不会超过最大线程数。 keepAliveTime 、 unit 表示超出核心线程数之外的线程的空闲存活时间，也就是核心线程不会消除，但是超出核心线程数的部分线程如果空闲一定的时间则会被消除,我们可以通过setKeepAliveTime 来设置空闲时间。 workQueue 用来存放待执行的任务，假设我们现在核心线程都已被使用，还有任务进来则全部放入队列，直到整个队列被放满但任务还再持续进入则会开始创建新的线程。 ThreadFactory 实际上是一个线程工厂，用来生产线程执行任务。我们可以选择使用默认的创建工厂，产生的线程都在同一个组内，拥有相同的优先级，且都不是守护线程。当然我们也可以选择自定义线程工厂，一般我们会根据业务来制定不同的线程工厂。 Handler 任务拒绝策略，有两种情况，第一种是当我们调用 shutdown 等方法关闭线程池后，这时候即使线程池内部还有没执行完的任务正在执行，但是由于线程池已经关闭，我们再继续想线程池提交任务就会遭到拒绝。另一种情况就是当达到最大线程数，线程池已经没有能力继续处理新提交的任务时，这是也就拒绝。 3、线程池中阻塞队列的作用？ 一般的队列只能保证作为一个有限长度的缓冲区，如果超出了缓冲长度，就无法保留当前的任务了，阻塞队列通过阻塞可以保留住当前想要继续入队的任务。 阻塞队列可以保证任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。 阻塞队列自带阻塞和唤醒的功能，不需要额外处理，无任务执行时，线程池利用阻塞队列的take方法挂起，从而维持核心线程的存活、不至于一直占用cpu资源。 4、为什么是先添加列队而不是先创建最大线程？ 在创建新线程的时候，是要获取全局锁的，这个时候其它的就得阻塞，影响了整体效率。 5、线程池线程复用原理 线程池将线程和任务进行解耦，线程是线程，任务是任务，摆脱了之前通过 Thread 创建线程时的一个线程必须对应一个任务的限制。 在线程池中，同一个线程可以从阻塞队列中不断获取新任务来执行，其核心原理在于线程池对Thread 进行了封装，并不是每次执行任务都会调用 Thread.start() 来创建新线程，而是让每个线程去执行一个“循环任务”，在这个“循环任务”中不停检查是否有任务需要被执行，如果有则直接执行，也就是调用任务中的 run 方法，将 run 方法当成一个普通的方法执行，通过这种方式只使用固定的线程就将所有任务的 run 方法串联起来。 6、线程池参数设置 监控线程池：1. 通过定期获取线程池的状态信息，如当前线程数、活跃线程数、任务队列长度等，来监控线程池的运行情况。可以使用线程池提供的方法，如 getPoolSize() 、 getActiveCount() 、 getQueue() 等来获取这些信息。 2. 设置合适的监控阈值：根据应用的需求和性能指标，设置合适的监控阈值。例如，当活跃线程数超过一定阈值或任务队列长度超过一定阈值时，可能需要调整线程池参数。 三、JVM内存泄露问题定位处理内存泄漏是指在程序中分配的内存空间无法被正常释放，导致内存占用不断增加，最终耗尽系统资源。下面是一些内存泄漏问题的定位和处理方法： 使用内存分析工具：使用专业的内存分析工具（如Java的HeapDump、MAT等）来检测和分析内存泄漏问题。这些工具可以帮助你查看内存中的对象和引用，找出占用内存较多的对象，并分析对象之间的引用关系。 分析内存快照：获取内存快照后，可以通过分析对象的引用关系，找出不再使用的对象或者存在循环引用的对象。确定哪些对象没有被正确释放是解决内存泄漏问题的关键。 检查代码逻辑：检查代码中的逻辑错误，例如未关闭的数据库连接、未释放的资源等。确保在不再使用对象时及时释放相关资源，以避免内存泄漏。 避免静态引用：静态变量的生命周期通常很长，如果不正确地使用静态引用，可能导致对象无法被垃圾回收。因此，避免在静态变量中持有对对象的引用，或者在不需要时及时将其置为null。 使用弱引用或软引用：对于一些临时性的对象或者缓存对象，可以考虑使用弱引用或软引用。这样，当内存不足时，垃圾回收器可以自动回收这些对象，避免内存泄漏。 定期进行性能测试和内存监控：通过定期进行性能测试和内存监控，可以及时发现内存泄漏问题，并进行修复。监控应用程序的内存使用情况，及时处理内存占用过高的情况。 以上是一些常见的内存泄漏问题定位和处理方法。在解决内存泄漏问题时，需要结合具体的应用场景和代码逻辑进行分析和调试，以找到并修复潜在的内存泄漏问题。 CMS和G1垃圾收集器G1垃圾收集器是一种以低延迟和高吞吐量为目标的垃圾收集器。它采用了分代收集和并发标记整理的方式来进行垃圾回收。G1垃圾收集器将堆内存划分为多个大小相等的区域（Region），并根据垃圾回收的情况动态调整每个区域的大小。在垃圾回收过程中，G1垃圾收集器会优先回收垃圾最多的区域，以达到高效回收的目的。 有了 CMS，为什么还要引入 G1？ 优点：CMS 最主要的优点在名字上已经体现出来——并发收集、低停顿。 缺点：CMS 同样有三个明显的缺点。 Mark Sweep 算法会导致内存碎片比较多 CMS 的并发能力比较依赖于 CPU 资源，并发回收时垃圾收集线程可能会抢占用户线程的资源，导致用户程序性能下降。 并发清除阶段，用户线程依然在运行，会产生所谓的理“浮动垃圾”（Floating Garbage），本次垃圾收集无法处理浮动垃圾，必须到下一次垃圾收集才能处理。如果浮动垃圾太多，会触发新的垃圾回收，导致性能降低。 G1 主要解决了内存碎片过多的问题。 CMS垃圾回收器产生内存碎片的原因主要有两个： 并发清除：CMS垃圾回收器在执行清除操作时，会与应用程序并发执行。这意味着它无法对整个堆进行整理，只能对已标记为垃圾的对象进行清除。这样会导致堆中存在大量不连续的空闲内存碎片。 并发标记：为了减少停顿时间，CMS垃圾回收器在标记阶段与应用程序并发执行。这意味着在标记过程中，应用程序可能会继续分配和释放对象。这样就可能导致堆内存中出现空洞，进一步增加了内存碎片的产生。 为了解决CMS垃圾回收器产生的内存碎片问题，可以考虑以下措施： 定期进行Full GC：通过定期进行Full GC，可以对整个堆进行整理，从而减少内存碎片的产生。 调整堆内存大小：适当调整堆内存的大小，可以减少内存碎片的产生。过小的堆内存可能导致过多的碎片，而过大的堆内存可能增加垃圾回收的时间。 使用压缩式垃圾回收器：压缩式垃圾回收器（如G1垃圾回收器）可以在执行垃圾回收时对堆内存进行整理，从而减少内存碎片的产生。 频繁 minor gc 怎么办？优化 Minor GC 频繁问题：通常情况下，由于新生代空间较小，Eden 区很快被填满，就会导致频繁 Minor GC，因此可以通过增大新生代空间-Xmn来降低 Minor GC 的频率。 频繁 Full GC 怎么办？Full GC 的排查思路大概如下： 1）清楚从程序角度，有哪些原因导致 FGC？ 大对象：系统一次性加载了过多数据到内存中（比如 SQL 查询未做分页），导致大对象进入了老年代。 内存泄漏：频繁创建了大量对象，但是无法被回收（比如 IO 对象使用完后未调用 close 方法释放资源），先引发 FGC，最后导致 OOM. 程序频繁生成一些长生命周期的对象，当这些对象的存活年龄超过分代年龄时便会进入老年代，最后引发 FGC. （即本文中的案例） 程序 BUG 代码中显式调用了 gc方法，包括自己的代码甚至框架中的代码。 JVM 参数设置问题：包括总内存大小、新生代和老年代的大小、Eden 区和 S 区的大小、元空间大小、垃圾回收算法等等。 2）清楚排查问题时能使用哪些工具 公司的监控系统：大部分公司都会有，可全方位监控 JVM 的各项指标。 JDK 的自带工具，包括 jmap、jstat 等常用命令： 四、Spring框架如何实现一个IOC容器 配置文件中指定需要扫描的包路径。 定义一些注解，分别表示访问控制层、业务服务层、数据持久层、依赖注入注解、获取配置文件注解。 从配置文件中获取需要扫描的包路径，获取到当前路径下的文件信息及文件夹信息，我们将当前路径下所有以.class结尾的文件添加到一个Set集合中进行存储。 遍历这个set集合，获取在类上有指定注解的类，并将其交给IOC容器，定义一个安全的Map用来存储这些对象。 遍历这个IOC容器，获取到每一个类的实例，判断里面是有有依赖其他的类的实例，然后进行递归注入 。 Spring是什么？轻量级的开源的J2EE框架。它是一个容器框架，用来装Javabean（Java对象），中间层框架，可以起一个连接作用，比如说把Struts和hibernate粘合在一起运用，可以让我们的企业开发更快、更简洁。 1、Spring是一个轻量级的控制反转（IoC)和面向切面（AOP）的容器框架。 1、从大小与开销两方面而言Spring都是轻量级的。2、通过控制反转(IoC)的技术达到松耦合的目的。 2、提供了面向切面编程的丰富支持，允许通过分离应用的业务逻辑与系统级服务进行内聚性的开发。 1、包含并管理应用对象(Bean)的配置和生命周期，这个意义上是一个容器。2、将简单的组件配置、组合成为复杂的应用，这个意义上是一个框架。 谈谈你对IOC的理解容器、控制反转、依赖注入 。 ioc容器：实际上就是个map（key，value），里面存的是各种对象（在xml里配置的bean节点、@repository、@service、@controller、@component），在项目启动的时候会读取配置文件里面的bean节点，根据全限定类名使用反射创建对象放到map里、扫描到打上上述注解的类还是通过反射创建对象放到map里。 这个时候map里就有各种对象了，接下来我们在代码里需要用到里面的对象时，再通过DI注入（autowired、resource等注解，xml里bean节点内的ref属性，项目启动的时候会读取xml节点ref属性根据id注入，也会扫描这些注解，根据类型或id注入；id就是对象名） 控制反转：没有引入IOC容器之前，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。 引入IOC容器之后，对象A与对象B之间失去了直接联系，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。 通过前后的对比，不难看出来：对象A获得依赖对象B的过程，由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。全部对象的控制权全部上缴给“第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成“粘合剂”的由来。 依赖注入：“获得依赖对象的过程被反转了”。控制被反转之后，获得依赖对象的过程由自身管理变为了由IOC容器主动注入。依赖注入是实现IOC的方法，就是由IOC容器在运行期间，动态地将某种依赖关系注入到对象之中。 AOP是什么？Spring AOP（面向切面编程）是Spring框架的一个重要特性，用于实现横切关注点的模块化开发。通过AOP，可以将与业务逻辑无关的功能（如日志记录、性能统计、事务管理等）从应用程序的核心业务逻辑中分离出来，使得代码更加简洁、可维护性更高。 Spring AOP基于代理模式实现，它通过在目标对象的方法执行前、执行后或抛出异常时动态地插入切面逻辑，从而实现对目标对象的增强。 以下是一些Spring AOP的关键概念： 切面（Aspect）：用于定义横切关注点及其逻辑。切面由切点和通知组成。 切点（Pointcut）：用于定义需要在目标对象中插入切面逻辑的方法集合。 通知（Advice）：定义在切点处执行的逻辑，包括前置通知、后置通知、异常通知、返回通知和环绕通知等。 连接点（Join Point）：在应用程序执行过程中可以插入切面逻辑的点，通常是方法的执行。 目标对象（Target Object）：被切面增强的对象。 代理对象（Proxy Object）：包装了目标对象，并在方法执行时插入切面逻辑的对象。 使用： 引入依赖：引入 AOP 依赖 自定义注解：自定义一个注解作为切点 配置 AOP 切面： @Aspect：标识切面 @Pointcut：设置切点，这里以自定义注解为切点，定义切点有很多其它种方式，自定义注解是比较常用的一种。 @Before：在切点之前织入，打印了一些入参信息 @Around：环绕切点，打印返回参数和接口执行时间 Spring AOP提供了两种代理方式：基于JDK动态代理和基于CGLIB的动态代理。如果目标对象实现了接口，Spring AOP将使用JDK动态代理；如果目标对象没有实现接口，Spring AOP将使用CGLIB动态代理。 JDK 动态代理和 CGLIB 代理JDK 动态代理 Interface：对于 JDK 动态代理，目标类需要实现一个 Interface。 InvocationHandler：InvocationHandler 是一个接口，可以通过实现这个接口，定义横切逻辑，再通过反射机制（invoke）调用目标类的代码，在次过程，可能包装逻辑，对目标方法进行前置后置处理。 Proxy：Proxy 利用 InvocationHandler 动态创建一个符合目标类实现的接口的实例，生成目标类的代理对象。 CgLib 动态代理 使用 JDK 创建代理有一大限制，它只能为接口创建代理实例，而 CgLib 动态代理就没有这个限制。 CgLib 动态代理是使用字节码处理框架 ASM，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。 CgLib 创建的动态代理对象性能比 JDK 创建的动态代理对象的性能高不少，但是 CGLib 在创建代理对象时所花费的时间却比 JDK 多得多，所以对于单例的对象，因为无需频繁创建对象，用 CGLib 合适，反之，使用 JDK 方式要更为合适一些。同时，由于 CGLib 由于是采用动态创建子类的方法，对于 final 方法，无法进行代理。 BeanFactory和ApplicationContext的区别ApplicationContext是BeanFactory的子接口，ApplicationContext提供了更完整的功能： ①继承MessageSource，因此支持国际化。 ②统一的资源文件访问方式。 ③提供在监听器中注册bean的事件。 ④同时加载多个配置文件。 ⑤载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。 BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。 ApplicationContext，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所有依赖属性是否注入。ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。 相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。 BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader。 BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 描述一下Spring Bean的生命周期？1、解析类得到BeanDefinition。2、如果有多个构造方法，则要推断构造方法。3、确定好构造方法后，进行实例化得到一个对象。4、对对象中的加了@Autowired注解的属性进行属性填充。5、回调Aware方法，比如BeanNameAware，BeanFactoryAware。6、调用BeanPostProcessor的初始化前的方法。7、调用初始化方法。8、调用BeanPostProcessor的初始化后的方法，在这里会进行AOP。9、如果当前创建的bean是单例的则会把bean放入单例池。10、使用bean。11、Spring容器关闭时调用DisposableBean中destory()方法。 Spring支持的几种bean的作用域 singleton：默认，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护。该对象的生命周期是与Spring IOC容器一致的（但在第一次被注入时才会创建）。 prototype：为每一个bean请求提供一个实例。在每次注入时都会创建一个新的对象。 request：bean被定义为在每个HTTP请求中创建一个单例对象，也就是说在单个请求中都会复用这一个单例对象。 session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效。 application：bean被定义为在ServletContext的生命周期中复用一个单例对象。 websocket：bean被定义为在websocket的生命周期中复用一个单例对象。 global-session：全局作用域，global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 Spring中单例Bean是线程安全的么？Spring中的Bean默认是单例模式的，框架并没有对bean进行多线程的封装处理。 如果Bean是有状态的 那就需要开发人员自己来进行线程安全的保证，最简单的办法就是改变bean的作用域 把 “singleton”改为’‘protopyte’ 这样每次请求Bean就相当于是 new Bean() 这样就可以保证线程的安全了。 有状态就是有数据存储功能，不是线程安全的 无状态就是不会保存数据，是线程安全的 controller、service和dao层本身并不是线程安全的，如果只是调用里面的方法，而且多线程调用一个实例的方法，会在内存中复制变量，这是自己的线程的工作内存，是安全的。 Dao会操作数据库Connection，Connection是带有状态的，比如说数据库事务，Spring的事务管理器使用Threadlocal为不同线程维护了一套独立的connection副本，保证线程之间不会互相影响（Spring是如何保证事务获取同一个Connection的）。 不要在bean中声明任何有状态的实例变量或类变量，如果必须如此，那么就使用ThreadLocal把变量变为线程私有的，如果bean的实例变量或类变量需要在多个线程之间共享，那么就只能使用synchronized、lock、CAS等这些实现线程同步的方法了。 Spring容器启动流程 在创建Spring容器，也就是启动Spring时，首先会进行扫描，扫描得到所有的BeanDefinition对象，并存在⼀个Map中。 然后筛选出非懒加载的单例BeanDefinition进行创建Bean，对于多例Bean不需要在启动过程中去进行创建，对于多例Bean会在每次获取Bean时利用BeanDefinition去创建。 利用BeanDefinition创建Bean就是Bean的创建生命周期，这期间包括了合并BeanDefinition、推断构造方法、实例化、属性填充、初始化前、初始化、初始化后等步骤，其中AOP就是发生在初始化后这⼀步骤中。 单例Bean创建完了之后，Spring会发布⼀个容器启动事件，Spring启动结束。 在源码中会更复杂，比如源码中会提供⼀些模板方法，让子类来实现，比如源码中还涉及到⼀些BeanFactoryPostProcessor和BeanPostProcessor的注册，Spring的扫描就是通过BenaFactoryPostProcessor来实现的，依赖注⼊就是通过BeanPostProcessor来实现的在Spring启动过程中还会去处理@Import等注解。 Spring 框架中都用到了哪些设计模式？简单工厂：由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。 Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 工厂方法： 实现了FactoryBean接口的bean是一类叫做factory的bean。其特点是，spring会在使用getBean()调用获得该bean时，会自动调用该bean的getObject()方法，所以返回的不是factory这个bean，而是这个bean.getOjbect()方法的返回值。 单例模式：保证一个类仅有一个实例，并提供一个访问它的全局访问点 spring对单例的实现：spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为spring管理的是任意的java对象。 适配器模式： Spring定义了一个适配接口，使得每一种Controller有一种对应的适配器实现类，让适配器代替controller执行相应的方法。这样在扩展Controller时，只需要增加一个适配器类就完成了SpringMVC的扩展了。 装饰器模式：动态地给一个对象添加一些额外的职责。就增加功能来说，Decorator模式相比生成子类更为灵活。 Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。 动态代理： 切面在应用运行的时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象创建动态的创建一个代理对象。SpringAOP就是以这种方式织入切面的。织入：把切面应用到目标对象并创建新的代理对象的过程。 观察者模式： spring的事件驱动模型使用的是观察者模式，spring中Observer模式常用的地方是listener的实现。 策略模式： Spring框架的资源访问Resource接口。该接口提供了更强的资源访问能力，Spring 框架本身大量使用Resource 接口来访问底层资源。 模板方法：父类定义了骨架（调用哪些方法及顺序），某些特定方法由子类实现。最大的好处：代码复用，减少重复代码。除了子类要实现的特定方法，其他方法及方法调用顺序都在父类中预先写好了 refresh方法。 Spring事务以及隔离级别？在使用Spring框架时，可以有两种使用事务的方式，一种是编程式的，一种是申明式的，@Transactional注解就是申明式的。 首先，事务这个概念是数据库层面的，Spring只是基于数据库中的事务进行了扩展，以及提供了一些能让程序员更加方便操作事务的方式。比如我们可以通过在某个方法上增加@Transactional注解，就可以开启事务，这个方法中所有的sql都会在一个事务中执行，统一成功或失败。 在一个方法上加了@Transactional注解后，Spring会基于这个类生成一个代理对象，会将这个代理对象作为bean，当在使用这个代理对象的方法时，如果这个方法上存在@Transactional注解，那么代理逻辑会先把事务的自动提交设置为false，然后再去执行原本的业务逻辑方法，如果执行业务逻辑方法没有出现异常，那么代理逻辑中就会将事务进行提交，如果执行业务逻辑方法出现了异常，那么则会将事务进行回滚。 当然，针对哪些异常回滚事务是可以配置的，可以利用@Transactional注解中的rollbackFor属性进行配置，默认情况下会对RuntimeException和Error进行回滚。 spring事务隔离级别就是数据库的隔离级别：外加一个默认级别 read uncommitted（未提交读） read committed（提交读、不可重复读） repeatable read（可重复读） serializable（可串行化） 数据库的配置隔离级别是Read Commited,而Spring配置的隔离级别是Repeatable Read，请问这时隔离级别是以哪一个为准？以Spring配置的为准，如果spring设置的隔离级别数据库不支持，效果取决于数据库. Spring事务传播机制多个事务方法相互调用时，事务如何在这些方法间传播。 方法A是一个事务的方法，方法A执行过程中调用了方法B，那么方法B有无事务以及方法B对事务的要求不同都会对方法A的事务具体执行造成影响，同时方法A的事务对方法B的事务执行也有影响，这种影响具体是什么就由两个方法所定义的事务传播类型所决定。 REQUIRED(Spring默认)：如果当前没有事务，则自己新建一个事务，如果当前存在事务，则加入这个事务 。 SUPPORTS：当前存在事务，则加入当前事务，如果当前没有事务，就以非事务方法执行。 MANDATORY：当前存在事务，则加入当前事务，如果当前事务不存在，则抛出异常。 REQUIRES_NEW：创建一个新事务，如果存在当前事务，则挂起该事务。 NOT_SUPPORTED：以非事务方式执行,如果当前存在事务，则挂起当前事务。 NEVER：不使用事务，如果当前事务存在，则抛出异常。 NESTED：如果当前事务存在，则在嵌套事务中执行，否则REQUIRED的操作一样（开启一个事务）。 和REQUIRES_NEW的区别 REQUIRES_NEW是新建一个事务并且新开启的这个事务与原有事务无关，而NESTED则是当前存在事务时（我们把当前事务称之为父事务）会开启一个嵌套事务（称之为一个子事务）。 在NESTED情况下父事务回滚时，子事务也会回滚，而在REQUIRES_NEW情况下，原有事务回滚，不会影响新开启的事务。 和REQUIRED的区别 REQUIRED情况下，调用方存在事务时，则被调用方和调用方使用同一事务，那么被调用方出现异常时，由于共用一个事务，所以无论调用方是否catch其异常，事务都会回滚 而在NESTED情况下，被调用方发生异常时，调用方可以catch其异常，这样只有子事务回滚，父事务不受影响 注：@Transactional注解默认传播属性为required，下面列举事务调用失效情况。 总结：方法A调用方法B：1、如果只有A加@Transactional注解；则AB在同一事务中；2、如果只有B加@Transactional注解；AB方法为同一类，事务失效；AB不同类，只有B有事务； Spring事务什么时候会失效?spring事务的原理是AOP，进行了切面增强，那么失效的根本原因是这个AOP不起作用了！常见情况有如下几种： 1、发生自调用，类里面使用this调用本类的方法（this通常省略），此时这个this对象不是代理类，而是UserService对象本身！解决方法很简单，让那个this变成UserService的代理类即可！ 2、方法不是public的 @Transactional 只能用于 public 的方法上，否则事务不会失效，如果要用在非 public 方法上，可以开启AspectJ 代理模式。 3、数据库不支持事务 4、没有被spring管理 5、异常被吃掉，事务不会回滚(或者抛出的异常没有被定义，默认为RuntimeException) 什么是bean的自动装配，有哪些方式？开启自动装配，只需要在xml配置文件中定义“autowire”属性。 1&lt;bean id=&quot;cutomer&quot; class=&quot;com.xxx.xxx.Customer&quot; autowire=&quot;&quot; /&gt; autowire属性有五种装配的方式： no – 缺省情况下，自动配置是通过“ref”属性手动设定 。 手动装配：以value或ref的方式明确指定属性值都是手动装配。需要通过‘ref’属性来连接bean。 byName-根据bean的属性名称进行自动装配。 Cutomer的属性名称是person，Spring会将bean id为person的bean通过setter方法进行自动装配。 byType-根据bean的类型进行自动装配 。 Cutomer的属性person的类型为Person，Spirng会将Person类型通过setter方法进行自动装配。 constructor-类似byType，不过是应用于构造器的参数。如果一个bean与构造器参数的类型形同，则进行自动装配，否则导致异常。 Cutomer构造函数的参数person的类型为Person，Spirng会将Person类型通过构造方法进行自动装配。 autodetect-如果有默认的构造器，则通过constructor方式进行自动装配，否则使用byType方式进行自动装配。 如果有默认的构造器，则通过constructor方式进行自动装配，否则使用byType方式进行自动装配。 @Autowired自动装配bean，可以在字段、setter方法、构造函数上使用。 Spring Boot、Spring MVC 和 Spring 有什么区别spring是一个IOC容器，用来管理Bean，使用依赖注入实现控制反转，可以很方便的整合各种框架，提供AOP机制弥补OOP的代码重复问题、更方便将不同类不同方法中的共同处理抽取成切面、自动注入给方法执行，比如日志、异常等。 springmvc是spring对web框架的一个解决方案，提供了一个总的前端控制器Servlet，用来接收请求，然后定义了一套路由策略（url到handle的映射）及适配执行handle，将handle结果使用视图解析技术生成视图展现给前端。 springboot是spring提供的一个快速开发工具包，让程序员能更方便、更快速的开发spring+springmvc应用，简化了配置（约定了默认配置），整合了一系列的解决方案（starter机制）、redis、mongodb、es，可以开箱即用。 SpringMVC 工作流程1）用户发送请求至前端控制器 DispatcherServlet，DispatcherServlet 收到请求调用 HandlerMapping 处理器映射器。处理器映射器找到具体的处理器(可以根据 xml 配置、注解进行查找)，生成处理器及处理器拦截器(如果有则生成)一并返回给 DispatcherServlet。 2）DispatcherServlet 调用 HandlerAdapter 处理器适配器。HandlerAdapter 经过适配器调用具体的处理器(Controller，也叫后端控制器)。Controller 执行完成返回ModelAndView。 3）HandlerAdapter 将 controller 执行结果 ModelAndView 返回给 DispatcherServlet。DispatcherServlet 将 ModelAndView 传给 ViewReslover 视图解析器。 4）ViewReslover 解析后返回具体 View。DispatcherServlet 根据 View 进行渲染视图（即将模型数据填充至视图中）。DispatcherServlet 响应用户。 Spring Boot 自动配置原理？详细见：SpringBoot 自动装配原理详解) @Import + @Configuration + Spring spi 自动配置类由各个starter提供，使用@Configuration + @Bean定义配置类，放到METAINF&#x2F;spring.factories下。 使用Spring spi扫描META-INF&#x2F;spring.factories下的配置类。 使用@Import导入自动配置类。 如何理解 Spring Boot 中的 Starter 使用spring + springmvc使用，如果需要引入mybatis等框架，需要到xml中定义mybatis需要的bean。 starter就是定义一个starter的jar包，写一个@Configuration配置类、将这些bean定义在里面，然后在starter包的META-INF&#x2F;spring.factories中写入该配置类，springboot会按照约定来加载该配置类。 开发人员只需要将相应的starter包依赖进应用，进行相应的属性配置（使用默认配置时，不需要配置），就可以直接进行代码开发，使用对应的功能了，比如mybatis-spring-boot–starter，springboot-starter-redis 。 五、MyBatisMyBatis的优缺点优点：1、基于 SQL 语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL 写在XML 里，解除 sql 与程序代码的耦合，便于统一管理；提供 XML 标签， 支持编写动态 SQL 语句， 并可重用。2、与 JDBC 相比，减少了 50%以上的代码量，消除了 JDBC 大量冗余的代码，不需要手动开关连接；3、很好的与各种数据库兼容（ 因为 MyBatis 使用 JDBC 来连接数据库，所以只要JDBC 支持的数据库MyBatis 都支持）。4、能够与 Spring 很好的集成；5、提供映射标签， 支持对象与数据库的 ORM 字段关系映射； 提供对象关系映射标签， 支持对象关系组件维护。 缺点：1、SQL 语句的编写工作量较大， 尤其当字段多、关联表多时， 对开发人员编写SQL 语句的功底有一定要求。2、SQL 语句依赖于数据库， 导致数据库移植性差， 不能随意更换数据库。 Mybatis中#{}和${}的区别 #{}是预编译处理、是占位符， ${}是字符串替换、是拼接符。 Mybatis 在处理#{}时，会将 sql 中的#{}替换为?号，调用 PreparedStatement 来赋值。Mybatis 在处理${}时， 就是把${}替换成变量的值，调用 Statement 来赋值。 #{} 的变量替换是在DBMS 中、变量替换后，#{} 对应的变量自动加上单引号。${} 的变量替换是在 DBMS 外、变量替换后，${} 对应的变量不会加上单引号 。 使⽤#{}可以有效的防止SQL注⼊，提高系统安全性。 示例： 12345select * from user where name = #&#123;name&#125; and password = #&#123;password&#125; 将转为select * from user where name = &#x27;zhou&#x27; and password = &#x27;1 or 1=1&#x27;select * from user where name = $&#123;name&#125; and password = $&#123;password&#125; 将转为select * from user where name = zhou and password = 1 or 1=1 六、MySQL索引的基本原理索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理：就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序。 对排序结果生成倒排表。 在倒排表内容上拼上数据地址链。 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据。 MySQL聚簇和非聚簇索引的区别 聚簇索引：将数据存储与索引放到了一块、并且是按照一定的顺序组织的，找到索引也就找到了数据，数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。 非聚簇索引：叶子节点不存储数据、存储的是数据行地址，也就是说根据索引查找到数据行的位置再取磁盘查找数据，这个就有点类似一本树的目录，比如我们要找第三章第一节，那我们先在这个目录里面找，找到对应的页码后再去对应的页码看文章。 优势：1、查询通过聚簇索引可以直接获取数据，相比非聚簇索引需要二次查询（非覆盖索引的情况下）效率要高。2、聚簇索引对于范围查询的效率很高，因为其数据是按照大小排列的。3、聚簇索引适合用在排序的场合，非聚簇索引不适合。 劣势：1、维护索引很昂贵，特别是插入新行或者主键被更新导至要分页(page split)的时候。建议在大量插入新行后，选在负载较低的时间段，通过OPTIMIZE TABLE优化表，因为必须被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片。2、表因为使用UUID（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫描更慢，所以建议使用int的auto_increment作为主键。3、如果主键比较大的话，那辅助索引将会变的更大，因为辅助索引的叶子存储的是主键值；过长的主键值，会导致非叶子节点占用占用更多的物理空间。 MySQL索引的数据结构与各自优劣MySQL中使用较多的索引有Hash索引，B+树索引等，InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 B+树：B+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的节点间有指针相互链接。在B+树上的常规检索，从根节点到叶子节点的搜索效率基本相当，不会出现大幅波动，而且基于索引的顺序扫描时，也可以利用双向指针快速左右移动，效率非常高。因此，B+树索引被广泛应用于数据库、文件系统等场景。 哈希索引：哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 比较： 如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；前提是键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据； 如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索； 哈希索引也没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）； 哈希索引也不支持多列联合索引的最左匹配规则； B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在哈希碰撞问题。 和其他数据结构的比较： 二叉查找树(BST) ：解决了排序的基本问题，但是由于无法保证平衡，可能退化为链表； 平衡二叉树(AVL) ：通过旋转解决了平衡的问题，但是旋转操作效率太低； 红黑树 ：通过舍弃严格的平衡和引入红黑节点，解决了 AVL 旋转效率过低的问题，但是在磁盘等场景下，树仍然太高，IO 次数太多； B 树 ：通过将二叉树改为多路平衡查找树，解决了树过高的问题； B+树 ：在 B 树的基础上，将非叶节点改造为不存储数据的纯索引节点，进⼀步降低了树的⾼度；此外将叶节点使用指针连接成链表，范围查询更加高效。 索引设计的原则 适合索引的列是出现在where子句中的列，或者连接子句中指定的列。 基数较小的表，索引效果较差，没有必要在此列建立索引。 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间，如果搜索词超过索引前缀长度，则使用索引排除不匹配的行，然后检查其余行是否可能匹配。 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 定义有外键的数据列一定要建立索引。 更新频繁字段不适合创建索引。 若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)。 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 对于定义为text、image和bit的数据类型的列不要建立索引。 什么是最左前缀原则和最左匹配原则？在MySQL数据库中，最左前缀原则和最左匹配原则可以解释为索引的使用规则。 最左前缀原则（Leftmost Prefix Rule）指的是在使用联合索引（多列索引）时，索引会按照索引列的顺序进行匹配。当查询条件中只使用了索引的前缀列，而没有使用后续的列，最左前缀原则可以确保索引的有效使用。 换句话说，如果一个联合索引包含(A, B, C)三个列，那么只有在查询条件中使用了A列或者(A, B)列时，索引才能被利用。 最左匹配原则（Leftmost Match Rule）是指在使用联合索引进行查询时，索引会从最左侧的列开始匹配查询条件。如果查询条件中只使用了索引的前缀列，而没有使用后续的列，最左匹配原则可以确保索引的有效匹配。 换句话说，如果一个联合索引包含(A, B, C)三个列，那么只有在查询条件中使用了A列或者(A, B)列时，索引才能被最左匹配原则匹配到。 这两个原则都是为了优化查询性能而设计的。通过遵循最左前缀原则和最左匹配原则，可以使索引的匹配更加准确和高效，提升数据库查询的性能。 锁的类型有哪些？基于锁的属性分类：共享锁、排他锁。 基于锁的粒度分类：行级锁(INNODB)、表级锁(INNODB、MYISAM)、页级锁(BDB引擎 )、记录锁、间隙锁、临键锁。 基于锁的状态分类：意向共享锁、意向排它锁。 共享锁(Share Lock) 1、共享锁又称读锁，简称S锁； 2、当一个事务为数据加上读锁之后，其他事务只能对该数据加读锁，而不能对数据加写锁，直到所有的读锁释放之后其他事务才能对其进行加持写锁。 3、共享锁的特性主要是为了支持并发的读取数据，读取数据的时候不支持修改，避免出现重复读的问题。 排他锁（eXclusive Lock） 1、排他锁又称写锁，简称X锁； 2、当一个事务为数据加上写锁时，其他请求将不能再为数据加任何锁，直到该锁释放之后，其他事务才能对数据进行加锁。 3、排他锁的目的是在数据修改时候，不允许其他人同时修改，也不允许其他人读取。避免了出现脏数据和脏读的问题。 表锁 1、表锁是指上锁的时候锁住的是整个表，当下一个事务访问该表的时候，必须等前一个事务释放了锁才能进行对表进行访问；2、特点： 粒度大，加锁简单，容易冲突； 行锁 1、行锁是指上锁的时候锁住的是表的某一行或多行记录，其他事务访问同一张表时，只有被锁住的记录不能访问，其他的记录可正常访问；2、特点：粒度小，加锁比表锁麻烦，不容易冲突，相比表锁支持的并发要高； 记录锁(Record Lock) 1、记录锁也属于行锁中的一种，只不过记录锁的范围只是表中的某一条记录，记录锁是说事务在加锁后锁住的只是表的某一条记录。2、精准条件命中，并且命中的条件字段是唯一索引。3、加了记录锁之后的数据可以避免数据在查询的时候被修改的重复读问题，也避免了在修改的事务未提交前被其他事务读取的脏读问题。 页锁 1、页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。2、特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 间隙锁(Gap Lock) 1、属于行锁中的一种，间隙锁是在事务加锁后其锁住的是表记录的某一个区间，当表的相邻ID之间出现空隙则会形成一个区间，遵循左开右闭原则。2、范围查询并且查询未命中记录，查询条件必须命中索引、间隙锁只会出现在REPEATABLE_READ（重复读)的事务级别中。3、触发条件：防止幻读问题，事务并发的时候，如果没有间隙锁，就会发生如下图的问题，在同一个事务里，A事务的两次查询出的结果会不一样。 示例解析：假设有一个表格t，其中有一个索引字段id。如果一个事务T1使用间隙锁锁住了id值为1到3的间隙，那么其他事务在这个间隙内插入id值为2的记录时，会被阻塞，直到事务T1释放锁。 临建锁(Next-Key Lock) 1、也属于行锁的一种，并且它是INNODB的行锁默认算法，总结来说它就是记录锁和间隙锁的组合，临键锁会把查询出来的记录锁住，同时也会把该范围查询内的所有间隙空间也会锁住，总之它会把相邻的下一个区间也会锁住。2、触发条件：范围查询并命中，查询命中了索引。3、结合记录锁和间隙锁的特性，临键锁避免了在范围查询时出现脏读、重复读、幻读问题。加了临键锁之后，在范围区间内数据不允许被修改和插入。 示例解析：假设有一个表格t，其中有一个索引字段id。如果一个事务T1使用临建锁锁住了id值为1的记录，那么其他事务在这个记录之前或之后插入新的记录时，会被阻塞，直到事务T1释放锁。 如果当事务A加锁成功之后就设置一个状态告诉后面的人，已经有人对表里的行加了一个排他锁了，你们不能对整个表加共享锁或排它锁了，那么后面需要对整个表加锁的人只需要获取这个状态就知道自己是不是可以对表加锁，避免了对整个索引树的每个节点扫描是否加锁，而这个状态就是意向锁。 意向共享锁 当一个事务试图对整个表进行加共享锁之前，首先需要获得这个表的意向共享锁。 意向排它锁 当一个事务试图对整个表进行加排它锁之前，首先需要获得这个表的意向排它锁。 InnoDB存储引擎的锁的算法Record lock：单个行记录上的锁Gap lock：间隙锁，锁定一个范围，不包括记录本身Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A.将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 MySQL死锁的原因和处理方法（1）表的死锁 产生原因: 用户A访问表A（锁住了表A），然后又访问表B；另⼀个用户B访问表B（锁住了表B），然后企图访问表A；这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。 解决方案： 对于数据库的多表操作时，尽量按照相同的顺序进行处理，尽量避免同时锁定两个资源，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。 （2）行级锁死锁 场景一：如果在事务中执行了⼀条没有索引条件的查询，引发全表扫描，把行级锁上升为全表记录锁定（等价于表级锁），多个这样的事务执行后，就很容易产生死锁和阻塞。 解决方案： SQL语句中不要使用太复杂的关联多表的查询；使用explain“执行计划”对SQL语句进行分析，对于有全表扫描和全表锁定的SQL语句，建立相应的索引进行优化。 场景二：两个事务分别想拿到对方持有的锁，互相等待，于是产生死锁 。 解决方案: 对索引加锁顺序的不⼀致很可能会导致死锁，所以如果可以，尽量以相同的顺序来访问索引记录和表。在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能； 慢查询都怎么优化？慢查询的优化首先要搞明白慢的原因是什么？是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？ 所以优化也是针对这三个方向来的， 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。 事务的基本特性和隔离级别事务基本特性ACID分别是： 原子性：是一个事务中的操作要么全部成功，要么全部失败。 一致性：数据库总是从一个一致性的状态转换到另外一个一致性的状态。 隔离性：一个事务的修改在最终提交前，对其他事务是不可见的。 持久性：一旦事务提交，所做的修改就会永久保存到数据库中。 隔离性有4个隔离级别，分别是： read uncommit 读未提交：可能会读到其他事务未提交的数据，可能会导致脏读、幻读或不可重复读。 read commit 读已提交：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 repeatable read 可重复复读：这是mysql的默认级别，就是每次读取结果都一样，但是有可能产生幻读。 serializable 可串行化：一般是不会使用的，他会给每一行读取的数据加锁，会导致大量超时和锁竞争的问题。 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 ACID靠什么保证的？ A原子性：由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql。 C一致性：由其他三大特性保证、程序代码要保证业务上的一致性。 I隔离性：由MVCC来保证。 D持久性：由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，宕机的时候可以从redo log恢复。redolog的刷盘会在系统空闲时进行。 什么是MVCC？多版本并发控制：读取数据时通过一种类似快照的方式将数据保存下来，这样读锁就和写锁不冲突了，不同的事务session会看到自己特定版本的数据。 MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作。其他两个隔离级别和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。 已提交读隔离级别下的事务在每次查询的开始都会生成一个独立的ReadView，而可重复读隔离级别则在第一次读的时候生成一个ReadView，之后的读都复用之前的ReadView。 这就是Mysql的MVCC,通过版本链，实现多版本，可并发读-写，写-读。通过ReadView生成策略的不同实现不同的隔离级别。 MySQL主从同步原理Mysql的主从复制中主要有三个线程： master（binlog dump thread）、 slave（I/O thread 、SQL thread） ，Master一条线程和Slave中的两条线程。 主节点 binlog，主从复制的基础是主库记录数据库的所有变更记录到 binlog。binlog 是数据库服务器启动的那一刻起，保存所有修改数据库结构或内容的一个文件。 主节点 binlog dump 线程，当 binlog 有变动时，binlog dump 线程读取其内容并发送给从节点。 从节点 I&#x2F;O线程接收 binlog 内容，并将其写入到 relay log 文件中。 从节点的SQL 线程读取 relay log 文件内容对数据更新进行重放，最终保证主从数据库的一致性。 注：主从节点使用 binglog 文件 + position 偏移量来定位主从同步的位置，从节点会保存其已接收到的偏移量，如果从节点发生宕机重启，则会自动从 position 的位置发起同步。 由于mysql默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已经处理，这样会产生一个问题就是假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。由此产生两个概念。 全同步复制 主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。 半同步复制 和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。 MySQL执行计划怎么看？执行计划就是sql的执行查询的顺序，以及如何使用索引查询，返回的结果集的行数。EXPLAIN SELECT * from A where X=? and Y=? 输出格式如下： 12345678mysql&gt; EXPLAIN SELECT `score`,`name` FROM `cus_order` ORDER BY `score` DESC;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+| 1 | SIMPLE | cus_order | NULL | ALL | NULL | NULL | NULL | NULL | 997572 | 100.00 | Using filesort |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+1 row in set, 1 warning (0.00 sec) id ：是一个有顺序的编号，是查询的顺序号，有几个 select 就显示几行。id的顺序是按 select 出现的顺序增长的。id列的值越大执行优先级越高越先执行，id列的值相同则从上往下执行，id列的值为NULL最后执行。 selectType 表示查询中每个select子句的类型。 SIMPLE： 表示此查询不包含 UNION 查询或子查询； PRIMARY： 表示此查询是最外层的查询（包含子查询）； SUBQUERY： 子查询中的第一个 SELECT； UNION： 表示此查询是 UNION 的第二或随后的查询； DEPENDENT UNION： UNION 中的第二个或后面的查询语句, 取决于外面的查询； UNION RESULT, UNION 的结果； DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果； DERIVED：衍生，表示导出表的SELECT（FROM子句的子查询）。 table：表示该语句查询的表。 type：优化sql的重要字段，也是我们判断sql性能和优化程度重要指标。他的取值类型范围： const：通过索引一次命中，匹配一行数据； system: 表中只有一行记录，相当于系统表； eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配； ref: 非唯一性索引扫描,返回匹配某个值的所有； range: 只检索给定范围的行，使用一个索引来选择行，一般用于between、&lt;、&gt;； index: 只遍历索引树； ALL: 表示全表扫描，这个类型的查询是性能最差的查询之一。 那么基本就是随着表的数量增多，执行效率越慢。 执行效率：ALL &lt; index &lt; range&lt; ref &lt; eq_ref &lt; const &lt; system。最好是避免ALL和index possible_keys：它表示Mysql在执行该sql语句的时候，可能用到的索引信息，仅仅是可能，实际不一定会用到。 key：此字段是 mysql 在当前查询时所真正使用到的索引。 他是possible_keys的子集。 key_len：表示查询优化器使用了索引的字节数，这个字段可以评估组合索引是否完全被使用，这也是我们优化sql时，评估索引的重要指标。 rows：mysql 查询优化器根据统计信息，估算该sql返回结果集需要扫描读取的行数，这个值相关重要，索引优化之后，扫描读取的行数越多，说明索引设置不对，或者字段传入的类型之类的问题，说明要优化空间越大。 filtered：返回结果的行占需要读到的行(rows列的值)的百分比，就是百分比越高，说明需要查询到数据越准确， 百分比越小，说明查询到的数据量大，而结果集很少。 extrausing filesort ：表示 mysql 对结果集进行外部排序，不能通过索引顺序达到排序效果。一般有using filesort都建议优化去掉，因为这样的查询 cpu 资源消耗大，延时大。using index：覆盖索引扫描，表示查询在索引树中就可查找所需数据，不用扫描表数据文件，往往说明性能不错。 using temporary：查询有使用临时表, 一般出现于排序， 分组和多表 join 的情况， 查询效率不高，建议优化。 using where ：sql使用了where过滤，效率较高。 七、Redis如何保证缓存和数据库数据的⼀致性？根据CAP理论，在保证可用性和分区容错性的前提下，无法保证一致性，所以缓存和数据库的绝对一致是不可能实现的，只能尽可能保存缓存和数据库的最终一致性。 （1）选择合适的缓存更新策略 1、删除缓存而不是更新缓存 当一个线程对缓存的key进行写操作的时候，如果其它线程进来读数据库的时候，读到的就是脏数据，产生了数据不一致问题。 相比较而言，删除缓存的速度比更新缓存的速度快很多，所用时间相对也少很多，读脏数据的概率也小很多。 2、先更数据，后删缓存 （2）缓存不一致处理 如果不是并发特别高，对缓存依赖性很强，其实一定程序的不一致是可以接受的。但是如果对一致性要求比较高，那就得想办法保证缓存和数据库中数据一致。 缓存和数据库数据不一致常见的两种原因： 缓存key删除失败 并发导致写入了脏数据 解决方案： 消息队列保证key被删除可以引入消息队列，把要删除的key或者删除失败的key丢尽消息队列，利用消息队列的重试机制，重试删除对应的key。 数据库订阅+消息队列保证key被删除可以用一个服务（比如阿里的 canal）去监听数据库的binlog，获取需要操作的数据。然后用一个公共的服务获取订阅程序传来的信息，进行缓存删除操作。 延时双删防止脏数据还有一种情况，是在缓存不存在的时候，写入了脏数据，这种情况在先删缓存，再更数据库的缓存更新策略下发生的比较多，解决方案是延时双删。 简单说，就是在第一次删除缓存之后，过了一段时间之后，再次删除缓存。 设置缓存过期时间兜底 这是一个朴素但是有用的办法，给缓存设置一个合理的过期时间，即使发生了缓存数据不一致的问题，它也不会永远不一致下去，缓存过期的时候，自然又会恢复一致。 如何保证本地缓存和分布式缓存的一致？在日常的开发中，我们常常采用两级缓存：本地缓存+分布式缓存。 所谓本地缓存，就是对应服务器的内存缓存，比如Caffeine，分布式缓存基本就是采用Redis。 Redis缓存，数据库发生更新，直接删除缓存的key即可，因为对于应用系统而言，它是一种中心化的缓存。 但是本地缓存，它是非中心化的，散落在分布式服务的各个节点上，没法通过客户端的请求删除本地缓存的key，所以得想办法通知集群所有节点，删除对应的本地缓存key。 可以采用消息队列的方式： 采用Redis本身的Pub&#x2F;Sub机制，分布式集群的所有节点订阅删除本地缓存频道，删除Redis缓存的节点，同时发布删除本地缓存消息，订阅者们订阅到消息后，删除对应的本地key。但是Redis的发布订阅不是可靠的，不能保证一定删除成功。 引入专业的消息队列，比如RocketMQ，保证消息的可靠性，但是增加了系统的复杂度。 设置适当的过期时间兜底，本地缓存可以设置相对短一些的过期时间。 怎么处理热key？ 什么是热Key？所谓的热key，就是访问频率比较的key。 对热key的处理，最关键的是对热点key的监控，可以从这些端来监控热点key: 客户端客户端其实是距离key“最近”的地方，因为Redis命令就是从客户端发出的，例如在客户端设置全局字典（key和调用次数），每次调用Redis命令时，使用这个字典进行记录。 代理端像Twemproxy、Codis这些基于代理的Redis分布式架构，所有客户端的请求都是通过代理端完成的，可以在代理端进行收集统计。 Redis服务端使用monitor命令统计热点key是很多开发和运维人员首先想到，monitor命令可以监控到Redis执行的所有命令。 只要监控到了热key，对热key的处理就简单了： 把热key打散到不同的服务器，降低压力 加入二级缓存，提前加载热key数据到内存中，如果redis宕机，走内存查询 大key问题处理 大key会造成什么问题呢？ 客户端耗时增加，甚至超时 对大key进行IO操作时，会严重占用带宽和CPU 造成Redis集群中数据倾斜 主动删除、被动删等，可能会导致阻塞 如何找到大key? bigkeys命令：使用bigkeys命令以遍历的方式分析Redis实例中的所有Key，并返回整体统计信息与每个数据类型中Top1的大Key redis-rdb-tools：redis-rdb-tools是由Python写的用来分析Redis的rdb快照文件用的工具，它可以把rdb快照文件生成json文件或者生成报表用来分析Redis的使用详情。 如何处理大key？ 删除大key 当Redis版本大于4.0时，可使用UNLINK命令安全地删除大Key，该命令能够以非阻塞的方式，逐步地清理传入的Key。 当Redis版本小于4.0时，避免使用阻塞式命令KEYS，而是建议通过SCAN命令执行增量迭代扫描key，然后判断进行删除。 压缩和拆分key 当vaule是string时，比较难拆分，则使用序列化、压缩算法将key的大小控制在合理范围内，但是序列化和反序列化都会带来更多时间上的消耗。 当value是string，压缩之后仍然是大key，则需要进行拆分，一个大key分为不同的部分，记录每个部分的key，使用multiget等操作实现事务读取。 当value是list&#x2F;set等集合类型时，根据预估的数据规模来进行分片，不同的元素计算后分到不同的片。 跳跃表详解（1）理解跳表 下图是一个简单的有序单链表，单链表的特性就是每个元素存放下一个元素的引用。即：通过第一个元素可以找到第二个元素，通过第二个元素可以找到第三个元素，依次类推，直到找到最后一个元素。 现在我们有个场景，想快速找到上图链表中的 10 这个元素，只能从头开始遍历链表，直到找到我们需要找的元素。查找路径：1、3、4、5、7、8、9、10。这样的查找效率很低，平均时间复杂度很高O(n)。那有没有办法提高链表的查找速度呢？如下图所示，我们从链表中每两个元素抽出来，加一级索引，一级索引指向了原始链表，即：通过一级索引 7 的down指针可以找到原始链表的 7 。那现在怎么查找 10 这个元素呢？ 先在索引找 1、4、7、9，遍历到一级索引的 9 时，发现 9 的后继节点是 13，比 10 大，于是不往后找了，而是通过 9 找到原始链表的 9，然后再往后遍历找到了我们要找的 10，遍历结束。有没有发现，加了一级索引后，查找路径：1、4、7、9、10，查找节点需要遍历的元素相对少了，我们不需要对 10 之前的所有数据都遍历，查找的效率提升了。 那如果加二级索引呢？如下图所示，查找路径：1、7、9、10。是不是找 10 的效率更高了？这就是跳表的思想，用“空间换时间”，通过给链表建立索引，提高了查找的效率。 特点: (1)、跳跃表的每一层都是一条有序的链表。 (2)、维护了多条节点路径。 (3)、最底层的链表包含所有元素。 (4)、跳跃表的空间复杂度为 O(n)。 (5)、跳跃表支持平均O(logN)、最坏O(N)复杂度的节点查找，还可以通过顺序性操作来批量处理节点。 Redis实现延时队列实现延时队列的思路如下： 生产者将需要延迟的消息 id 添加到 zset 中，其分数设置为“当前时间 + 需要延时的时间” 消费者不断轮询有序集合中的第一个元素与当前时间的大小，若超过当前时间，则认为延时已经满足，消费掉消息。 八、分布式&#x2F;微服务负载均衡算法1、轮询法将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。 2、随机法通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。 3、源地址哈希法源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。 4、加权轮询法不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。 5、加权随机法与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。 6、最小连接数法最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 类型： DNS 方式实现负载均衡 硬件负载均衡：F5 和 A10 软件负载均衡：Nginx 、 HAproxy 、 LVS 。 分布式架构下Session 共享方案1、采用无状态服务，抛弃session 2、存入cookie（有安全风险） 3、服务器之间进行 Session 同步 这样可以保证每个服务器上都有全部的 Session 信息，不过当服务器数量比较多的时候，同步是会有延迟甚至同步失败； 4、 IP 绑定策略 使用 Nginx （或其他复杂均衡软硬件）中的 IP 绑定策略，同一个 IP 只能在指定的同一个机器访问，但是这样做失去了负载均衡的意义，当挂掉一台服务器的时候，会影响一批用户的使用，风险很大； 5、使用 Redis 存储 把 Session 放到 Redis 中存储，虽然架构上变得复杂，并且需要多访问一次 Redis ，但是这种方案带来的好处也是很大的： 实现了 Session 共享； 可以水平扩展（增加 Redis 服务器）； 服务器重启 Session 不丢失（不过也要注意 Session 在 Redis 中的刷新&#x2F;失效机制）； 不仅可以跨服务器 Session 共享，甚至可以跨平台（例如网页端和 APP 端）。 分布式id生成方案1、UUID 当前日期和时间。【时间戳】 时钟序列。 【计数器】 全局唯一的IEEE机器识别号，如果有网卡，从网卡MAC地址获得，没有网卡以其他方式获得。【识别号】 优点： 代码简单，性能好（本地生成，没有网络消耗） 保证唯一（相对而言，重复概率极低可以忽略） 缺点： 每次生成的ID都是无序的，而且不是全数字，且无法保证趋势递增。 UUID生成的是字符串，字符串存储性能差，查询效率慢，写的时候由于不能产生顺序的append操作，需要进 行insert操作，导致频繁的页分裂，这种操作在记录占用空间比较大的情况下，性能下降比较大，还会增加读 取磁盘次数。 UUID长度过长，不适用于存储，耗费数据库性能。 ID无一定业务含义，可读性差。 有信息安全问题，有可能泄露mac地址。 2、数据库自增ID （1）单机模式： 优点： 实现简单，依靠数据库即可，成本小。 ID数字化，单调自增，满足数据库存储和查询性能。 具有一定的业务可读性。 缺点： 强依赖DB，存在单点问题，如果数据库宕机，则业务不可用。 DB生成ID性能有限，单点数据库压力大，无法扛高并发场景。 信息安全问题，比如暴露订单量，url查询改一下id查到别人的订单。 （2）数据库高可用：多主模式做负载，基于序列的起始值和步长设置，不同的初始值，相同的步长，步长大于节点数。 优点： 解决了ID生成的单点问题，同时平衡了负载。 缺点： 系统扩容困难：系统定义好步长之后，增加机器之后调整步长困难。 数据库压力大：每次获取一个ID都必须读写一次数据库。 主从同步的时候：电商下单-&gt;支付insert master db select数据 ，因为数据同步延迟导致查不到这个数据。加cache(不是最好的解决方式)数据要求比较严谨的话查master主库。 3、雪花算法 生成一个64bit的整型数字。第一位符号位固定为0，41位时间戳，10位workId，12位序列号，位数可以有不同实现。 优点： 每个毫秒值包含的ID值很多，不够可以变动位数来增加，性能佳（依赖workId的实现）。 时间戳值在高位，中间是固定的机器码，自增的序列在低位，整个ID是趋势递增的。 能够根据业务场景数据库节点布置灵活挑战bit位划分，灵活度高。 缺点： 强依赖于机器时钟，如果时钟回拨，会导致重复的ID生成，所以一般基于此的算法发现时钟回拨，都会抛异常处 理，阻止ID生成，这可能导致服务不可用。 如何实现接口的幂等性 insert前先select。在保存数据的接口中，在insert前，先根据requestId等字段先select一下数据。如果该数据已存在，则直接返回，如果不存在，才执行 insert操作。 唯一id。每次操作，都根据操作和内容生成唯一的id，在执行之前先判断id是否存在，如果不存在则执行后续操作，并且保存到数据库或者redis等。 服务端提供发送token的接口，业务调用接口前先获取token，然后调用业务接口请求时，把token携带过去，服务器判断token是否存在redis中，存在表示第一次请求，可以继续执行业务，执行业务完成后，最后需要把redis中的token删除。 建去重表。将业务中有唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了。 版本控制。增加版本号，当版本号符合时，才能更新数据。 状态控制。例如订单有状态已支付 未支付 支付中 支付失败，当处于未支付的时候才允许修改为支付中。 分布式锁。 跨域问题及解决方式跨域是指浏览器在发起网络请求时，会检查该请求所对应的协议、域名、端口和当前网页是否⼀致，如果不⼀致则浏览器会进行限制，比如在www.baidu.com 的某个网页中，如果使用ajax去访问www.jd.com 是不行的，但是如果是img、iframe、script等标签的src属性去访问则是可以的，之所以浏览器要做这层限制，是为了用户信息安全。但是如果开发者想要绕过这层限制也是可以的。 response添加header，比如resp.setHeader(“Access-Control-Allow-Origin”, “*”)，表示可以访问所有网站，不受是否同源的限制。 js的方式，该技术底层就是基于script标签来实现的，因为script标签是可以跨域的。 后台自己控制，先访问同域名下的接口，然后在接口中再去使用HTTPClient等工具去调用目标接口。 网关，和第三种方式类似，都是交给后台服务来进行跨域访问 。 分布式锁 ZooKeeper分布式锁 ZooKeeper可以用于实现分布式锁，以下是一种基于ZooKeeper的分布式锁的实现方式： 在ZooKeeper上创建一个锁节点，例如“&#x2F;lock”。 当一个进程需要获取锁时，在“&#x2F;lock”节点下创建一个顺序节点，例如“&#x2F;lock&#x2F;00000001”。 进程检查是否是第一个创建的节点，如果是，则表示它已经获得了锁；否则，它需要等待前面的节点释放锁。 当进程释放锁时，删除它创建的节点。 这种实现方式可以保证每个节点在获取锁时都是按顺序排队的。如果一个进程需要释放锁但是它不是第一个创建的节点，那么它需要删除它创建的节点并等待前面的节点释放锁。 Redis分布式锁 Redis实现分布式锁，是当前应用最广泛的分布式锁实现方式。Redis执行命令是单线程的，Redis实现分布式锁就是利用这个特性。 实现分布式锁最简单的一个命令：setNx(set if not exist)，如果不存在则更新： 1setNx resourceName value 加锁了之后如果机器宕机，那我这个锁就无法释放，所以需要加入过期时间，而且过期时间需要和setNx同一个原子操作，在Redis2.8之前需要用lua脚本，但是redis2.8之后redis支持nx和ex操作是同一原子操作。 1set resourceName value ex 5 nx Redission 当然，一般生产中都是使用Redission客户端，非常良好地封装了分布式锁的api，而且支持RedLock。 分布式限流一种基于Redis的Zset实现： 在Redis中使用有序集合（sorted set）实现分布式限流时，可以将IP地址作为key，访问次数作为score，最后一次访问的时间戳作为value。以下是详细的步骤说明： 创建一个有序集合，用于存储IP地址的访问次数和最后一次访问的时间戳（当前时间+限流时间）。 当有请求到达时，首先根据IP地址从有序集合中获取对应的访问次数和最后一次访问的时间戳。 判断获取到的访问次数是否已经超过了限制，并且距离最后一次访问的时间是否还在限制时间范围内。 如果访问次数已经超过限制或时间未到限制时间，表示该请求需要被限流，拒绝该请求。 如果访问次数未超过限制或时间已到限制时间，表示该请求可以通过限流，将该IP地址对应的访问次数加1，并更新最后一次访问的时间戳。 为了保证多个Redis节点之间的数据一致性，可以使用Redis的Lua脚本来执行上述操作，确保原子性。 通过以上步骤，使用Redis的有序集合可以实现分布式限流，其中key存储IP地址，value存储最后一次访问的时间戳，score存储访问次数。这样可以方便地统计每个IP的访问情况，并进行限流控制。 更多：限流方式","categories":["Java面试"]},{"title":"设计模式摘录","path":"/2023/10/05/设计模式摘录/","content":"《深入设计模式》一书阅读摘录。 1、设计模式是什么？设计模式是软件设计中常见问题的典型解决方案。 它们就像能根据需求进行调整的预制蓝图， 可用于解决代码中反复出现的设计问题。 2、创建型模式创建型模式提供了创建对象的机制， 能够提升已有代码的灵活性和可复用性。 （1）工厂方法模式工厂方法模式是一种创建型设计模式， 其在父类中提供一个创建对象的方法， 允许子类决定实例化对象的类型。 示例：父类提供了一个Transport接口，并提供deliver方法，子类通过继承父类实例化具体类型。 工厂方法模式优缺点： 你可以避免创建者和具体产品之间的紧密耦合。 单一职责原则。 你可以将产品创建代码放在程序的单一位置， 从而使得代码更容易维护。 开闭原则。 无需更改现有客户端代码， 你就可以在程序中引入新的产品类型。 应用工厂方法模式需要引入许多新的子类， 代码可能会因此变得更复杂。 最好的情况是将该模式引入创建者类的现有层次结构中。 （2）抽象工厂模式抽象工厂模式是一种创建型设计模式， 它能创建一系列相关的对象， 而无需指定其具体类。 示例： 1、声明抽象工厂——包含系列中所有产品构造方法的接口。 例如 create­Chair创建椅子 、 create­Sofa创建沙发和 create­Coffee­Table创建咖啡桌 。 这些方法必须返回抽象产品类型， 即我们之前抽取的那些接口： 椅子 ， 沙发和 咖啡桌等等。 2、我们都将基于 抽象工厂接口创建不同的工厂类。 每个工厂类都只能返回特定类别的产品， 例如， 现代家具工厂Modern­Furniture­Factory只能创建 现代椅子Modern­Chair 、 现代沙发Modern­Sofa和 现代咖啡桌Modern­Coffee­Table对象。 抽象工厂模式优缺点： 你可以确保同一工厂生成的产品相互匹配。 你可以避免客户端和具体产品代码的耦合。 单一职责原则。 你可以将产品生成代码抽取到同一位置， 使得代码易于维护。 开闭原则。 向应用程序中引入新产品变体时， 你无需修改客户端代码。 由于采用该模式需要向应用中引入众多接口和类， 代码可能会比之前更加复杂。 （3）建造者模式建造者模式是一种创建型设计模式， 使你能够分步骤创建复杂对象。 该模式允许你使用相同的创建代码生成不同类型和形式的对象。 示例：我们想要建造不同风格的房子，如果为每种可能的房子对象都创建一个子类， 这可能会导致程序变得过于复杂。 生成器模式建议将对象构造代码从产品类中抽取出来， 并将其放在一个名为生成器的独立对象中。 该模式会将对象构造过程划分为一组步骤， 比如 build­Walls创建墙壁和 build­Door创建房门创建房门等。 每次创建对象时， 你都需要通过生成器对象执行一系列步骤。 重点在于你无需调用所有步骤， 而只需调用创建特定对象配置所需的那些步骤即可。 生成器模式优缺点： 你可以分步创建对象， 暂缓创建步骤或递归运行创建步骤。 生成不同形式的产品时， 你可以复用相同的制造代码。 单一职责原则。 你可以将复杂构造代码从产品的业务逻辑中分离出来。 由于该模式需要新增多个类， 因此代码整体复杂程度会有所增加。 （4）原型模式原型模式是一种创建型设计模式， 使你能够复制已有对象， 而又无需使代码依赖它们所属的类。 示例：有丝分裂会产生一对完全相同的细胞。 原始细胞就是一个原型， 它在复制体的生成过程中起到了推动作用。 原型模式将克隆过程委派给被克隆的实际对象。 模式为所有支持克隆的对象声明了一个通用接口， 该接口让你能够克隆对象， 同时又无需将代码和对象所属类耦合。 通常情况下， 这样的接口中仅包含一个 克隆方法。 克隆方法会创建一个当前类的对象， 然后将原始对象所有的成员变量值复制到新建的类中。 你甚至可以复制私有成员变量， 因为绝大部分编程语言都允许对象访问其同类对象的私有成员变量。 支持克隆的对象即为原型。 原型模式优缺点： 你可以克隆对象， 而无需与它们所属的具体类相耦合。 你可以克隆预生成原型， 避免反复运行初始化代码。 你可以更方便地生成复杂对象。 你可以用继承以外的方式来处理复杂对象的不同配置。 克隆包含循环引用的复杂对象可能会非常麻烦。 （5）单例模式单例模式是一种创建型设计模式， 让你能够保证一个类只有一个实例， 并提供一个访问该实例的全局节点。 保证一个类只有一个实例。最常见的原因是控制某些共享资源 （例如数据库或文件） 的访问权限。 为该实例提供一个全局访问节点。 单例模式优缺点： 你可以保证一个类只有一个实例。 你获得了一个指向该实例的全局访问节点。 仅在首次请求单例对象时对其进行初始化。 违反了单一职责原则。 该模式同时解决了两个问题。 单例模式可能掩盖不良设计， 比如程序各组件之间相互了解过多等。 该模式在多线程环境下需要进行特殊处理， 避免多个线程多次创建单例对象。 单例的客户端代码单元测试可能会比较困难 单例模式的几种实现方式： 1、懒汉式，线程不安全 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 2、懒汉式，线程安全 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 3、饿汉式 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 4、双检锁&#x2F;双重校验锁 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 两次判空： 如果单例已经创建了，直接调用synchronized加锁比较耗性能。所以首先判断有没有创建，没创建再加锁。 第二层非空检查的原因是在同时多个线程调用时，A线程获得锁并创建成功实例，之后释放锁，前面一起竞争的B线程获得锁，首先判断非空，代表已经创建了，所以不会继续去创建实例。 volatile（内存可见性、禁止指令重排）： A、B两个线程创建单例，此时A已经赋值，但是没有完成变量初始化，而B线程看到instance已经赋值就拿来使用，因为instance没有完成初始化，所以使用过程中可能产生无法预料的后果。 3、行为型模式行为模式负责对象间的高效沟通和职责委派。 （1）观察者模式观察者模式是一种行为设计模式， 允许你定义一种订阅机制， 可在对象事件发生时通知多个 “观察” 该对象的其他对象。 示例：如果你订阅了一份杂志或报纸， 那就不需要再去报摊查询新出版的刊物了。 出版社 （即应用中的 “发布者”） 会在刊物出版后 （甚至提前） 直接将最新一期寄送至你的邮箱中。 观察者模式优缺点： 开闭原则。 你无需修改发布者代码就能引入新的订阅者类 （如果是发布者接口则可轻松引入发布者类）。 你可以在运行时建立对象之间的联系。 订阅者的通知顺序是随机的。 示例： 1、创建Subject类 12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List; public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 2、创建Observer类 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 3、创建实体观察者类 12345678910111213public class BinaryObserver extends Observer&#123; public BinaryObserver(Subject subject)&#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; System.out.println( &quot;Binary String: &quot; + Integer.toBinaryString( subject.getState() ) ); &#125;&#125; 4、使用 Subject 和实体观察者对象。 1234567891011121314public class ObserverPatternDemo &#123; public static void main(String[] args) &#123; Subject subject = new Subject(); //new HexaObserver(subject); //new OctalObserver(subject); new BinaryObserver(subject); System.out.println(&quot;First state change: 15&quot;); subject.setState(15); System.out.println(&quot;Second state change: 10&quot;); subject.setState(10); &#125;&#125; （2）策略模式在策略模式（Strategy Pattern）中一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。 在策略模式定义了一系列算法或策略，并将每个算法封装在独立的类中，使得它们可以互相替换。通过使用策略模式，可以在运行时根据需要选择不同的算法，而不需要修改客户端代码。 在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 示例：假如你需要前往机场。 你可以选择乘坐公共汽车、 预约出租车或骑自行车。 这些就是你的出行策略。 你可以根据预算或时间等因素来选择其中一种策略。 策略模式建议找出负责用许多不同方式完成特定任务的类， 然后将其中的算法抽取到一组被称为策略的独立类中。 名为上下文的原始类必须包含一个成员变量来存储对于每种策略的引用。 上下文并不执行任务， 而是将工作委派给已连接的策略对象。 策略模式优缺点： 你可以在运行时切换对象内的算法。 你可以将算法的实现和使用算法的代码隔离开来。 你可以使用组合来代替继承。 开闭原则。 你无需对上下文进行修改就能够引入新的策略。 如果你的算法极少发生改变， 那么没有任何理由引入新的类和接口。 使用该模式只会让程序过于复杂。 客户端必须知晓策略间的不同——它需要选择合适的策略。 示例： 1、创建一个接口。 12345//Strategy.javapublic interface Strategy &#123; public int doOperation(int num1, int num2);&#125; 2、创建实现接口的实体类。 12345678//OperationAdd.javapublic class OperationAdd implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 + num2; &#125; &#125; 12345678//OperationSubtract.javapublic class OperationSubtract implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 - num2; &#125; &#125; 12345678//OperationMultiply.javapublic class OperationMultiply implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 * num2; &#125; &#125; 3、创建 Context 类。 1234567891011//Context.javapublic class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2)&#123; return strategy.doOperation(num1, num2); &#125; &#125; 4、使用 Context 来查看当它改变策略 Strategy 时的行为变化。 1234567891011121314//StrategyPatternDemo.javapublic class StrategyPatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(new OperationAdd()); System.out.println(&quot;10 + 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationSubtract()); System.out.println(&quot;10 - 5 = &quot; + context.executeStrategy(10, 5)); context = new Context(new OperationMultiply()); System.out.println(&quot;10 * 5 = &quot; + context.executeStrategy(10, 5)); &#125;&#125; （3）模板方法模式模板方法模式是一种行为设计模式， 它在超类中定义了一个算法的框架， 允许子类在不修改结构的情况下重写算法的特定步骤。 示例：模板方法可用于建造大量房屋。 标准房屋建造方案中可提供几个扩展点， 允许潜在房屋业主调整成品房屋的部分细节。 模板方法模式建议将算法分解为一系列步骤， 然后将这些步骤改写为方法， 最后在 “模板方法” 中依次调用这些方法。 步骤可以是 抽象的， 也可以有一些默认的实现。 为了能够使用算法， 客户端需要自行提供子类并实现所有的抽象步骤。 如有必要还需重写一些步骤 （但这一步中不包括模板方法自身）。 模板方法模式优缺点： 你可仅允许客户端重写一个大型算法中的特定部分， 使得算法其他部分修改对其所造成的影响减小。 你可将重复代码提取到一个超类中。 部分客户端可能会受到算法框架的限制。 通过子类抑制默认步骤实现可能会导致违反里氏替换原则。 模板方法中的步骤越多， 其维护工作就可能会越困难。 4、结构型模式结构型模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。 5、六大原则单一职责：对象设计要求独立，不能设计万能对象。 开闭原则：对象修改最小化。 里式替换：程序扩展中抽象被具体可以替换（接口、父类、可以被实现类对象、子类替换对象） 迪米特：高内聚，低耦合。尽量不要依赖细节。 依赖倒置：面向抽象编程。也就是参数传递，或者返回值，可以使用父类类型或者接口类型。从广义上讲：基于接口编程，提前设计好接口框架。 接口隔离：接口设计大小要适中。过大导致污染，过小，导致调用麻烦","categories":["系统设计"]},{"title":"微信&支付宝付款实现","path":"/2023/10/05/微信&支付宝付款实现/","content":"微信&amp;支付宝付款实现，包括API接入指引和相关方法实现。 一、微信支付介绍和接入指引 1、微信支付产品介绍 微信支付产品介绍 1.1、付款码支付 用户展示微信钱包内的“付款码”给商家，商家扫描后直接完成支付，适用于线下面对面收银的场景。 1.2、JSAPI支付 线下场所：商户展示一个支付二维码，用户使用微信扫描二维码后，输入需要支付的金额，完成支付。 公众号场景：用户在微信内进入商家公众号，打开某个页面，选择某个产品，完成支付。 PC网站场景：在网站中展示二维码，用户使用微信扫描二维码，输入需要支付的金额，完成支付。 特点：用户在客户端输入支付金额 1.3、小程序支付 在微信小程序平台内实现支付的功能。 1.4、Native支付 Native支付是指商户展示支付二维码，用户再用微信“扫一扫”完成支付的模式。这种方式适用于PC网 站。 特点：商家预先指定支付金额 1.5、APP支付 商户通过在移动端独立的APP应用程序中集成微信支付模块，完成支付。 1.6、刷脸支付 用户在刷脸设备前通过摄像头刷脸、识别身份后进行的一种支付方式。 2、接入指引2.1、获取商户号 微信商户平台：https://pay.weixin.qq.com/ 场景：Native支付 步骤：提交资料 &#x3D;&gt; 签署协议 &#x3D;&gt; 获取商户号 2.2、获取APPID 微信公众平台：https://mp.weixin.qq.com/ 步骤：注册服务号 &#x3D;&gt; 服务号认证 &#x3D;&gt; 获取APPID &#x3D;&gt; 绑定商户号 2.3、获取API秘钥 APIv2版本的接口需要此秘钥 步骤：登录商户平台 &#x3D;&gt; 选择 账户中心 &#x3D;&gt; 安全中心 &#x3D;&gt; API安全 &#x3D;&gt; 设置API密钥 2.4、获取APIv3秘钥 APIv3版本的接口需要此秘钥 步骤：登录商户平台 &#x3D;&gt; 选择 账户中心 &#x3D;&gt; 安全中心 &#x3D;&gt; API安全 &#x3D;&gt; 设置APIv3 密钥 随机密码生成工具：https://suijimimashengcheng.bmcx.com/ 2.5、申请商户API证书 APIv3版本的所有接口都需要； APIv2版本的高级接口需要（如：退款、企业红包、企业付款等） 步骤：登录商户平台 &#x3D;&gt; 选择 账户中心 &#x3D;&gt; 安全中心 &#x3D;&gt; API安全 &#x3D;&gt; 申请API证书 2.6、获取微信平台证书 可以预先下载，也可以通过编程的方式获取。后面的课程中，我们会通过编程的方式来获取。 注意：以上所有API秘钥和证书需妥善保管防止泄露 二、支付安全（证书&#x2F;秘钥&#x2F;签名） 1、信息安全的基础 - 机密性 明文：加密前的消息叫“明文”（plain text） 密文：加密后的文本叫“密文”（cipher text） 密钥：只有掌握特殊“钥匙”的人，才能对加密的文本进行解密，这里的“钥匙”就叫做“密钥”（key） “密钥”就是一个字符串，度量单位是“位”（bit），比如，密钥长度是 128，就是 16 字节的二进制串 加密：实现机密性最常用的手段是“加密”（encrypt） 按照密钥的使用方式，加密可以分为两大类：对称加密和非对称加密。 解密：使用密钥还原明文的过程叫“解密”（decrypt） 加密算法：加密解密的操作过程就是“加密算法” 所有的加密算法都是公开的，而算法使用的“密钥”则必须保密 2、对称加密和非对称加密 对称加密 特点：只使用一个密钥，密钥必须保密，常用的有 AES算法 优点：运算速度快 缺点：秘钥需要信息交换的双方共享，一旦被窃取，消息会被破解，无法做到安全的密钥交换 非对称加密 特点：使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，常用的有 RSA 优点：黑客获取公钥无法破解密文，解决了密钥交换的问题 缺点：运算速度非常慢，混合加密，实际场景中把对称加密和非对称加密结合起来使用。 3、身份认证 公钥加密，私钥解密的作用是加密信息 私钥加密，公钥解密的作用是身份认证 示例：bob给susan写了一封信，为了加密，bob用它的公钥进行加密，susan拥有bob的私钥，因此可以进行解密。而如果bob用私钥进行加密，bob的其他好友都有公钥，并且能够解密，说明这封信是bob写的，相当于身份认证。 4、摘要算法（Digest Algorithm）摘要算法就是我们常说的散列函数、哈希函数（Hash Function），它能够把任意长度的数据“压缩”成 固定长度、而且独一无二的“摘要”字符串，就好像是给这段数据生成了一个数字“指纹”。 作用： 保证信息的完整性和唯一性 ，确保信息没有被篡改。 特性： 不可逆：只有算法，没有秘钥，只能加密，不能解密 难题友好性：想要破解，只能暴力枚举 发散性：只要对原文进行一点点改动，摘要就会发生剧烈变化 抗碰撞性：原文不同，计算后的摘要也要不同 常见摘要算法： MD5、SHA1、SHA2（SHA224、SHA256、SHA384） 5、数字签名数字签名是使用私钥对摘要加密生成签名，需要由公钥将签名解密后进行验证，实现身份认证和不可否认签名和验证签名的流程： https://zhuanlan.zhihu.com/p/403704980 6、数字证书数字证书解决“公钥的信任”问题，可以防止黑客伪造公钥。 不能直接分发公钥，公钥的分发必须使用数字证书，数字证书由CA颁发 。 https协议中的数字证书： 7、微信APIv3证书商户证书： 商户API证书是指由商户申请的，包含商户的商户号、公司名称、公钥信息的证书。 商户证书在商户后台申请：https://pay.weixin.qq.com/index.php/core/cert/api_cert#/ 8、API密钥和APIv3密钥都是对称加密需要使用的加密和解密密钥，一定要保管好，不能泄露。 API密钥对应V2版本的API APIv3密钥对应V3版本的API 三、基础支付API V31、引入支付参数1、定义微信支付相关参数 将资料文件夹中的 wxpay.properties 复制到resources目录中这个文件定义了之前我们准备的微信支付相关的参数，例如商户号、APPID、API秘钥等 。 2、读取支付参数 将资料文件夹中的 config 目录中的 WxPayConfig.java 复制到源码目录中 2、加载商户私钥1、复制商户私钥 将下载的私钥文件复制到项目根目录下： 2、引入SDK https://pay.weixin.qq.com/wiki/doc/apiv3/wechatpay/wechatpay6_0.shtml 我们可以使用官方提供的 SDK，帮助我们完成开发。实现了请求签名的生成和应答签名的验证。 3、获取商户私钥 https://github.com/wechatpay-apiv3/wechatpay-apache-httpclient （如何加载商户私钥） 3、获取签名验证器和HttpClient 1、证书密钥使用说明 https://pay.weixin.qq.com/wiki/doc/apiv3_partner/wechatpay/wechatpay3_0.shtml 2、获取签名验证器 https://github.com/wechatpay-apiv3/wechatpay-apache-httpclient （定时更新平台证书功能） 平台证书：平台证书封装了微信的公钥，商户可以使用平台证书中的公钥进行验签。 签名验证器：帮助我们进行验签工作，我们单独将它定义出来，方便后面的开发。 3、 获取 HttpClient 对象 https://github.com/wechatpay-apiv3/wechatpay-apache-httpclient （定时更新平台证书功能） HttpClient 对象：是建立远程连接的基础，我们通过SDK创建这个对象。 4、API字典和相关工具1、API列表 https://pay.weixin.qq.com/wiki/doc/apiv3/open/pay/chapter2_7_3.shtml 我们的项目中要实现以下所有API的功能。 2、接口规则 https://pay.weixin.qq.com/wiki/doc/apiv3/wechatpay/wechatpay2_0.shtml 微信支付 APIv3 使用 JSON 作为消息体的数据交换格式。 5、Native下单API5.1、Native支付流程https://pay.weixin.qq.com/wiki/doc/apiv3/apis/chapter3_4_4.shtml 5.2、签名和验签原理签名生成流程：https://pay.weixin.qq.com/wiki/doc/apiv3/wechatpay/wechatpay4_0.shtml 签名验证流程：https://pay.weixin.qq.com/wiki/doc/apiv3/wechatpay/wechatpay4_1.shtml 6、支付通知API6.1、内网穿透ngrok是一款用于实现内网穿透的工具。它可以帮助你将本地部署的服务器暴露给公网，使外部网络可以访问到你的本地服务。 支付通知API：https://pay.weixin.qq.com/wiki/doc/apiv3/apis/chapter3_4_5.shtml 流程： 启用ngrok内网穿透 设置通知地址 创建通知接口 通知验签 通知数据解密 处理订单 处理重复通知，在处理订单时加数据锁。 6.2、商户定时查询本地订单支付成功后，商户侧查询本地数据库，订单是否支付成功 。 1、后端定义商户查单接口 2、前端定时轮询查单 在二维码展示页面，前端定时轮询查询订单是否已支付，如果支付成功则跳转到订单页面 。 7、用户取消订单API流程： 添加取消订单接口 调用微信支付的关单接口（创建远程请求对象、组装json请求体、完成签名并执行请求） 处理订单状态 8、微信支付查单API商户后台未收到异步支付结果通知时，商户应该主动调用《微信支付查单接口》，同步订单状态 。 流程： 集成Spring Task启动定时任务查找超时未支付的订单 调用微信查单接口获取订单状态，如已支付，更新订单状态，若未支付，调用关单接口。 9、申请退款API文档：https://pay.weixin.qq.com/wiki/doc/apiv3/apis/chapter3_4_9.shtml 流程： 创建退款单 调用退款API 更新订单、退款单 10、更多同下单流程，同样有查询退款API、退款结果通知API、申请和下载账单API。","categories":["项目实战"]},{"title":"自定义注解实现AOP切面","path":"/2023/10/05/自定义注解实现AOP切面/","content":"面向切面编程，Spring AOP核心概念、使用示例。 1、基础概念1.1、切面(Aspect)首先要理解‘切’字，需要把对象想象成一个立方体，传统的面向对象思维，类定义完成之后（封装）。每次实例化一个对象，对类定义中的成员变量赋值，就相当于对这个立方体进行了一个定义，定义完成之后，那个对象就在那里，不卑不亢，不悲不喜，等着被使用，等着被回收。 面向切面编程则是指，对于一个我们已经封装好的类，我们可以在编译期间或在运行期间，对其进行切割，把立方体切开，在原有的方法里面添加（织入）一些新的代码，对原有的方法代码进行一次增强处理。而那些增强部分的代码，就被称之为切面，如下面代码实例中的通用日志处理代码，常见的还有事务处理、权限认证等等。 2、切入点(PointCut)要对哪些类中的哪些方法进行增强，进行切割，指的是被增强的方法。即要切哪些东西。 3、连接点(JoinPoint)我们知道了要切哪些方法后，剩下的就是什么时候切，在原方法的哪一个执行阶段加入增加代码，这个就是连接点。如方法调用前，方法调用后，发生异常时等等。 4、通知(Advice)通知被织入方法，该如何被增强。定义切面的具体实现。那么这里面就涉及到一个问题，空间（切哪里）和时间（什么时候切，在何时加入增加代码），空间我们已经知道了就是切入点中定义的方法，而什么时候切，则是连接点的概念，如下面实例中，通用日志处理（切面），@Pointcut规则中指明的方法即为切入点，@Before、@After是连接点，而下面的代码就是对应通知。 1234@Before(&quot;cutMethod()&quot;)public void begin() &#123; System.out.println(&quot;==@Before== lingyejun blog logger : begin&quot;);&#125; 5、目标对象(Target Object)被一个或多个切面所通知的对象，即为目标对象。 6、AOP代理对象(AOP Proxy Object)AOP代理是AOP框架所生成的对象，该对象是目标对象的代理对象。代理对象能够在目标对象的基础上，在相应的连接点上调用通知。 7、织入(Weaving)将切面切入到目标方法之中，使目标方法得到增强的过程被称之为织入。 2、示例插曲：IDEA右键新建时，选项竟然没有Java Class 背景：利用aop实现日志切面。 步骤： 1、导入切面需要的依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 2、自定义注解 AOPLogTarget：描述了注解修饰的对象范围，有如下取值： METHOD：用于描述方法 PACKAGE：用于描述包 PARAMETER：用于描述方法变量 TYPE：用于描述类、接口或enum类型 Retention：表示注解保留时间长短 SOURCE：在源文件中有效，编译过程中会被忽略 CLASS：随源文件一起编译在class文件中，运行时忽略 RUNTIME：在运行时有效 只有定义为 RetentionPolicy.RUNTIME（在运行时有效）时，我们才能通过反射获取到注解，然后根据注解的一系列值，变更不同的操作。 12345678910111213// 指定注解使用在方法上@Target(ElementType.METHOD)// 指定生效至运行时@Retention(RetentionPolicy.RUNTIME)public @interface AOPLog &#123; /** * 指定是否详情显示 * true 显示详情, 默认false * * @return */ boolean isDetail() default false;&#125; 3、定义切面类补充：切入点表达式 1）bean表达式 bean表达式一般应用于类级别，实现粗粒度的切入点定义，案例分析： bean(“userServiceImpl”) 指定一个userServiceImpl类中所有方法。 bean(“*ServiceImpl”) 指定所有后缀为ServiceImpl的类中所有方法。 2）within表达式 within表达式应用于类级别，实现粗粒度的切入点表达式定义，案例分析： within(“aop.service.UserServiceImpl”) 指定当前包中这个类内部的所有方法。 within(“aop.service.*”) 指定当前目录下的所有类的所有方法。 within(“aop.service…*”) 指定当前目录以及子目录中类的所有方法。 3）execution表达式 execution表达式应用于方法级别，实现细粒度的切入点表达式定义，案例分析：语法：execution(返回值类型 包名.类名.方法名(参数列表))。 execution(void aop.service.UserServiceImpl.addUser()) 匹配addUser方法。 execution(void aop.service.PersonServiceImpl.addUser(String)) 方法参数必须为String的addUser方法。 execution(* aop.service…*.*(…)) 万能配置。 4）@annotation表达式 @annotaion表达式应用于方法级别，实现细粒度的切入点表达式定义，案例分析 @annotation(anno.RequiredLog) 匹配有此注解描述的方法。 @annotation(anno.RequiredCache) 匹配有此注解描述的方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Aspect@Componentpublic class AOPLogAspect &#123; private static Logger log = LoggerFactory.getLogger(AOPLogAspect.class); /** * 指定切点, 切点的位置是存在该注解com.example.aoptest.AOPLog */ @Pointcut(&quot;@annotation(com.example.aoptest.AOPLog)&quot;) public void logPointCut() &#123; &#125; /** * 环绕通知, 该处写具体日志逻辑 * * @param joinPoint */ @Around(&quot;logPointCut()&quot;) public void logAround(ProceedingJoinPoint joinPoint) &#123; MethodSignature signature = (MethodSignature) joinPoint.getSignature(); // 获取方法名称 String methodName = signature.getName(); // 获取入参 Object[] param = joinPoint.getArgs(); StringBuilder sb = new StringBuilder(); for (Object o : param) &#123; sb.append(o).append(&quot;; &quot;); &#125; log.info(&quot;进入方法[&#123;&#125;], 参数有[&#123;&#125;]&quot;, methodName, sb.toString()); String resp = &quot;&quot;; try &#123; Object proceed = joinPoint.proceed(); resp = JSON.toJSONString(proceed, SerializerFeature.WriteMapNullValue); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; // 获取方法上的注解，判断如果isDetail值为true，则打印结束日志 Method method = signature.getMethod(); AOPLog annotation = method.getAnnotation(AOPLog.class); boolean isDetail = annotation.isDetail(); if (isDetail) &#123; log.info(&quot;方法[&#123;&#125;]执行结束, 返回值[&#123;&#125;]&quot;, methodName, resp); &#125; &#125;&#125; 4. 编写测试接口12345678910111213141516@RestControllerpublic class AopLogController &#123; // 指定注解@AOPLog @AOPLog @GetMapping(&quot;/testAOP&quot;) public String testAOPLog() &#123; return &quot;ok&quot;; &#125; // 指定注解@AOPLog, 同时isDetail = true @AOPLog(isDetail = true) @GetMapping(&quot;/testAOPLogDetail&quot;) public String testAOPLogDetail() &#123; return &quot;ok&quot;; &#125;&#125;","categories":["Spring Boot"]},{"title":"算法输入输出测试","path":"/2023/10/05/算法输入输出测试/","content":"牛客机考算法题输入输出case。 1、A+B(1)输入描述： 输入包括两个正整数a,b(1 &lt;&#x3D; a, b &lt;&#x3D; 1000),输入数据包括多组。 输出描述： 输出a+b的结果 示例1 输入： 1 510 20 输出： 6 30 1234567891011121314import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) &#123; // 注意 while 处理多个 case int a = in.nextInt(); int b = in.nextInt(); System.out.println(a + b); &#125; &#125;&#125; 2、A+B(2)输入描述： 12输入第一行包括一个数据组数t(1 &lt;= t &lt;= 100)接下来每行包括两个正整数a,b(1 &lt;= a, b &lt;= 1000) 输出描述： 输出a+b的结果 示例1 输入： 2 1 5 10 20 输出： 6 30 1234567891011121314151617import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 int n=in.nextInt(); while (in.hasNextInt()) &#123; // 注意 while 处理多个 case for(int i=0;i&lt;n;i++)&#123; int a = in.nextInt(); int b = in.nextInt(); System.out.println(a + b); &#125; &#125; &#125;&#125; 3、A+B(3)输入描述： 输入包括两个正整数a,b(1 &lt;&#x3D; a, b &lt;&#x3D; 10^9),输入数据有多组, 如果输入为0 0则结束输入 输出描述： 输出a+b的结果 示例 输入： 1 5 10 20 0 0 输出： 6 30 1234567891011121314151617import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) &#123; // 注意 while 处理多个 case int a = in.nextInt(); int b = in.nextInt(); if(a==0||b==0)&#123; break; &#125; System.out.println(a + b); &#125; &#125;&#125; 4、A+B(4)输入描述： 123输入数据包括多组。每组数据一行,每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100), n为0的时候结束输入。接下来n个正整数,即需要求和的每个正整数。 输出描述： 每组数据输出求和的结果 示例1 输入： 4 1 2 3 4 5 1 2 3 4 5 0 输出： 10 15 123456789101112131415161718192021import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 int sum=0; while (in.hasNextLine()) &#123; // 注意 while 处理多个 case int n=in.nextInt(); if(n==0)&#123; break; &#125; for(int i=0;i&lt;n;i++)&#123; sum+=in.nextInt(); &#125; System.out.println(sum); sum=0; &#125; &#125;&#125; 5、A+B(5)输入描述： 1234输入的第一行包括一个正整数t(1 &lt;= t &lt;= 100), 表示数据组数。接下来t行, 每行一组数据。每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100)。接下来n个正整数, 即需要求和的每个正整数。 输出描述： 每组数据输出求和的结果 示例1 输入例子： 2 4 1 2 3 4 5 1 2 3 4 5 输出例子： 10 15 1234567891011121314151617181920212223import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int row=in.nextInt(); int count=0; // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextLine()) &#123; // 注意 while 处理多个 case int sum=0; int size=in.nextInt(); for(int i=0;i&lt;size;i++)&#123; sum+=in.nextInt(); &#125; System.out.println(sum); count++; if(count==row)&#123; break; &#125; &#125; &#125;&#125; 6、A+B(6)输入描述： 123输入数据有多组, 每行表示一组输入数据。每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100)。接下来n个正整数, 即需要求和的每个正整数。 输出描述： 每组数据输出求和的结果 示例1 输入例子： 4 1 2 3 4 5 1 2 3 4 5 输出例子： 10 15 1234567891011121314151617import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) &#123; // 注意 while 处理多个 case int size=in.nextInt(); int sum=0; for(int i=0;i&lt;size;i++)&#123; sum+=in.nextInt(); &#125; System.out.println(sum); &#125; &#125;&#125; 7、A+B(7)输入描述： 123输入数据有多组, 每行表示一组输入数据。每行不定有n个整数，空格隔开。(1 &lt;= n &lt;= 100)。 输出描述： 每组数据输出求和的结果 示例1 输入例子： 1 2 3 4 5 0 0 0 0 0 输出例子： 6 9 0 1234567891011121314151617import java.util.Scanner;// 注意类名必须为 Main, 不要有任何 package xxx 信息public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); // 注意 hasNext 和 hasNextLine 的区别 while (in.hasNextInt()) &#123; // 注意 while 处理多个 case int sum=0; String[] str=in.nextLine().split(&quot; &quot;); for(int n =0;n&lt;str.length;n++)&#123; sum += Integer.parseInt(str[n]); &#125; System.out.println(sum); &#125; &#125;&#125; 8、字符串排序(1)对输入的字符串进行排序后输出 输入描述： 123输入有两行，第一行n第二行是n个字符串，字符串之间用空格隔开 输出描述： 输出一行排序后的字符串，空格隔开，无结尾空格 示例1 输入例子： 5c d a bb e 输出例子： a bb c d e 1234567891011121314151617import java.util.*;public class Main&#123; public static void main(String[] args)&#123; Scanner sc = new Scanner(System.in); int countWord = sc.nextInt(); while(sc.hasNextLine())&#123; String[] str = sc.nextLine().split(&quot; &quot;); Arrays.sort(str); StringBuffer sb = new StringBuffer(); for(String s:str)&#123; sb.append(s).append(&quot; &quot;); &#125; System.out.print(sb.substring(0,sb.length()-1)); &#125; &#125;&#125; 9、字符串排序(2)对输入的字符串进行排序后输出 输入描述： 123多个测试用例，每个测试用例一行。每行通过空格隔开，有n个字符，n＜100 输出描述： 对于每组测试用例，输出一行排序过的字符串，每个字符串通过空格隔开 示例1 输入例子： a c bbf ddddnowcoder 输出例子： a bb c dddd f nowcoder 123456789101112131415import java.util.*; public class Main&#123; public static void main(String[] args)&#123; Scanner sc = new Scanner(System.in); while(sc.hasNextLine())&#123; String[] str = sc.nextLine().split(&quot; &quot;); Arrays.sort(str); StringBuffer sb = new StringBuffer(); for(String s:str)&#123; sb.append(s).append(&quot; &quot;); &#125; System.out.println(sb.substring(0,sb.length()-1));&#125; &#125; 10、字符串排序(3)对输入的字符串进行排序后输出 输入描述： 12多个测试用例，每个测试用例一行。每行通过,隔开，有n个字符，n＜100 输出描述： 对于每组用例输出一行排序后的字符串，用’,’隔开，无结尾空格 示例1 输入例子： a,c,bb f,dddd nowcoder 输出例子： a,bb,c dddd,f nowcoder 123456789101112131415161718import java.util.*;public class Main &#123; public static void main(String[] args)&#123; Scanner sc = new Scanner(System.in); while(sc.hasNextLine())&#123; String[] arrs = sc.nextLine().split(&quot;,&quot;); Arrays.sort(arrs); for(int i =0;i&lt;arrs.length;i++)&#123; if(i == arrs.length -1)&#123; System.out.print(arrs[i]); &#125;else&#123; System.out.print(arrs[i]+&quot;,&quot;); &#125; &#125; System.out.println(); &#125; &#125; &#125;","categories":["数据结构与算法"]},{"title":"Swagger接口文档使用","path":"/2023/10/05/Swagger接口文档使用/","content":"Spring Boot集成Swagger显示后端服务方法RestFul接口文档、方便功能测试和前端联调。 1、Swagger简介 前后端分离 前端 -&gt; 前端控制层、视图层 后端 -&gt; 后端控制层、服务层、数据访问层 前后端通过API进行交互 前后端相对独立且松耦合 产生的问题 前后端集成，前端或者后端无法做到“及时协商，尽早解决”，最终导致问题集中爆发。 解决方案 首先定义schema [ 计划的提纲 ]，并实时跟踪最新的API，降低集成风险。 Swagger 号称世界上最流行的API框架 Restful Api 文档在线自动生成器 &#x3D;&gt; API 文档 与API 定义同步更新 直接运行，在线测试API 支持多种语言 （如：Java，PHP等） 官网：https://swagger.io/ 2、SpringBoot集成Swagger SpringBoot集成Swagger &#x3D;&gt; springfox，两个jar包 Springfox-swagger2 swagger-springmvc 要求：jdk 1.8 + 否则swagger2无法运行 步骤： 1、新建一个SpringBoot-web项目 2、添加Maven依赖 123456789101112&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger-ui --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 注：swagger2.9.2版本需要在spring boot2.6之前的版本才能完美运行。 123&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.5.7&lt;/version&gt; 3、编写HelloController，测试确保运行成功！ 4、要使用Swagger，我们需要编写一个配置类-SwaggerConfig来配置 Swagger 1234@Configuration //配置类@EnableSwagger2// 开启Swagger2的自动配置public class SwaggerConfig &#123; &#125; 5、访问测试 ：http://localhost:8080/swagger-ui.html ，可以看到swagger的界面； 3、配置Swagger 3.1、初步集成1、Swagger实例Bean是Docket，所以通过配置Docket实例来配置Swaggger。 在SwaggerConfig类中添加如下代码： 1234@Bean //配置docket以配置Swagger具体参数public Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2);&#125; 2、可以通过apiInfo()属性配置文档信息 1234567891011121314//配置文档信息private ApiInfo apiInfo() &#123; Contact contact = new Contact(&quot;联系人名字&quot;, &quot;http://xxx.xxx.com/联系人访问链接&quot;, &quot;联系人邮箱&quot;); return new ApiInfo( &quot;Swagger学习&quot;, // 标题 &quot;学习演示如何配置Swagger&quot;, // 描述 &quot;v1.0&quot;, // 版本 &quot;http://terms.service.url/组织链接&quot;, // 组织链接 contact, // 联系人信息 &quot;Apach 2.0 许可&quot;, // 许可 &quot;许可链接&quot;, // 许可连接 new ArrayList&lt;&gt;()// 扩展 );&#125; 3、Docket 实例关联上 apiInfo() 1234@Beanpublic Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo());&#125; 4、重启项目，访问测试 http://localhost:8080/swagger-ui.html 看下效果； 3.2、配置扫描接口1、构建Docket时通过select()方法配置怎么扫描接口。 12345678@Beanpublic Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select()// 通过.select()方法，去配置扫描接口,RequestHandlerSelectors配置如何扫描接口 .apis(RequestHandlerSelectors.basePackage(&quot;com.example.swaggertest.controller&quot;)) .build();&#125; 2、重启项目测试，由于我们配置根据包的路径扫描接口，所以我们只能看到一个类 3、除了通过包路径配置扫描接口外，还可以通过配置其他方式扫描接口： 1234567any() // 扫描所有，项目中的所有接口都会被扫描到none() // 不扫描接口// 通过方法上的注解扫描，如withMethodAnnotation(GetMapping.class)只扫描get请求withMethodAnnotation(final Class&lt;? extends Annotation&gt; annotation)// 通过类上的注解扫描，如.withClassAnnotation(Controller.class)只扫描有controller注解的类中的接口withClassAnnotation(final Class&lt;? extends Annotation&gt; annotation)basePackage(final String basePackage) // 根据包路径扫描接口 4、除此之外，我们还可以配置接口扫描过滤： 12345678910@Beanpublic Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select()// 通过.select()方法，去配置扫描接口,RequestHandlerSelectors配置如何扫描接口 .apis(RequestHandlerSelectors.basePackage(&quot;com.kuang.swagger.controller&quot;)) // 配置如何通过path过滤,即这里只扫描请求以/kuang开头的接口 .paths(PathSelectors.ant(&quot;/kuang/**&quot;)) .build();&#125; 5、这里的可选值还有 1234any() // 任何请求都扫描none() // 任何请求都不扫描regex(final String pathRegex) // 通过正则表达式控制ant(final String antPattern) // 通过ant()控制 3.3、配置Swagger开关1、通过enable()方法配置是否启用swagger，如果是false，swagger将不能在浏览器中访问了 1234567891011@Beanpublic Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .enable(false) //配置是否启用Swagger，如果是false，在浏览器将无法访问 .select()// 通过.select()方法，去配置扫描接口,RequestHandlerSelectors配置如何扫描接口 .apis(RequestHandlerSelectors.basePackage(&quot;com.kuang.swagger.controller&quot;)) // 配置如何通过path过滤,即这里只扫描请求以/kuang开头的接口 .paths(PathSelectors.ant(&quot;/kuang/**&quot;)) .build();&#125; 2、如何动态配置当项目处于test、dev环境时显示swagger，处于prod时不显示？ 1234567891011121314151617@Beanpublic Docket docket(Environment environment) &#123; // 设置要显示swagger的环境 Profiles of = Profiles.of(&quot;dev&quot;, &quot;test&quot;); // 判断当前是否处于该环境 // 通过 enable() 接收此参数判断是否要显示 boolean b = environment.acceptsProfiles(of); return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .enable(b) //配置是否启用Swagger，如果是false，在浏览器将无法访问 .select()// 通过.select()方法，去配置扫描接口,RequestHandlerSelectors配置如何扫描接口 .apis(RequestHandlerSelectors.basePackage(&quot;com.kuang.swagger.controller&quot;)) // 配置如何通过path过滤,即这里只扫描请求以/jun开头的接口 .paths(PathSelectors.ant(&quot;/jun/**&quot;)) .build();&#125; 3、可以在项目中增加一个dev的配置文件查看效果！ 3.4、配置API分组 1、如果没有配置分组，默认是default。通过groupName()方法即可配置分组： 123456@Beanpublic Docket docket(Environment environment) &#123; return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()) .groupName(&quot;hello&quot;) // 配置分组 // 省略配置....&#125; 2、重启项目查看分组 3、如何配置多个分组？配置多个分组只需要配置多个docket即可： 123456789101112@Beanpublic Docket docket1()&#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;group1&quot;);&#125;@Beanpublic Docket docket2()&#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;group2&quot;);&#125;@Beanpublic Docket docket3()&#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;group3&quot;);&#125; 4、重启项目查看即可 3.5、实体配置1、新建一个实体类 1234567@ApiModel(&quot;用户实体&quot;)public class User &#123; @ApiModelProperty(&quot;用户名&quot;) public String username; @ApiModelProperty(&quot;密码&quot;) public String password;&#125; 2、只要这个实体在请求接口的返回值上（即使是泛型），都能映射到实体项中： 1234@RequestMapping(&quot;/getUser&quot;)public User getUser()&#123; return new User();&#125; 3、重启查看测试 注：并不是因为@ApiModel这个注解让实体显示在这里了，而是只要出现在接口方法的返回值上的实体都会显示在这里，而@ApiModel和@ApiModelProperty这两个注解只是为实体添加注释的。 @ApiModel为类添加注释 @ApiModelProperty为类属性添加注释 3.6、常用注解Swagger的所有注解定义在io.swagger.annotations包下 下面列一些经常用到的，未列举出来的可以另行查阅说明： Swagger注解 简单说明 @Api(tags &#x3D; “xxx模块说明”) 作用在模块类上 @ApiOperation(“xxx接口说明”) 作用在接口方法上 @ApiModel(“xxxPOJO说明”) 作用在模型类上：如VO、BO @ApiModelProperty(value &#x3D; “xxx属性说明”,hidden &#x3D; true) 作用在类方法和属性上，hidden设置为true可以隐藏该属性 @ApiParam(“xxx参数说明”) 作用在参数、方法和字段上，类似@ApiModelProperty 我们也可以给请求的接口配置一些注释 123456@ApiOperation(&quot;xxx的接口&quot;)@PostMapping(&quot;/jun&quot;)@ResponseBodypublic String test(@ApiParam(&quot;这个名字会被返回&quot;)String username)&#123; return username;&#125; 这样的话，可以给一些比较难理解的属性或者接口，增加一些配置信息，让人更容易阅读！ 相较于传统的Postman或Curl方式测试接口，使用swagger简直就是傻瓜式操作，不需要额外说明文档(写得好本身就是文档)而且更不容易出错，只需要录入数据然后点击Execute，如果再配合自动化框架，可以说基本就不需要人为操作了。 Swagger是个优秀的工具，现在国内已经有很多的中小型互联网公司都在使用它，相较于传统的要先出Word接口文档再测试的方式，显然这样也更符合现在的快速迭代开发行情。当然了，提醒下大家在正式环境要记得关闭Swagger，一来出于安全考虑二来也可以节省运行时内存。 3.7、拓展：其他皮肤我们可以导入不同的包实现不同的皮肤定义： 1、默认的 访问 http://localhost:8080/swagger-ui.html 12345&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 2、bootstrap-ui 访问 http://localhost:8080/doc.html 123456&lt;!-- 引入swagger-bootstrap-ui包 /doc.html--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;swagger-bootstrap-ui&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt; 3、Layui-ui 访问 http://localhost:8080/docs.html 123456&lt;!-- 引入swagger-ui-layer包 /docs.html--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.caspar-chen&lt;/groupId&gt; &lt;artifactId&gt;swagger-ui-layer&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt; 4、mg-ui 访问 http://localhost:8080/document.html 123456&lt;!-- 引入swagger-ui-layer包 /document.html--&gt;&lt;dependency&gt; &lt;groupId&gt;com.zyplayer&lt;/groupId&gt; &lt;artifactId&gt;swagger-mg-ui&lt;/artifactId&gt; &lt;version&gt;1.0.6&lt;/version&gt;&lt;/dependency&gt;","categories":["Spring Boot"]},{"title":"Spring Cloud Gateway详解","path":"/2023/10/05/Spring-Cloud-Gateway详解/","content":"微服务网关，为什么需要API网关？网关的工作流程，断言、动态路由、过滤器、token认证。 1、为什么需要API网关？在 SpringCloud 微服务架构中，往往有多个微服务，这些微服务可能部署在不同的机器上，而且一个微服务可能会扩容成多个相同的微服务，组成微服务集群。 这种情况下，会存在如下问题： 如果需要添加鉴权功能，则需要对每个微服务进行改造。 如果需要对流量进行控制，则需要对每个微服务进行改造。 跨域问题，需要对每个微服务进行改造。 存在安全问题，每个微服务需要暴露自己的 Endpoint 给客户端。 Endpoint 就是服务的访问地址 + 端口，比如下面的地址： http://order.passjava.cn:8000 灰度发布、动态路由需要对每个微服务进行改造。 这个问题的痛点是各个微服务都是一个入口，有没有办法统一入口呢？ 解决这个问题的方式就是在客户端和服务器之间加个中间商就好了呀，只有中间商一个入口，这个中间商就是网关。 2、工作流程Gateway 的工作流程如下图所示： ① 路由判断；客户端的请求到达网关后，先经过 Gateway Handler Mapping 处理，这里面会做断言（Predicate）判断，看下符合哪个路由规则，这个路由映射后端的某个服务。 ② 请求过滤：然后请求到达 Gateway Web Handler，这里面有很多过滤器，组成过滤器链（Filter Chain），这些过滤器可以对请求进行拦截和修改，比如添加请求头、参数校验等等，有点像净化污水。然后将请求转发到实际的后端服务。这些过滤器逻辑上可以称作 Pre-Filters，Pre 可以理解为“在…之前”。 ③ 服务处理：后端服务会对请求进行处理。 ④ 响应过滤： 后端处理完结果后，返回给 Gateway 的过滤器再次做处理，逻辑上可以称作 Post-Filters，Post 可以理解为“在…之后”。 ⑤ 响应返回：响应经过过滤处理后，返回给客户端。 小结：客户端的请求先通过匹配规则找到合适的路由，就能映射到具体的服务。然后请求经过过滤器处理后转发给具体的服务，服务处理后，再次经过过滤器处理，最后返回给客户端。 3、断言断言（Predicate）这个词听起来极其深奥，它是一种编程术语，我们生活中根本就不会用它。说白了它就是对一个表达式进行 if 判断，结果为真或假，如果为真则做这件事，否则做那件事。 在 Gateway 中，如果客户端发送的请求满足了断言的条件，则映射到指定的路由器，就能转发到指定的服务上进行处理。 断言配置的示例如下，配置了两个路由规则，有一个 predicates 断言配置，当请求 url 中包含 api&#x2F;thirdparty，就匹配到了第一个路由 route_thirdparty。 接下来我们看下 Route 路由和 Predicate 断言的对应关系： 一对多：一个路由规则可以包含多个断言。如上图中路由 Route1 配置了三个断言 Predicate。 同时满足：如果一个路由规则中有多个断言，则需要同时满足才能匹配。如上图中路由 Route2 配置了两个断言，客户端发送的请求必须同时满足这两个断言，才能匹配路由 Route2。 第一个匹配成功：如果一个请求可以匹配多个路由，则映射第一个匹配成功的路由。如上图所示，客户端发送的请求满足 Route3 和 Route4 的断言，但是 Route3 的配置在配置文件中靠前，所以只会匹配 Route3。 常见的 Predicate 断言配置如下所示，假设匹配路由成功后，转发到 http://localhost:9001。 代码演示： 下面演示 Gateway 中通过断言来匹配路由的例子。 新建一个 Maven 工程，引入 Gateway 依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; 新建 application.yml 文件，添加 Gateway 的路由规则。 1234567891011121314spring: cloud: gateway: routes: - id: route_qq uri: http://www.qq.com predicates: - Query=url,qq - id: route_baidu uri: http://www.baidu.com predicates: - Query=url,baiduserver: port: 8060 复制复制失败复制成功 第一条路由规则：断言为 Query&#x3D;url,qq，表示当请求路径中包含 url&#x3D;qq，则跳转到http://www.qq.com 第二条路由规则：当请求路径中包含 url&#x3D;baidu，则跳转到http://www.baidu.com 4、动态路由在微服务架构中，我们不会直接通过 IP + 端口的方式访问微服务，而是通过服务名的方式来访问。如下图所示，微服务中加入了注册中心，多个微服务将自己注册到了注册中心，这样注册中心就保存了服务名和 IP+端口的映射关系。 网关经过断言匹配到一个路由后，将请求转发给指定 uri，这个 uri 可以配置成 微服务的名字，比如 passjava-member。 那么这个服务名具体要转发到哪个 IP 地址和端口上呢？这个就依赖注册中心的注册表了，Gateway 从注册中心拉取注册表，就能知道服务名对应具体的 IP + 端口，如果一个服务部署了多台机器，则还可以通过负载均衡进行请求的转发。 对应的配置为 uri 字段如下所示 uri: lb:&#x2F;&#x2F;passjava-question，表示将请求转发给 passjava-question 微服务，且支持负载均衡。lb 是 loadbalance（负载均衡) 单词的缩写。 那什么叫动态路由呢？ 当 passjava-question 服务添加一个微服务，或者 IP 地址更换了，Gateway 都是可以感知到的，但是配置是不需要更新的。这里的动态指的是微服务的集群个数、IP 和端口是动态可变的。 5、过滤器过滤器 Filter 按照请求和响应可以分为两种：Pre 类型和 Post 类型。 Pre 类型：在请求被转发到微服务之前，对请求进行拦截和修改，例如参数校验、权限校验、流量监控、日志输出以及协议转换等操作。 Post 类型：微服务处理完请求后，返回响应给网关，网关可以再次进行处理，例如修改响应内容或响应头、日志输出、流量监控等。 另外一种分类是按照过滤器 Filter 作用的范围进行划分： GlobalFilter：全局过滤器，应用在所有路由上的过滤器。 GatewayFilter：局部过滤器，应用在单个路由或一组路由上的过滤器。 6、token认证在用 Gateway 做登录认证的时候，通常需要我们自定义一个全局过滤器做登录认证。 比如客户端登录时，将用户名和密码发送给网关，网关转发给认证服务器后，如果账号密码正确，则拿到一个 JWT token，然后客户端再访问应用服务时，先将请求发送给网关，网关统一做 JWT 认证，如果 JWT 符合条件，再将请求转发给应用服务。 原理如下图所示： 7、跨域处理在Spring Cloud Gateway中实现跨域处理的步骤如下： 首先，在你的Spring Cloud Gateway项目中添加CORS过滤器。你可以创建一个类来实现 GlobalFilter 接口，然后在过滤器中添加跨域处理的逻辑。 123456789101112131415161718192021222324252627282930313233@Componentpublic class CorsFilter implements GlobalFilter, Ordered &#123;private static final String ALLOWED_HEADERS = &quot;x-requested-with, authorization, Content-Type, Authorization, credential, X-XSRF-TOKEN&quot;;private static final String ALLOWED_METHODS = &quot;GET, PUT, POST, DELETE, OPTIONS&quot;;private static final String ALLOWED_ORIGIN = &quot;*&quot;;private static final String MAX_AGE = &quot;7200&quot;; // 2 hours@Overridepublic Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; ServerHttpRequest request = exchange.getRequest(); if (!request.getHeaders().containsKey(HttpHeaders.ORIGIN)) &#123; return chain.filter(exchange); &#125; ServerHttpResponse response = exchange.getResponse(); HttpHeaders headers = response.getHeaders(); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_ORIGIN, ALLOWED_ORIGIN); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_HEADERS, ALLOWED_HEADERS); headers.add(HttpHeaders.ACCESS_CONTROL_ALLOW_METHODS, ALLOWED_METHODS); headers.add(HttpHeaders.ACCESS_CONTROL_MAX_AGE, MAX_AGE); if (request.getMethod() == HttpMethod.OPTIONS) &#123; response.setStatusCode(HttpStatus.OK); return Mono.empty(); &#125; return chain.filter(exchange);&#125;@Overridepublic int getOrder() &#123; return Ordered.HIGHEST_PRECEDENCE;&#125; 然后，确保你的Spring Cloud Gateway应用程序已经启用了跨域请求。在你的配置文件（例如application.yml ）中添加以下配置： 12345678910111213141516spring: cloud: gateway: globalcors: corsConfigurations: &#x27;[/**]&#x27;: # 任意路径都允许配置的跨域 allowedOrigins: &quot;*&quot; # 允许所有请求来源进行跨域 allowedMethods: # 允许所有请求方式进行跨域 - GET - POST - PUT - DELETE allowedHeaders: # 允许所有请求头进行跨域 - &quot;*&quot; allowCredentials: true # 允许携带cookie进行跨域 maxAge: 3600 这样就允许了所有路径的跨域请求。 最后，重新启动你的Spring Cloud Gateway应用程序。现在，它应该能够处理跨域请求了。 8、网关聚合Swagger-UIspringcloud：gateway聚合swagger","categories":["Spring Cloud"]},{"title":"Redis实现排行榜","path":"/2023/10/05/Redis实现排行榜/","content":"基于Redis的基本数据结构和业务逻辑设计实现用户活跃排行榜。 1.1、场景说明技术派中，设计了一个社区用户的活跃排行榜，包括日榜和月榜。 用户活跃度计算方式： 用户每访问一个新的页面， +1分 对于一篇文章，点赞、收藏， +2分；取消点赞、取消收藏，将之前的活跃分收回。 文章评论， +3分 发布一篇审核通过的文章， +10分 榜单：展示活跃度最高的前三十名用户。 效果如下： 1.2、方案设计使用Redis的ZSet数据结构实现，以下是ZSet的简介： Redis的ZSet（有序集合）是一种有序的、唯一的数据结构。它类似于Set，但每个元素都关联着一个分数（score），用于进行排序。 ZSet的特点包括： 有序性：ZSet中的元素按照分数进行排序，可以根据分数进行范围查询、区间获取等操作。 唯一性：ZSet中的元素是唯一的，不会存在重复的元素。 快速的插入和删除：ZSet使用了跳跃表（Skip List）和哈希表（Hash Table）的结合体，使得插入和删除操作的时间复杂度为O(log N)。 高效的查找：通过索引和跳跃表的特性，可以在O(log N)的时间复杂度内查找某个元素。 ZSet常用的操作包括： ZADD：向ZSet中添加一个元素，同时指定其分数。 ZREM：从ZSet中移除一个元素。 ZRANGE：按照分数的顺序，获取指定范围内的元素。 ZSCORE：获取指定元素的分数。 ZINCRBY：将指定元素的分数增加一个特定的值。 ZSet广泛应用于排行榜、计分系统、排行榜、时间轴等场景，提供了高效的排序和检索功能。 1.3、排行榜实现1.3.1、业务实体设计​ 我们先实现一个更新用户活跃的方法，首先定义一个涵盖该业务场景的参数传递实体 ActivityScoreBo，记录用户活动（是否访问页面、点赞、收藏、评论、关注、发布文章）。 12345678910111213141516171819202122232425262728293031323334353637383940414243@Data@Accessors(chain = true)public class ActivityScoreBo &#123; /** * 访问页面增加活跃度 */ private String path; /** * 目标文章 */ private Long articleId; /** * 评论增加活跃度 */ private Boolean rate; /** * 点赞增加活跃度 */ private Boolean praise; /** * 收藏增加活跃度 */ private Boolean collect; /** * 发布文章增加活跃度 */ private Boolean publishArticle; /** * 被关注的用户 */ private Long followedUserId; /** * 关注增加活跃度 */ private Boolean follow;&#125; 有了业务实体，进一步我们需要计算活跃度。活跃度包括日榜和月榜，如下为对应的key生成。 1234567891011121314151617/** * 当天活跃度排行榜 * * @return 当天排行榜key */ private String todayRankKey() &#123; return ACTIVITY_SCORE_KEY + DateUtil.format(DateTimeFormatter.ofPattern(&quot;yyyyMMdd&quot;), System.currentTimeMillis()); &#125; /** * 本月排行榜 * * @return 月度排行榜key */ private String monthRankKey() &#123; return ACTIVITY_SCORE_KEY + DateUtil.format(DateTimeFormatter.ofPattern(&quot;yyyyMM&quot;), System.currentTimeMillis()); &#125; 1.3.2、计算活跃度（1）根据传入的业务实体，判断文章浏览、点赞、收藏、关注、排列、发文等行为，并计算对应的字段field和分数score，不同行为的加分机制见上述场景说明。 （2）接着，根据用户ID和当天日期（日为单位）生成一个唯一的活跃度信息的键值（userActionKey）。 （3）通过RedisClient从Redis中获取该userActionKey键对应的field值（ans）。 （4）如果ans为null，说明之前没有加分记录，执行加分操作： 如果加分数大于0，将加分记录保存到Redis的hash结构中，并设置有效期为一个月。&#x2F;注：【加分记录使用Redis的Hash结构存储，key为userActionKey，字段为field，value为分数score】 更新当天和当月的活跃度排行榜，使用Redis的zIncrBy函数。 如果新的活跃度得分大于等于加分数，更新日活跃榜单和月活跃榜单的有效期。 （5）如果ans大于0，说明之前该field已经加过分，继续判断： 如果分数小于0，说明是减分行为，应从Redis中删除加分记录。 更新当天和当月的活跃度排行榜。 幂等策略： 在上述加分操作中，为了防止重复加活跃度，我们做了一个幂等操作。 就是将用户的每个加分项，都记录下来，在执行具体加分时，基于此来做幂等判定 。 因此，我们对每个用户维护一个活跃更新的操作历史记录表，保存在redis的hash数据结构中，每天一个记录。 123key: activity_rank_&#123;user_id&#125;_&#123;年月日&#125; field: 活跃度更新keyvalue: 添加的活跃度 思考： 1、事务问题：虽然单个redis操作是原子性的，但多次的redis操作，存在事务问题。 2、并发问题：没有做并发，幂等无法100%生效，依然可能存在重复添加&#x2F;扣减活跃度的情况 问题一： 通过最终一致性（Eventual Consistency）来解决多次Redis操作的事务问题是一种常见的方法。最终一致性是指在分布式系统中，经过一段时间后，系统的所有副本最终会达到一致的状态。 在Redis中，可以使用以下方法来实现最终一致性： 批量操作：将多个操作组合成一个批量操作，减少网络往返的次数。例如，使用管道（Pipeline）来发送多个命令，然后一次性获取它们的响应。 异步操作：将操作异步化，即将操作放入消息队列或任务队列中，由后台线程或其他服务异步处理。这样可以避免阻塞主线程，并允许操作在不同的时间点执行。 回滚机制：在执行操作之前，先将相关数据备份或记录下来。如果操作失败，可以使用备份数据进行回滚操作。 重试机制：如果某个操作失败，可以进行重试，直到操作成功或达到最大重试次数。 业务层面的补偿机制：如果操作失败，可以通过业务逻辑来进行补偿操作，以达到一致性。 需要注意的是，最终一致性并不能提供强一致性的保证，因此在某些场景下可能会出现数据不一致的情况。在选择使用最终一致性来解决事务问题时，需要根据具体的业务需求和数据一致性要求来评估和权衡。 问题二： 通过加锁解决并发问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void addActivityScore(Long userId, ActivityScoreBo activityScore) &#123; if (userId == null) &#123; return; &#125; // 1. 计算活跃度(正为加活跃,负为减活跃) String field; int score = 0; if (activityScore.getPath() != null) &#123; field = &quot;path_&quot; + activityScore.getPath(); score = 1; &#125; else if (activityScore.getArticleId() != null) &#123; field = activityScore.getArticleId() + &quot;_&quot;; if (activityScore.getPraise() != null) &#123; field += &quot;praise&quot;; score = BooleanUtils.isTrue(activityScore.getPraise()) ? 2 : -2; &#125; else if (activityScore.getCollect() != null) &#123; field += &quot;collect&quot;; score = BooleanUtils.isTrue(activityScore.getCollect()) ? 2 : -2; &#125; else if (activityScore.getRate() != null) &#123; // 评论回复 field += &quot;rate&quot;; score = BooleanUtils.isTrue(activityScore.getRate()) ? 3 : -3; &#125; else if (BooleanUtils.isTrue(activityScore.getPublishArticle())) &#123; // 发布文章 field += &quot;publish&quot;; score += 10; &#125; &#125; else if (activityScore.getFollowedUserId() != null) &#123; field = activityScore.getFollowedUserId() + &quot;_follow&quot;; score = BooleanUtils.isTrue(activityScore.getFollow()) ? 2 : -2; &#125; else &#123; return; &#125; final String todayRankKey = todayRankKey(); final String monthRankKey = monthRankKey(); // 2. 幂等：判断之前是否有更新过相关的活跃度信息 final String userActionKey = ACTIVITY_SCORE_KEY + userId + DateUtil.format(DateTimeFormatter.ofPattern(&quot;yyyyMMdd&quot;), System.currentTimeMillis()); Integer ans = RedisClient.hGet(userActionKey, field, Integer.class); if (ans == null) &#123; // 2.1 之前没有加分记录，执行具体的加分 if (score &gt; 0) &#123; // 记录加分记录 RedisClient.hSet(userActionKey, field, score); // 个人用户的操作记录，保存一个月的有效期，方便用户查询自己最近31天的活跃情况 RedisClient.expire(userActionKey, 31 * DateUtil.ONE_DAY_SECONDS); // 更新当天和当月的活跃度排行榜 Double newAns = RedisClient.zIncrBy(todayRankKey, String.valueOf(userId), score); RedisClient.zIncrBy(monthRankKey, String.valueOf(userId), score); if (log.isDebugEnabled()) &#123; log.info(&quot;活跃度更新加分! key#field = &#123;&#125;#&#123;&#125;, add = &#123;&#125;, newScore = &#123;&#125;&quot;, todayRankKey, userId, score, newAns); &#125; if (newAns &lt;= score) &#123; // 日活跃榜单，保存31天；月活跃榜单，保存1年 RedisClient.expire(todayRankKey, 31 * DateUtil.ONE_DAY_SECONDS); RedisClient.expire(monthRankKey, 12 * DateUtil.ONE_MONTH_SECONDS); &#125; &#125; &#125; else if (ans &gt; 0) &#123; // 2.2 之前已经加过分，因此这次减分可以执行 if (score &lt; 0) &#123; Boolean oldHave = RedisClient.hDel(userActionKey, field); if (BooleanUtils.isTrue(oldHave)) &#123; Double newAns = RedisClient.zIncrBy(todayRankKey, String.valueOf(userId), score); RedisClient.zIncrBy(monthRankKey, String.valueOf(userId), score); if (log.isDebugEnabled()) &#123; log.info(&quot;活跃度更新减分! key#field = &#123;&#125;#&#123;&#125;, add = &#123;&#125;, newScore = &#123;&#125;&quot;, todayRankKey, userId, score, newAns); &#125; &#125; &#125; &#125; &#125; 1.3.3、触发活跃度更新前面只是提供了一个增加活跃度的方法，但是什么时候调用它呢？ 我们这里借助之前实现 Event&#x2F;Listenter方式来处理活跃度更新。 文章&#x2F;用户的相关操作事件监听，并更新对应的活跃度： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class UserActivityListener &#123; @Autowired private UserActivityRankService userActivityRankService; /** * 用户操作行为，增加对应的积分 * * @param msgEvent */ @EventListener(classes = NotifyMsgEvent.class) @Async public void notifyMsgListener(NotifyMsgEvent msgEvent) &#123; switch (msgEvent.getNotifyType()) &#123; case COMMENT: case REPLY: CommentDO comment = (CommentDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo() .setRate(true).setArticleId(comment.getArticleId())); break; case COLLECT: UserFootDO foot = (UserFootDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setCollect(true).setArticleId(foot.getDocumentId())); break; case CANCEL_COLLECT: foot = (UserFootDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setCollect(false).setArticleId(foot.getDocumentId())); break; case PRAISE: foot = (UserFootDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setPraise(true).setArticleId(foot.getDocumentId())); break; case CANCEL_PRAISE: foot = (UserFootDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setPraise(false).setArticleId(foot.getDocumentId())); break; case FOLLOW: UserRelationDO relation = (UserRelationDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setFollow(true).setArticleId(relation.getUserId())); break; case CANCEL_FOLLOW: relation = (UserRelationDO) msgEvent.getContent(); userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setFollow(false).setArticleId(relation.getUserId())); break; default: &#125; &#125; /** * 发布文章，更新对应的积分 * * @param event */ @Async @EventListener(ArticleMsgEvent.class) public void publishArticleListener(ArticleMsgEvent&lt;ArticleDO&gt; event) &#123; ArticleEventEnum type = event.getType(); if (type == ArticleEventEnum.ONLINE) &#123; userActivityRankService.addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setPublishArticle(true).setArticleId(event.getContent().getId())); &#125; &#125;&#125; 接下来，就是对应方法触发后事件更新了，包括ArticleMsgEvent、NotifyMsgEvent等。 另外，针对用户浏览页面的活跃度触发，我们在 Filte&#x2F;Inteceptor 层实现，通过GlobalViewInterceptor的preHandle方法实现。 123456789101112131415161718public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; if (handler instanceof HandlerMethod) &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Permission permission = handlerMethod.getMethod().getAnnotation(Permission.class); if (permission == null) &#123; permission = handlerMethod.getBeanType().getAnnotation(Permission.class); &#125; if (permission == null || permission.role() == UserRole.ALL) &#123; if (ReqInfoContext.getReqInfo() != null) &#123; // 用户活跃度更新 SpringUtil.getBean(UserActivityRankService.class).addActivityScore(ReqInfoContext.getReqInfo().getUserId(), new ActivityScoreBo().setPath(ReqInfoContext.getReqInfo().getPath())); &#125; return true; &#125; &#125; return true; &#125; 1.3.4、排行榜查询接下来就是将这个榜单展示给用户看。 基本流程如下： 1、从redis中获取topN的用户+评分 2、查询用户的信息 3、根据用户评分进行排序，并更新每个用户的排名 1234567891011121314151617181920212223public List&lt;RankItemDTO&gt; queryRankList(ActivityRankTimeEnum time, int size) &#123; String rankKey = time == ActivityRankTimeEnum.DAY ? todayRankKey() : monthRankKey(); // 1. 获取topN的活跃用户 List&lt;ImmutablePair&lt;String, Double&gt;&gt; rankList = RedisClient.zTopNScore(rankKey, size); if (CollectionUtils.isEmpty(rankList)) &#123; return Collections.emptyList(); &#125; // 2. 查询用户对应的基本信息 // 构建userId -&gt; 活跃评分的map映射，用于补齐用户信息 Map&lt;Long, Integer&gt; userScoreMap = rankList.stream().collect(Collectors.toMap(s -&gt; Long.valueOf(s.getLeft()), s -&gt; s.getRight().intValue())); List&lt;SimpleUserInfoDTO&gt; users = userService.batchQuerySimpleUserInfo(userScoreMap.keySet()); // 3. 根据评分进行排序 List&lt;RankItemDTO&gt; rank = users.stream() .map(user -&gt; new RankItemDTO().setUser(user).setScore(userScoreMap.getOrDefault(user.getUserId(), 0))) .sorted((o1, o2) -&gt; Integer.compare(o2.getScore(), o1.getScore())) .collect(Collectors.toList()); // 4. 补齐每个用户的排名 IntStream.range(0, rank.size()).forEach(i -&gt; rank.get(i).setRank(i + 1)); return rank; &#125; 其中核心方法是Redis的zRangeWithScores，用以获取指定排名的用户和对应分数。 zrevrange 是Redis中的一个有序集合操作命令，用于按照分数从大到小的顺序获取有序集合中指定范围内的成员。 命令语法如下：ZREVRANGE key start stop [WITHSCORES]参数说明： key ：有序集合的键名。 start ：指定范围的起始位置，从0开始计数，表示成员的排名。 stop ：指定范围的结束位置，从0开始计数，表示成员的排名。 WITHSCORES （可选）：如果提供了该参数，命令会返回成员和对应的分数，以一个成员和一个分数交替排列的方式返回结果。 示例：假设有一个有序集合名为 myset ，包含以下成员和对应的分数：“member1” -&gt; 10“member2” -&gt; 20“member3” -&gt; 30“member4” -&gt; 40“member5” -&gt; 50使用 ZREVRANGE myset 0 2 命令，将返回范围为0到2的成员： “member5” “member4” “member3”使用 ZREVRANGE myset 0 2 WITHSCORES 命令，将返回范围为0到2的成员和对应的分数： “member5” “50” “member4” “40” “member3” “30” 1234567891011121314public static List&lt;ImmutablePair&lt;String, Double&gt;&gt; zTopNScore(String key, int n) &#123; return template.execute(new RedisCallback&lt;List&lt;ImmutablePair&lt;String, Double&gt;&gt;&gt;() &#123; @Override public List&lt;ImmutablePair&lt;String, Double&gt;&gt; doInRedis(RedisConnection connection) throws DataAccessException &#123; Set&lt;RedisZSetCommands.Tuple&gt; set = connection.zRangeWithScores(keyBytes(key), -n, -1); if (set == null) &#123; return Collections.emptyList(); &#125; return set.stream() .map(tuple -&gt; ImmutablePair.of(toObj(tuple.getValue(), String.class), tuple.getScore())) .sorted((o1, o2) -&gt; Double.compare(o2.getRight(), o1.getRight())).collect(Collectors.toList()); &#125; &#125;); &#125;","categories":["项目实战"]},{"title":"Redis核心技术与原理","path":"/2023/10/05/Redis核心技术与原理/","content":"Redis核心知识介绍。包括持久化、事务、过期策略、内存淘汰机制、主从复制、哨兵机制、集群部署、缓存和数据库一致性问题。 1、Redis是如何执行的？一条命令的执行过程有很多细节，但大体可分为： 客户端先将用户输入的命令，转化为 Redis 相关的通讯协议，再用 socket 连接的方式将内容发送给服务器端。 服务器端在接收到相关内容之后，先将内容转化为具体的执行命令，再判断用户授权信息和其他相关信息，当验证通过之后会执行命令。 命令执行完之后，会进行相关的信息记录和数据统计，然后再把执行结果发送给客户端，这样一条命令的执行流程就结束了。 如果是集群模式的话，主节点还会将命令同步至子节点。 2、Redis持久化Redis 的读写都是在内存中，所以它的性能较高，但在内存中的数据会随着服务器的重启而丢失，为了保证数据不丢失，我们需要将内存中的数据存储到磁盘，以便 Redis 重启时能够从磁盘中恢复原有的数据，而整个过程就叫做 Redis 持久化。 Redis 持久化拥有以下三种方式： 快照方式（RDB, Redis DataBase）将某一个时刻的内存数据，以二进制的方式写入磁盘； 文件追加方式（AOF, Append Only File），记录所有的操作命令，并以文本的形式追加到文件中； 混合持久化方式，Redis 4.0 之后新增的方式，混合持久化是结合了 RDB 和 AOF 的优点，在写入的时候，先把当前的数据以 RDB 的形式写入文件的开头，再将后续的操作命令以 AOF 的格式存入文件，这样既能保证 Redis 重启时的速度，又能减低数据丢失的风险。 2.1、RDBRDB（Redis DataBase）是将某一个时刻的内存快照（Snapshot），以二进制的方式写入磁盘的过程。 持久化触发： 1）手动触发 手动触发持久化的操作有两个： save 和 bgsave ，它们主要区别体现在：是否阻塞 Redis 主线程的执行。 2）自动触发 save m n save m n 是指在 m 秒内，如果有 n 个键发生改变，则自动触发持久化。 flushall flushall 命令用于清空 Redis 数据库，在生产环境下一定慎用，当 Redis 执行了 flushall 命令之后，则会触发自动持久化，把 RDB 文件清空。 主从同步触发 在 Redis 主从复制中，当从节点执行全量复制操作时，主节点会执行 bgsave 命令，并将 RDB 文件发送给从节点，该过程会自动触发 Redis 持久化。 RDB文件恢复： 当 Redis 服务器启动时，如果 Redis 根目录存在 RDB 文件 dump.rdb，Redis 就会自动加载 RDB 文件恢复持久化数据。 优点： RDB 的内容为二进制的数据，占用内存更小，更紧凑，更适合做为备份文件； RDB 对灾难恢复非常有用，它是一个紧凑的文件，可以更快的传输到远程服务器进行 Redis 服务恢复； RDB 可以更大程度的提高 Redis 的运行速度，因为每次持久化时 Redis 主进程都会 fork() 一个子进程，进行数据持久化到磁盘，Redis 主进程并不会执行磁盘 I&#x2F;O 等操作； 与 AOF 格式的文件相比，RDB 文件可以更快的重启。 缺点： 因为 RDB 只能保存某个时间间隔的数据，如果中途 Redis 服务被意外终止了，则会丢失一段时间内的 Redis 数据； RDB 需要经常 fork() 才能使用子进程将其持久化在磁盘上。如果数据集很大，fork() 可能很耗时，并且如果数据集很大且 CPU 性能不佳，则可能导致 Redis 停止为客户端服务几毫秒甚至一秒钟。 2.2、AOFAOF（Append Only File）中文是附加到文件，顾名思义 AOF 可以把 Redis 每个键值对操作都记录到文件（appendonly.aof）中。 注：使用 RDB 持久化有一个风险，它可能会造成最新数据丢失的风险。因为 RDB 的持久化有一定的时间间隔，在这个时间段内如果 Redis 服务意外终止（断电，系统崩溃、Redis进程死亡）的话，就会造成最新的数据全部丢失。 持久化触发： 1）自动触发 always：每条 Redis 操作命令都会写入磁盘，最多丢失一条数据； everysec：每秒钟写入一次磁盘，最多丢失一秒的数据； no：不设置写入磁盘的规则，根据当前操作系统来决定何时写入磁盘，Linux 默认 30s 写入一次数据至磁盘。 2）手动触发 在客户端执行 bgrewriteaof 命令就可以手动触发 AOF 持久化。 AOF重写： AOF 是通过记录 Redis 的执行命令来持久化（保存）数据的，所以随着时间的流逝 AOF 文件会越来越多，这样不仅增加了服务器的存储压力，也会造成 Redis 重启速度变慢，为了解决这个问题 Redis 提供了 AOF 重写的功能。 AOF 重写流程 AOF 文件重写是生成一个全新的文件，并把当前数据的最少操作命令保存到新文件上，当把所有的数据都保存至新文件之后，Redis 会交换两个文件，并把最新的持久化操作命令追加到新文件上。 数据恢复： 正常情况下，只要开启了 AOF 持久化，并且提供了正常的 appendonly.aof 文件，在 Redis 启动时就会自定加载 AOF 文件并启动。 持久化文件加载规则 如果只开启了 AOF 持久化，Redis 启动时只会加载 AOF 文件（appendonly.aof），进行数据恢复； 如果只开启了 RDB 持久化，Redis 启动时只会加载 RDB 文件（dump.rdb），进行数据恢复； 如果同时开启了 RDB 和 AOF 持久化，Redis 启动时只会加载 AOF 文件（appendonly.aof），进行数据恢复。 在 AOF 开启的情况下，即使 AOF 文件不存在，则创建一个空的 appendonly.aof 文件，并基于这个空的 appendonly.aof 文件启动。 2.3、混合持久化在开启混合持久化的情况下，AOF 重写时会把 Redis 的持久化数据，以 RDB 的格式写入到 AOF 文件的开头，之后的数据再以 AOF 的格式化追加的文件的末尾。 混合持久化的加载规则： 3、Redis事务Redis 中的事务从开始到结束也是要经历三个阶段： 开启事务 命令入列 执行事务&#x2F;放弃事务 其中，开启事务使用 multi 命令，事务执行使用 exec 命令，放弃事务使用 discard 命令。 Redis不支持事务回滚的原因有以下两个： 认为 Redis 事务的执行时，错误通常都是编程错误造成的，这种错误通常只会出现在开发环境中，而很少会在实际的生产环境中出现，所以他认为没有必要为 Redis 开发事务回滚功能； 不支持事务回滚是因为这种复杂的功能和 Redis 追求的简单高效的设计主旨不符合。 这里不支持事务回滚，指的是不支持运行时错误的事务回滚。 监控 watch 命令用于客户端并发情况下，为事务提供一个乐观锁（CAS，Check And Set），也就是可以用 watch 命令来监控一个或多个变量，如果在事务的过程中，某个监控项被修改了，那么整个事务就会终止执行。 4、Redis过期策略常见的过期策略有以下三种： 定时删除 惰性删除 定期删除 定时删除 在设置键值过期时间时，创建一个定时事件，当过期时间到达时，由事件处理器自动执行键的删除操作。 优点：保证内存可以被尽快地释放。 缺点：在 Redis 高负载的情况下或有大量过期键需要同时处理时，会造成 Redis 服务器卡顿，影响主业务执行。 惰性删除 不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null。 优点：因为每次访问时，才会判断过期键，所以此策略只会使用很少的系统资源。 缺点：系统占用空间删除不及时，导致空间利用率降低，造成了一定的空间浪费。 定期删除 每隔一段时间检查一次数据库，随机删除一些过期键。 Redis 默认每秒进行 10 次过期扫描，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。 注： 1、Redis 每次扫描并不是遍历过期字典中的所有键，而是随机抽取判断并删除过期键的形式执行的。 2、过期键在主从模式下，从库对过期键的处理要完全依靠主库，主库删除过期键之后会发送 del 命令给所有的从库。 5、Redis管道技术管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。 通常情况下 Redis 是单行执行的，客户端先向服务器发送请求，服务端接收并处理请求后再把结果返回给客户端，这种处理模式在非频繁请求时不会有任何问题。 但如果出现集中大批量的请求时，因为每个请求都要经历先请求再响应的过程，这就会造成网络资源浪费，此时就需要管道技术来把所有的命令整合一次发给服务端，再一次响应给客户端，这样就能大大的提升了 Redis 的响应速度。 6、内存淘汰机制早期版本的 Redis 有以下 6 种淘汰策略： noeviction：不淘汰任何数据，当内存不足时，新增操作会报错，Redis 默认内存淘汰策略； allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-random：随机淘汰任意键值; volatile-lru：淘汰所有设置了过期时间的键值中最久未使用的键值； volatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 在 Redis 4.0 版本中又新增了 2 种淘汰策略： volatile-lfu：淘汰所有设置了过期时间的键值中，最少使用的键值； allkeys-lfu：淘汰整个键值中最少使用的键值。 其中 allkeys-xxx 表示从所有的键值中淘汰数据，而 volatile-xxx 表示从设置了过期键的键值中淘汰数据。 7、主从库怎么实现数据一致？Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库。 主从库如何进行第一次同步？ 1、当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。 runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。 offset，此时设为 -1，表示第一次复制。 主库收到 psync 命令后，会用 FULLRESYNC 响应命令（全量复制）带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。 在第二阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。 最后，也就是第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 主从级联模式：“主 - 从 - 从”模式中，为了减轻主库压力，手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。建立起主从关系： 1replicaof 所选从库的IP 6379 这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，这就可以减轻主库上的压力。 主从库之间网络断了怎么办？ 网络断了之后，主从库会采用增量复制的方式继续同步。即只会把主从库网络断连期间主库收到的命令，同步给从库。 8、哨兵机制 主观下线： 哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”。 客观下线： 为了避免单个哨兵因为自身网络状况不好，而误判主库下线的情况，引入多个哨兵，被称为哨兵集群。 当大多数的哨兵实例，都判断主库已经“主观下线”了，主库就会被标记为“客观下线” 如何选定新主库？ 三个规则：从库优先级、从库复制进度以及从库 ID 号。只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。如果没有出现得分最高的从库，那么就继续进行下一轮。 第一轮：优先级最高的从库得分高。 第二轮：和旧主库同步程度最接近的从库得分高。 第三轮：ID 号小的从库得分高。 9、Redis切片集群如何保存更多数据？ 为了保存大量数据，有大内存云主机和切片集群两种方法。实际上，这两种方法分别对应着 Redis 应对数据量增多的两种方案：纵向扩展（scale up）和横向扩展（scale out）。 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。 横向扩展：横向增加当前 Redis 实例的个数， 数据和实例之间如何对应呢？ 在切片集群中，数据需要分布在不同实例上，那么，数据和实例之间如何对应呢？ 从 3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。 具体来说，Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。 那么，这些哈希槽又是如何被映射到具体的 Redis 实例上的呢？ 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384&#x2F;N 个。 当然， 我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用 cluster addslots 命令，指定每个实例上的哈希槽个数。 客户端如何定位数据？ 在定位键值对数据时，它所处的哈希槽是可以通过计算得到的，这个计算可以在客户端发送请求时来执行。但是，要进一步定位到实例，还需要知道哈希槽分布在哪个实例上。 一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。 那么，客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？这是因为，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。 客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。 但是，在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个： 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽； 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。 情况一：当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽。 那么，这个实例就会给客户端返回 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。 12GET hello:key(error) MOVED 13320 172.16.19.5:6379 其中，MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。 情况二：客户端向实例 2 发送请求，但此时，Slot 2 中的数据只有一部分迁移到了实例 3，还有部分数据没有迁移。 在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息，如下所示： 12GET hello:key(error) ASK 13320 172.16.19.5:6379 这个结果中的 ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。 比较：ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 10、如何解决缓存和数据库的数据一致性问题？缓存和数据库不一致的问题，我们可以分成读写缓存和只读缓存两种情况进行分析。 为了解决Redis缓存数据一致性问题，可以采用以下方案： 缓存失效：在更新数据库数据时，同时使缓存失效。这种方式可以保证数据的最终一致性，但可能会导致一些短暂的数据不一致情况。 缓存更新：在更新数据库数据时，同时更新缓存。这种方式可以保证数据的强一致性，但可能会增加系统的复杂性和负载。 异步更新：在更新数据库数据时，采用异步方式更新缓存。这种方式可以保证系统的可用性和性能，但可能会导致一些短暂的数据不一致情况。 最佳实践： 合理选择缓存失效的时间：根据业务需求和系统特点，选择适当的缓存失效时间，以保证数据的及时性和一致性。 使用分布式锁：在进行数据库和缓存的更新操作时，使用分布式锁来避免并发问题导致的数据不一致情况。 定期同步缓存和数据库：定期进行缓存和数据库的同步操作，以保证数据的一致性。 采用合理的缓存策略：根据业务需求和系统特点，采用合理的缓存策略，如缓存失效、缓存更新、异步更新等，以保证数据的一致性和性能。 使用分布式事务：在进行数据库和缓存的更新操作时，使用分布式事务来保证数据的一致性和可靠性。 监控和告警：对Redis缓存系统进行监控和告警，及时发现和解决数据一致性问题。 ————————————————","categories":["Redis"]},{"title":"NodeJs&Vue安装配置教程","path":"/2023/10/05/NodeJs&Vue安装配置教程/","content":"NodeJs安装、Vue安装、前端工程部署环境配置教程。 一、安装Node.js1、下载 Node.js官网下载 2、安装 一路Next，默认安装即可。 finish！ 安装完成后，检查一下是否安装成功。 打开cmd，输入如下指令，输出对应版本即安装成功。 123node -vnpm -v1 二、创建全局安装和缓存目录在我们的安装目录下，创建名为node_cache和node_global的两个文件夹。 打开cmd窗口，执行如下命令，将npm的全局模块目录和缓存目录配置到我们刚才创建的那两个目录。 12npm config set prefix &quot;D:\\DevelopingEnvironment odejs ode_global&quot;npm config set cache &quot;D:\\DevelopingEnvironment odejs ode_cache&quot; 为了以后下载包快速，修改源为淘宝镜像。 1npm config set registry https://registry.npm.taobao.org 查看npm配置修改是否成功 1npm config list 三、配置环境变量在安装过程中，自动配置了两个环境变量，一个是环境变量—用户变量—Path里面的C:\\Users\\你的用户名\\AppData\\Roaming pm，另一个是环境变量—系统变量—Path里面的软件安装目录，我们需要增加和修改一下。 1、环境变量—用户变量—选中Path—点编辑，将C:\\Users\\你的用户名\\AppData\\Roaming pm修改为 你的安装目录 ode_global 2、系统变量—Path添加node_global路径，和nodejs环境变量（默认安装时已加上） 三、安装vue1、安装vue.js 123npm install vue -gnpm install vue-cli -gnpm install vue-router -g 其中-g是全局安装，指安装到global全局目录去，如果不加-g，模块就会安装到当前路径下的node_modules文件夹下，没有目录则自动创建。 输入vue -V查看安装结果。 注：管理员权限打开cmd窗口执行 2、安装webpack模板 1npm install webpack -g 此外，在webpack 4x以上，webpack将命令相关的内容都放到了webpack-cli，所以还需要安装webpack-cli 1npm install webpack-cli -g 输入 webpack -v 查看安装结果。 参考： https://blog.csdn.net/zhu_liu_kun/article/details/130616720","categories":["环境配置教程"]},{"title":"Nacos使用笔记","path":"/2023/10/05/Nacos学习笔记/","content":"微服务部署下使用Nacos集成服务注册和发现，配置管理。 1、Spring Cloud整合Nacos注册中心1.1、引入Nacos服务发现组件12345&lt;!-- nacos discovery 服务发现组件--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 1.2、下载Nacos Server并启动 下载Nacos Server 压缩包 https://github.com/alibaba/nacos/releases 启动 Server 进入解压后文件夹或编译打包好的文件夹，找到如下相对文件夹 nacos&#x2F;bin，并对照操作系统实际情况之下如下命令，-m standalone表示单机模式启动。 1234# Linux/Unix/Mac 操作系统，执行命令 sh startup.sh -m standalone# Windows 操作系统，执行命令 startup.cmd -m standalone 1.3、配置Nacos server地址和微服务名称 为每个微服务都配置Nacos Server 地址 在每个微服务应用的 &#x2F;src&#x2F;main&#x2F;resources&#x2F;application.yml配置文件中配置 Nacos Server 地址。 12345678spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 application: name: passjava-question 1.4、添加注解为每个服务使用 @EnableDiscoveryClient 注解开启服务注册与发现功能。 12345678910@EnableDiscoveryClient@MapperScan(&quot;com.jun.passjava.question.dao&quot;)@SpringBootApplicationpublic class PassjavaQuestionApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(PassjavaQuestionApplication.class, args); &#125;&#125; 1.5、访问Nacos Server后台登录后台 http://localhost:8848/nacos/index.html#/login 用户名：nacos 密码：nacos 2、Spring Cloud整合Nacos配置中心2.1、传统配置方式 application.properties文件中定义两个配置： 12member.nickname = &quot;jun&quot;member.age = &quot;18&quot; 示例控制器中定义私有变量nickname和age，@value代表从配置中取值 12345@Value(&quot;$&#123;member.nickname&#125;&quot;)private String nickname;@Value(&quot;$member.age&quot;)private Integer age; 示例控制器中定义方法：获取nick和age的值 1234@RequestMapping(&quot;/test-local-config&quot;)public R testLocalConfig() &#123; return R.ok().put(&quot;nickname&quot;, nickname).put(&quot;age&quot;, age);&#125; 测试结果 总结 从配置文件中获取配置。这种方式的缺点是什么呢？如果要修改配置参数，则需要重新启动服务。如果服务很多，则需要重启所有服务，非常不方便。 有没有什么办法不停服务修改配置而且使其生效呢？ 答案：有的，用Spring Cloud Alibaba的Nacos 组件就可以完成。 2.2、引入Nacos config依赖1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 2.3、配置Nacos元数据 微服务下添加 &#x2F;src&#x2F;main&#x2F;resources&#x2F;bootstrap.properties 配置文件。 注意：bootstrap.properties 是系统级的，优先级高于其他配置文件， application.properties 是应用级别的，加载比较晚，其次是bootstrap.yml、application.yml，所以引入 Nacos 组件时，需要配置 bootstrap.properties 配置 Nacos Config 元数据。 需要配置Nacos的服务名，也就是对应Nacos管理平台的配置文件，和Nacos服务地址。 12spring.application.name=nacos-demospring.cloud.nacos.config.server-addr=127.0.0.1:8848 2.4、Nacos后台新增配置Data ID: nacos-demo.properties Group: DEFAULT_GROUP 配置格式： 12user.name=&quot;jun&quot;user.age=27 2.5、开启动态刷新配置添加注解@RefreshScope开启动态刷新配置功能。 123456@RequestMapping(&quot;/test-local-config&quot;)@RestController@RefreshScopepublic R testLocalConfig() &#123; return R.ok().put(&quot;nickname&quot;, name).put(&quot;age&quot;,age);&#125; 日志： 1232023-09-27 10:59:06.646 INFO 13252 --- [-127.0.0.1_8848] o.s.c.e.event.RefreshEventListener : Refresh keys changed: [user.name]2023-09-27 10:59:06.646 INFO 13252 --- [-127.0.0.1_8848] c.a.nacos.client.config.impl.CacheData : [fixed-127.0.0.1_8848] [notify-ok] dataId=nacos-demo.properties, group=DEFAULT_GROUP, md5=bd3c381399ca063d5ee908060b8c1afa, listener=com.alibaba.cloud.nacos.refresh.NacosContextRefresher$1@186a3635 2023-09-27 10:59:06.647 INFO 13252 --- [-127.0.0.1_8848] c.a.nacos.client.config.impl.CacheData : [fixed-127.0.0.1_8848] [notify-listener] time cost=2133ms in ClientWorker, dataId=nacos-demo.properties, group=DEFAULT_GROUP, md5=bd3c381399ca063d5ee908060b8c1afa, listener=com.alibaba.cloud.nacos.refresh.NacosContextRefresher$1@186a3635 user.name更新了，通知了nacos-demo服务，刷新了配置。对应的配置id为nacos-demo.properties，分组为DEFAULT_GROUP。监听器为com.alibaba.cloud.nacos.refresh.NacosContextRefresher 测试结果： 2.6、更多命名空间、分组、多配置集用法。","categories":["Spring Cloud"]},{"title":"MyBatis-Plus整合笔记","path":"/2023/10/05/MyBatis-Plus整合笔记/","content":"Spring Boot集成MyBatis-Plus持久层框架实现数据库连接，支持自定义增删改查、条件构造器、分页插件、代码生成。 简介 | MyBatis-Plus (baomidou.com) 1、快速整合MyBatis-Plus第一步，在 pom.xml 中引入 starter。 1234&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 第二步，使用 @MapperScan 注解扫描 mapper 文件。 1234567891011@Configuration@ComponentScan(&quot;com.github.paicoding.forum.service&quot;)@MapperScan(basePackages = &#123; &quot;com.github.paicoding.forum.service.article.repository.mapper&quot;, &quot;com.github.paicoding.forum.service.user.repository.mapper&quot;, &quot;com.github.paicoding.forum.service.comment.repository.mapper&quot;, &quot;com.github.paicoding.forum.service.config.repository.mapper&quot;, &quot;com.github.paicoding.forum.service.statistics.repository.mappe &quot;com.github.paicoding.forum.service.notify.repository.mapper&quot;,&#125;public class ServiceAutoConfig &#123;&#125; ServiceAutoConfig 是单独的配置类，mapper 接口按照业务进行了分类，mapper.xml 放在 resources 目录下。 第三步，在 application.yml 文件中增加MyBatis-Plus 的统一配置。 123456# mybatis 相关统一配置mybatis-plus: configuration: #开启下划线转驼峰 map-underscore-to-camel-case: true 将数据库表中的下划线命名方式 （underscore case）映射为 Java 对象中的驼峰命名方式（camel case）。例如，数据库 表中的列名为 user_name，对应的 Java 对象的属性名为 userName。 2、MyBatis-Plus的基本使用2.1、Service CRUD示例：比如说我们要保存一个文章的标签。 123@Autowiredprivate TagDao tagDao;tagDao.save(tagDO); tagDao 是我们定义的数据访问对象（Data Access Object，简称 DAO），它继承自 MyBatis-Plus 提供的 ServiceImpl 类。 @Autowired 注解将 TagDao 自动注入到当前类 中。这是 Spring 提供的依赖注入（DI）功能，可以让我们在当前类中方便地使用 TagDao。 123@Repositorypublic class TagDao extends ServiceImpl&lt;TagMapper, TagDO&gt; &#123;&#125; @Repository 注解：这是 Spring 提供的注解，用于标识这个类是一个数据访问层 （DAO）组件。Spring 会自动扫描并将其实例化为一个 Bean，方便在其他类中通过依赖 注入（DI）使用。 ServiceImpl 是 MyBatis-Plus 提供的一个抽象类，提供了通用的 CRUD 方法。泛型参数 意味着 TagDao 类 主要用于处理 TagDO 数据对象的数据库操作，并使用 TagMapper 接口定义的方法进行操作。 通过继承 ServiceImpl 类，TagDao 就可以使用 MyBatis-Plus 提供的通用 CRUD 方法，如 save、getById、updateById 等。这些方法已经实现了基本的数据库操作，通常无需自 己编写 SQL 语句。 参数 tagDO 是一个数据对象（Data Object，简称 DO），表示数据库中的 tag 表。 12345678910111213141516171819202122@Data@EqualsAndHashCode(callSuper = true)@TableName(&quot;tag&quot;)public class TagDO extends BaseDO &#123; private static final long serialVersionUID = 3796460143933607644L; /** * 标签名称 */ private String tagName; /** * 标签类型：1-系统标签，2-自定义标签 */ private Integer tagType; /** * 状态：0-未发布，1-已发布 */ private Integer status; /** * 是否删除 */ private Integer deleted;&#125; @Data 注解是 Lombok 提供的，用于自动生成类的 getter、setter、equals、 hashCode 和 toString 方法，简化了代码编写。 @EqualsAndHashCode(callSuper &#x3D; true) 注解也是 Lombok 提供的注解， cal lSuper &#x3D; true 表示要调用父类（BaseDO）的 equals 和 hashCode 方法。 BaseDO 是我们自定义的 DO 基类，实现了 Serializable 接口 ，并且定义了主键 id（ @TableI d(type &#x3D; IdType.AUTO) 表示自增长，是 MyBatis-Plus 提供的注解），创建时间 createTime和更新时间 updateTime。 1234567@Datapublic class BaseDO implements Serializable &#123; @TableId(type = IdType.AUTO) private Long id; private Date createTime; private Date updateTime;&#125; @TableName(“tag”) 注解是 MyBatis-Plus 提供的注解，用于指定数据库表名。 另外定义了四个属性：tagName（标签名称）、tagType（标签类型）、status（状 态）和 deleted（是否删除）。这些属性对应数据库表中的列。 2.2、Mapper CRUDMyBatis-Plus 除了提供 Service 的 CRUD， 还提供了基于 Mapper 的 CRUD。 通常一些特殊的增删改查是通过 MyBatis-Plus 的 Mapper CRUD 接口实现的。 示例：比如说我们要保存文章，可以通过下面这种方式。 1234567891011121314@Repositorypublic class ArticleDao extends ServiceImpl&lt;ArticleMapper, ArticleDO&gt; &#123; @Resource private ArticleDetailMapper articleDetailMapper; public Long saveArticleContent(Long articleId, String content) &#123; ArticleDetailDO detail = new ArticleDetailDO(); detail.setArticleId(articleId); detail.setContent(content); detail.setVersion(1L); articleDetailMapper.insert(detail); return detail.getId(); &#125;&#125; articleDetailMapper 是我们在当前类中注入的一个 Mapper 接口，它继承自 MyBatis-Plus 的 BaseMapper 接口。 12public interface ArticleDetailMapper extends BaseMapper&lt;ArticleDetailDO&gt;&#123;&#125; 12345678910111213141516171819202122232425262728293031323334353637/** * Mapper 继承该接口后，无需编写 mapper.xml 文件，即可获得CRUD功能 * &lt;p&gt;这个 Mapper 支持 id 泛型&lt;/p&gt; * * @author hubin * @since 2016-01-23 */public interface BaseMapper&lt;T&gt; extends Mapper&lt;T&gt; &#123; /** * 插入一条记录 * * @param entity 实体对象 */ int insert(T entity); /** * 根据 entity 条件，删除记录 * * @param queryWrapper 实体对象封装操作类（可以为 null,里面的 entity 用 */ int delete(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper); /** * 根据 whereEntity 条件，更新记录 * * @param entity 实体对象 (set 条件值,可以为 null) * @param updateWrapper 实体对象封装操作类（可以为 null,里面的 entity */ int update(@Param(Constants.ENTITY) T entity, @Param(Constants.WRAP /** * 根据 ID 查询 * * @param id 主键ID */ T selectById(Serializable id);&#125; 这样，articleDetailMapper 也就具备了基本的增删改查功能。 2.3、常用注解 @TableName 用于指定数据库表名，通常在实体类（DO 或 Entity）上使用。例如： @TableName(“user”) 。 @TableId 用于指定表中的主键字段。通常在实体类的主键属性上使用。例如： @ TableId(value &#x3D; “id”, type &#x3D; IdType.AUTO) ，其中 value 表示主键字段名， type 表示主键生成策略。 @TableField 用于指定表中的非主键字段。可以用于实体类的属性上，以映射属 性和数据库字段。 @TableLogic 用于指定逻辑删除字段。逻辑删除是指在数据库中标记某个记录已 删除，而不是真正地删除记录。例如： @TableLogic(value &#x3D; “0”, delval &#x3D; “1”) ，其中 value 表示未删除状态的默认值，delval 表示删除状态的值。 @Version 用于指定乐观锁字段。乐观锁是一种并发控制策略，用于解决多个线程 同时修改同一条记录的问题。例如： @Version private Integer version; 3、MyBatis-Plus查询3.1、普通查询MyBatis-Plus 的 BaseMapper 提供了多种查询方法，比如说根据 ID 查找文章是这样用的： 1ArticleDO article = baseMapper.selectById(articleId); 除此之外，还有根据ID 批量查询的 selectBatchIds： 1baseMapper.selectBatchIds(Arrays.asList(1,2)); 根据键值对查询的 selectByMap： 123Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();map.put(&quot;id&quot;, 15L);List&lt;ArticleDO&gt; dtoList = baseMapper.selectByMap(map); 3.2、条件构造器MyBatis-Plus 的 Wrapper 是一个条件构造器，用于简化复杂的 SQL 查询条件的构建。它 提供了一系列易于使用的 API，让你能够以链式编程的方式编写查询条件，而不需要手动编 写 SQL 语句。 示例：假如我们来查询这样一个结果，包含“j”且状态是已发布的标签。我们可以这样来构建条件构造器。 12345678910@Testpublic void testWrapper() &#123; QueryWrapper&lt;TagDO&gt; wrapper = new QueryWrapper&lt;&gt;(); // 包含“j”且状态是已发布 wrapper.like(&quot;tag_name&quot;, &quot;j&quot;).eq(&quot;status&quot;, 1); BaseMapper&lt;TagDO&gt; baseMapper = tagDao.getBaseMapper(); List&lt;TagDO&gt; tagList = baseMapper.selectList(wrapper); tagList.forEach(System.out::println);&#125; QueryWrapper：用于构建查询条件。它继承自 AbstractWrapper，提供了各种查询条件 的构建方法，如 eq, ne, gt, ge, lt, le, like, isNull, orderBy 等等。详细见：条件构造器 | MyBatis-Plus (baomidou.com) 但是，通过表的字段总感觉很不舒服，万一哪一天数据库表发生变化了怎么办呢？代码和数据库就不匹配了呀。 更优雅的做法是采用 Lambda 的方式，如下查询标签示例： 123456789101112public List&lt;TagDTO&gt; listOnlineTag(String key, PageParam pageParam) &#123; LambdaQueryWrapper&lt;TagDO&gt; query = Wrappers.lambdaQuery(); query.eq(TagDO::getStatus, PushStatusEnum.ONLINE.getCode()) .eq(TagDO::getDeleted, YesOrNoEnum.NO.getCode()) .and(!StringUtils.isEmpty(key), v -&gt; v.like(TagDO::getTagNa .orderByDesc(TagDO::getId); if (pageParam != null) &#123; query.last(PageParam.getLimitSql(pageParam)); &#125; List&lt;TagDO&gt; list = baseMapper.selectList(query); return ArticleConverter.toDtoList(list);&#125; ①、可以通过 Wrappers.lambdaQuery() 静态方法创建一个 Lambda 条件构造器。 ②、 eq(TagDO::getStatus, PushStatusEnum.ONLINE.getCode()) ：表示查询条件 为 status 等于 PushStatusEnum.ONLINE 的值（即查询上线的标签）。 ③、 eq(TagDO::getDeleted, YesOrNoEnum.NO.getCode()) ：表示查询条件为 deleted 等于 YesOrNoEnum.NO 的值（即查询未删除的记录）。 ④、 and(!StringUtils.isEmpty(key), v -&gt; v.like(TagDO::getTagName, ke y)) ：表示如果 key 不为空，则添加一个查询条件，要求 tag_name 包含 key。 ⑤、 orderByDesc(TagDO::getId) ：表示按照 id 字段降序排序。 ⑥、 if (pageParam !&#x3D; null) { query.last(PageParam.getLimitSql(pagePara  m)); } ：如果 pageParam 不为 null，则添加分页参数。 这样的话，就可以和数据库的字段隔离开，完全通过代码的方式去查询。 3.3、MyBatis-Plus自定义SQLMyBatis-Plus 支持自定义 SQL 语句，我们可以在 Mapper 接口中编写自定义 SQL 方法， 并使用注解添加自定义的 SQL 语句。 示例：微信登录的时候会执行这条 SQL 语句。 12345678910public interface UserMapper extends BaseMapper&lt;UserDO&gt; &#123; /** * 根据三方唯一id进行查询 * * @param accountId * @return */ @Select(&quot;select * from user where third_account_id = #&#123;account_id&#125; UserDO getByThirdAccountId(@Param(&quot;account_id&quot;) String accountId);&#125; 接口中定义了一个名为 getByThirdAccountId 的方法，它接收一个名为 accountId 的参 数。 该方法使用了 @Select 注解，这个注解用于编写自定义的 SQL 查询。 @Select 注解内的 SQL 语句是： select * from user where third_account_id = #&#123;account_i d&#125; limit 1 ，它会根据传入的 account_id 参数查询 user 表中的记录。 同时，方法参数 accountId 使用了 @Param 注解，指定了参数在 SQL 语句中的名称为 account_id。这样，在执行 SQL 语句时，MyBatis 会将参数值替换到对应的位置上。 除此之外，可以使用 xml 的方式，用来定义一些复杂的 SQL。 比如说，我们要统计网站的 PV、UV，那么我们在 resources 目录下新建一个名为 QueryCountMapper.xml 的文件，内容如下： 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://my&lt;mapper namespace=&quot;com.github.paicoding.forum.service.statistics.reposi &lt;select id=&quot;getPvTotalCount&quot; resultType=&quot;java.lang.Long&quot;&gt; select sum(cnt) from request_count &lt;/select&gt; &lt;select id=&quot;getPvDayList&quot; resultType=&quot;com.github.paicoding.forum.ap SELECT sum(cnt) as count, date FROM request_count group by date order by date asc limit #&#123;day&#125;; &lt;/select&gt; &lt;select id=&quot;getUvDayList&quot; resultType=&quot;com.github.paicoding.forum.ap SELECT count(*) as count, date FROM request_count group by date order by date asc limit #&#123;day&#125;; &lt;/select&gt;&lt;/mapper&gt; ①、在 resources 目录下的好处是，MyBatis-Plus 默认帮我们配置了 xml 的位置，这样我们就不需要在 application.yml 中再配置了。 ②、该 XML 文件定义了一个RequestCountMapper 映射器，它包含三个自定义查询：getPvTotalCount、getPvDayList 和 getUvDayList。与com.github.paicoding.forum.service.statistics.repository.mapper.RequestCountMapper 相匹配。 12345678910111213141516171819202122public interface RequestCountMapper extends BaseMapper&lt;RequestCountDO&gt; /** * 获取 PV 总数 * * @return */ Long getPvTotalCount(); /** * 获取 PV 数据列表 * @param day * @return */ List&lt;StatisticsDayDTO&gt; getPvDayList(@Param(&quot;day&quot;) Integer day); /** * 获取 UV 数据列表 * * @param day * @return */ List&lt;StatisticsDayDTO&gt; getUvDayList(@Param(&quot;day&quot;) Integer day);&#125; 4、MyBatis-Plus主键策略 策略 含义 IdType.AUTO 自增策略，也就是说，在插入数据时，无需设置主键值，数据库会自动分配主键值，数据库表的 ID 会设置为 Auto Increment。 IdType.NONE 无主键策略。表示不使用任何主键生成策略，主键值需要手动设置。 IdType.UUID 使用 UUID 作为主键。插入数据时，MyBatis-Plus 会自动生成一个 UUID 值作为主键值。 IdType.ID_WORKER 使用雪花算法生成分布式唯一 ID。插入数据时，MyBatis-Plus 会 自动生成一个雪花 ID 作为主键值。 12345public class User &#123; @TableId(type = IdType.ID_WORKER) //默认 private Long id; // ...&#125;","categories":["Spring Boot"]},{"title":"Gateway+JWT实现登录认证","path":"/2023/10/05/Gateway+JWT实现登录认证/","content":"微服务架构整合Spring Cloud Gateway网关和JWT Token实现登录认证。 1、认证、授权、凭证1.1 认证（Authentication）认证表示你是谁。系统如何正确分辨出操作用户的真实身份，比如通过输入用户名和密码来辨别身份。 1.2 授权（Authorization）授权表示你能干什么。系统如何控制一个用户能看到哪些数据和操作哪些功能，也就是具有哪些权限。 1.3 凭证（Credential）表示你如何证明你的身份。系统如何保证它与用户之间的承诺是双方当时真实意图的体现，是准确、完整和不可抵赖的。 关于凭证的存储方案，业界的安全架构中有两种方案： Cookie-Session 模式 JWT 方案 1.3.1、Cookie-Session 模式如下图示： 优点： 状态信息都存储于服务器，只要依靠客户端的同源策略和 HTTPS 的传输层安全，保证 Cookie 中的键值不被窃取而出现被冒认身份的情况，就能完全规避掉上下文信息在传输过程中被泄漏和篡改的风险。 缺点： 在单节点的单体服务中再适合不过，但是如果需要水平扩展要部署集群就很麻烦。 如果让 session 分配到不同的的节点上，不重复地保存着一部分用户的状态，用户的请求固定分配到对应的节点上，如果某个节点崩溃了，则里面的用户状态就会完全丢失。如果让 session 复制到所有节点上，那么同步的成本又会很高。 1.3.2、JWT方案上面说到 Cookie-Session 机制在分布式环境下会遇到一致性和同步成本的问题，而且如果在多方系统中，则更不能将 Session 共享存放在多方系统的服务端中，即使服务端之间能共享数据，Cookie 也没有办法跨域。 转换思路，服务端不保存任何状态信息，由客户端来存储，每次发送请求时携带这个状态信息发给后端服务。原理图如下所示： JWT（JSON WEB TOKEN）是一种令牌格式，经常与 OAuth2.0 配合应用于分布式、多方的应用系统中。 我们先来看下 JWT 的格式长什么样： 左边的字符串就是 JWT 令牌，JWT 令牌是服务端生成的，客户端会拿着这个 JWT 令牌在每次发送请求时放到 HTTP header 中。 而右边是 JWT 经过 Base64 解码后展示的明文内容，而这段明文内容的最下方，又有一个签名内容，可以防止内容篡改，但是不能解决泄漏的问题。 JWT 格式 JWT 令牌是以 JSON 结构存储，用点号分割为三个部分。 第一部分是令牌头（Header），内容如下所示： 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; 它描述了令牌的类型（统一为 typ:JWT）以及令牌签名的算法，示例中 HS256 为 HMAC SHA256 算法的缩写。 令牌的第二部分是负载（Payload），这是令牌是真正需要向服务端传递的信息。但是服务端不会直接用这个负载，而是通过加密传过来的 Header 和 Payload 后再比对签名是否一致来判断负载是否被篡改，如果没有被篡改，才能用 Payload 中的内容。因为负载只是做了 base64 编码，并不是加密，所以是不安全的，千万别把敏感信息比如密码放到负载里面。 12345&#123; &quot;sub&quot;: &quot;passjava&quot;, &quot;name&quot;: &quot;悟空聊架构&quot;, &quot;iat&quot;: 1516239022&#125; 令牌的第三部分是签名（Signature），使用在对象头中公开的特定签名算法，通过特定的密钥（Secret，由服务器进行保密，不能公开）对前面两部分内容进行加密计算，以例子里使用的 JWT 默认的 HMAC SHA256 算法为例，将通过以下公式产生签名值： 1HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload) , secret) 签名的意义：确保负载中的信息是可信的、没有被篡改的，也没有在传输过程中丢失任何信息。因为被签名的内容哪怕发生了一个字节的变动，也会导致整个签名发生显著变化。此外，由于签名这件事情只能由认证授权服务器完成（只有它知道 Secret），任何人都无法在篡改后重新计算出合法的签名值，所以服务端才能够完全信任客户端传上来的 JWT 中的负载信息。 JWT 的优势 无状态：不需要服务端保存 JWT 令牌，也就是说不需要服务节点保留任何一点状态信息，就能在后续的请求中完成认证功能。 天然的扩容便利：服务做水平扩容不用考虑 JWT 令牌，而 Cookie-Session 是需要考虑扩容后服务节点如何存储 Session 的。 不依赖 Cookie：JWT 可以存放在浏览器的 LocalStorage，不一定非要存储在 Cookie 中。 JWT 的劣势 令牌难以主动失效：JWT 令牌签发后，理论上和认证的服务器就没有什么关系了，到期之前始终有效。除非服务器加些特殊的逻辑处理来缓存 JWT，并来管理 JWT 的生命周期，但是这种方式又会退化成有状态服务。而这种要求有状态的需求又很常见：譬如用户退出后，需要重新输入用户名和密码才能登录；或者用户只允许在一台设备登录，登录到另外一台设备，要求强行退出。但是这种有状态的模式，降低了 JWT 本身的价值。 更容易遭受重放攻击：Cookie-Session 也有重放攻击的问题，也就是客户端可以拿着这个 cookie 不断发送大量请求，对系统性能造成影响。但是因为 Session 在服务端也有一份，服务端可以控制 session 的生命周期，应对重放攻击更加主动一些。但是 JWT 的重放攻击对于服务端来说就很被动，比如通过客户端的验证码、服务端限流或者缩短令牌有效期，应用起来都会麻烦些。 存在泄漏的风险：客户端存储，很有可能泄漏出去，被其他人重复利用。 信息大小有限：HTTP 协议并没有强制约束 Header 的最大长度，但是服务器、浏览器会做限制。而且如果令牌很大还会消耗传输带宽。 2、认证原理图在如下的认证时序图中，有以下几种角色： 客户端：表示 APP 端或 PC 端的前端页面。 网关：表示 Spring Cloud Gateway 网关服务。 认证服务：用来接收客户的登录请求、登出请求、刷新令牌的操作。 业务服务：和系统业务相关的微服务。 认证和校验身份的流程如下所示： ① 用户登录：客户端在登录页面输入用户名和密码，提交表单，调用登录接口。 ② 转发请求：这里会先将登录请求发送到网关服务 passjava-gateway，网关对于登录请求会直接转发到认证服务 passjava-auth。（网关对登录请求不做 token 校验，这个可以配置不校验哪些请求 URL） ③ 认证：认证服务会将请求参数中的用户名+密码和数据库中的用户进行比对，如果完全匹配，则认证通过。 ④ 生成令牌：生成两个令牌：access_token 和 refresh_token（刷新令牌），刷新令牌我们后面再说，这里其实也可以只用生成一个令牌 access_token。令牌里面会包含用户的身份信息，如果要做权限管控，还需要在 token 里面包含用户的权限信息，权限这一块不在本篇展开，会放到下一篇中进行讲解。 ⑤ 客户端缓存 token：客户端拿到两个 token 缓存到 cookie 中或者 LocalStorage 中。 ⑥ 携带 token 发起请求：客户端下次想调用业务服务时，将 access_token 放到请求的 header 中。 ⑦ 网关校验 token：请求还是先到到网关服务，然后由它校验 access_token 是否合法。如果 access_token 未过期，且能正确解析出来，就说明是合法的 access_token。 ⑧ 携带用户身份信息转发请求：网关将 access_token 中携带的用户的 user_id 放到请求的 header 中，转发给真正的业务服务。 ⑨ 处理业务逻辑：业务服务从 header 中拿到用户的 user_id，然后处理业务逻辑，处理完后将结果延原理返回给客户端。 3、实现细节更多细节见：实战SpringCloud gateway+JWT认证 3.1、如何做登录认证 步骤： 提交用户名和密码 网关服务转发登录请求 数据库查找用户密码，验证成功后生成JWT令牌 3.2、如何生成令牌生成令牌就是通过工具类 PassJavaJwtTokenUtil 生成 JWT Token。 3.3、如何携带JWT发送请求 客户端（浏览器或 APP）拿到 JWT 后，可以将 JWT 存放在浏览器的 Cookie 或 LocalStorage（本地存储） 或者内存中。 发送请求时在请求 Header 的 Authorization 字段中设置 JWT。 3.4、网关如何验证和转发请求 网关接收到前端发起的业务请求后，会先验证请求的 Header 中是否携带 Authorization 字段，以及里面的 Token 是否合法。然后解析 Token 中的 userId 和 username，放到 header 中再进行转发。 网关是通过多个过滤器 Filter对请求进行串行拦截处理的，所以我们可以自定义一个全局过滤器，对所有请求进行校验，当然对于一些特殊请求比如登录请求就不需要校验了，因为调用登录请求的时候还没有生成 Token。 3.5、会员业务逻辑处理会员服务接收到网关转发的请求后，就从 Header 中拿到用户身份信息，然后通过 userId 获取会员信息。 获取 userId 的方式其实可以通过加一个拦截器，由拦截器将 Header 中的 userId 和 username 放到线程中，后续的 controller，service，dao 类都可以从线程里面拿到 userId 和 username，不用通过传参的方式。 获取 userId 的方式： 方式一：从 request 的 Header 中拿到 userId。代码简单，但是如果其他地方也要用到 userId，则需要通过方法传参的方式传递 userId。 方式二：从线程变量里面拿到 userId。代码复杂，使用简单。好处是所有地方统一从一个地方获取。 3.6、如何刷新令牌当认证服务返回给客户端的 JWT 也就是 access_token 过期后，客户端是通过发送登录请求重新拿到 access_token 吗？ 这种重新登录的操作如果很频繁（因 JWT 过期时间较短），对于用户来说体验就很差了。客户端需要跳转到登录页面，让用户重新提交用户名和密码，即使客户端有记住用户名和密码，但是这种跳转的到登录页的操作会大幅度降低用户的体验。 有没有一种比较优雅的方式让客户端重新拿到 access_token 或者说延长 access_token 有效期呢？ 我们知道 JWT 生成后是不能篡改里面的内容，即使是 JWT 的有效期也不行。所以延长 access_token 有效期的做法并不适合，而且如果长期保持一个 access_token 有效，也是不安全的。 那就只能重新生成 access_token 了。方案其实挺简单，客户端拿之前生成的 JWT 调用后端一个接口，然后后端校验这个 JWT 是否合法，如果是合法的就重新生成一个新的返回给客户端。客户端自行替换掉之前本地保存的 access_token 就可以了。 这里有一个巧妙的设计，就是生成 JWT 时，返回了两个 JWT token，一个 access_token，一个 refresh_token，这两个 token 其实都可以用来刷新 token，但是我们把 refresh_token 设置的过期时间稍微长一点，比如两倍于 access_token，当 access_token 过期后，refresh_token 如果还没有过期，就可以利用两者的过期时间差进行重新生成令牌的操作，也就是刷新令牌，这里的刷新指的是客户端重置本地保存的令牌，以后都用新的令牌。 总结： 在登录认证中，刷新令牌和设置过期重登是常见的安全机制，用于保证用户的登录状态和防止令牌被滥用。下面是一种常见的实现方式： 生成令牌和刷新令牌： - 在用户登录成功后，生成一个访问令牌（access token）和一个刷新令牌（refresh token）。 - 访问令牌用于验证用户的身份和访问权限，通常具有较短的有效期。 - 刷新令牌用于在访问令牌过期时获取新的访问令牌，通常具有较长的有效期。 访问令牌的验证： - 在每个请求中，服务端需要验证访问令牌的有效性。 - 可以通过在请求头或请求参数中携带访问令牌，并在服务端进行解析和验证。 - 如果访问令牌已过期或无效，服务端会返回相应的错误响应。 刷新令牌的使用： - 当访问令牌过期时，客户端可以使用刷新令牌来获取新的访问令牌。 - 客户端发送一个特殊的请求，携带刷新令牌。 - 服务端验证刷新令牌的有效性，并根据刷新令牌颁发一个新的访问令牌。 设置过期重登： - 如果用户长时间不活动或注销登录，访问令牌会过期失效。 - 当用户再次访问需要登录的接口时，服务端会返回一个需要重新登录的错误响应。 - 客户端收到该错误响应后，需要引导用户重新进行登录认证。 通过刷新令牌和设置过期重登的机制，可以在保证用户登录状态的同时，提高系统的安全性。请注意，具体的实现方式可能会根据实际情况和框架的不同而有所差异。","categories":["项目实战"]},{"title":"Easy-RPC框架实现","path":"/2023/10/05/Easy-RPC框架实现/","content":"一款基于 Nacos 实现的 RPC 框架。网络传输实现了基于 Java 原生 Socket 与 Netty 版本，并且实现了多种序列化与负载均衡算法。 项目地址：https://gitcode.net/mirrors/cn-guoziyang/my-rpc-framework?utm_source=csdn_github_accelerator 文档记录：https://blog.csdn.net/qq_40856284/category_10138756.html RPC框架图： 1、一个最简单的实现大体思路： 客户端和服务端都可以访问到通用的接口，但是只有服务端有这个接口的实现类，客户端调用这个接口的方式，是通过网络传输，告诉服务端我要调用这个接口，服务端收到之后找到这个接口的实现类，并且执行，将执行的结果返回给客户端，作为客户端调用接口方法的返回值。 需要考虑的： 客户端怎么知道服务端的地址？客户端怎么告诉服务端我要调用的接口？客户端怎么传递参数？只有接口客户端怎么生成实现类？ 这一章，我们就来探讨一个最简单的实现。一个最简单的实现，基于这样一个假设，那就是客户端已经知道了服务端的地址，这部分会由后续的服务发现机制完善。 1.1、通用接口我们先把通用的接口写好，然后再来看怎么实现客户端和服务端。 接口如下： 123public interface HelloService &#123; String hello(HelloObject object);&#125; hello方法需要传递一个对象，HelloObject对象，定义如下： 123456@Data@AllArgsConstructorpublic class HelloObject implements Serializable &#123; private Integer id; private String message;&#125; 注意这个对象需要实现Serializable接口，因为它需要在调用过程中从客户端传递给服务端。 接着我们在服务端对这个接口进行实现，实现的方式也很简单，返回一个字符串就行： 12345678public class HelloServiceImpl implements HelloService &#123; private static final Logger logger = LoggerFactory.getLogger(HelloServiceImpl.class); @Override public String hello(HelloObject object) &#123; logger.info(&quot;接收到：&#123;&#125;&quot;, object.getMessage()); return &quot;这是掉用的返回值，id=&quot; + object.getId(); &#125;&#125; 1.2、传输方式我们来思考一下，服务端需要哪些信息，才能唯一确定服务端需要调用的接口的方法呢？ 首先，就是接口的名字，和方法的名字，但是由于方法重载的缘故，我们还需要这个方法的所有参数的类型，最后，客户端调用时，还需要传递参数的实际值，那么服务端知道以上四个条件，就可以找到这个方法并且调用了。我们把这四个条件写到一个对象里，到时候传输时传输这个对象就行了。即RpcRequest对象： 1234567891011121314151617181920@Data@Builderpublic class RpcRequest implements Serializable &#123; /** * 待调用接口名称 */ private String interfaceName; /** * 待调用方法名称 */ private String methodName; /** * 调用方法的参数 */ private Object[] parameters; /** * 调用方法的参数类型 */ private Class&lt;?&gt;[] paramTypes;&#125; 参数类型我是直接使用Class对象，其实用字符串也是可以的。 那么服务器调用完这个方法后，需要给客户端返回哪些信息呢？如果调用成功的话，显然需要返回值，如果调用失败了，就需要失败的信息，这里封装成一个RpcResponse对象： 12345678910111213141516171819202122232425262728@Datapublic class RpcResponse&lt;T&gt; implements Serializable &#123; /** * 响应状态码 */ private Integer statusCode; /** * 响应状态补充信息 */ private String message; /** * 响应数据 */ private T data; public static &lt;T&gt; RpcResponse&lt;T&gt; success(T data) &#123; RpcResponse&lt;T&gt; response = new RpcResponse&lt;&gt;(); response.setStatusCode(ResponseCode.SUCCESS.getCode()); response.setData(data); return response; &#125; public static &lt;T&gt; RpcResponse&lt;T&gt; fail(ResponseCode code) &#123; RpcResponse&lt;T&gt; response = new RpcResponse&lt;&gt;(); response.setStatusCode(code.getCode()); response.setMessage(code.getMessage()); return response; &#125;&#125; 这里还多写了两个静态方法，用于快速生成成功与失败的响应对象。其中，statusCode属性可以自行定义，客户端服务端一致即可。 1.3、客户端的实现——动态代理客户端方面，由于在客户端这一侧我们并没有接口的具体实现类，就没有办法直接生成实例对象。这时，我们可以通过动态代理的方式生成实例，并且调用方法时生成需要的RpcRequest对象并且发送给服务端。 这里我们采用JDK动态代理，代理类是需要实现InvocationHandler接口的。 1234567891011121314public class RpcClientProxy implements InvocationHandler &#123; private String host; private int port; public RpcClientProxy(String host, int port) &#123; this.host = host; this.port = port; &#125; @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T getProxy(Class&lt;T&gt; clazz) &#123; return (T) Proxy.newProxyInstance(clazz.getClassLoader(), new Class&lt;?&gt;[]&#123;clazz&#125;, this); &#125;&#125; 我们需要传递host和port来指明服务端的位置。并且使用getProxy()方法来生成代理对象。 InvocationHandler接口需要实现invoke()方法，来指明代理对象的方法被调用时的动作。在这里，我们显然就需要生成一个RpcRequest对象，发送出去，然后返回从服务端接收到的结果即可： 1234567891011@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; RpcRequest rpcRequest = RpcRequest.builder() .interfaceName(method.getDeclaringClass().getName()) .methodName(method.getName()) .parameters(args) .paramTypes(method.getParameterTypes()) .build(); RpcClient rpcClient = new RpcClient(); return ((RpcResponse) rpcClient.sendRequest(rpcRequest, host, port)).getData();&#125; 生成RpcRequest很简单，我使用Builder模式来生成这个对象。发送的逻辑我使用了一个RpcClient对象来实现，这个对象的作用，就是将一个对象发过去，并且接受返回的对象。 1234567891011121314151617public class RpcClient &#123; private static final Logger logger = LoggerFactory.getLogger(RpcClient.class); public Object sendRequest(RpcRequest rpcRequest, String host, int port) &#123; try (Socket socket = new Socket(host, port)) &#123; ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream()); ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); objectOutputStream.writeObject(rpcRequest); objectOutputStream.flush(); return objectInputStream.readObject(); &#125; catch (IOException | ClassNotFoundException e) &#123; logger.error(&quot;调用时有错误发生：&quot;, e); return null; &#125; &#125;&#125; 我的实现很简单，直接使用Java的序列化方式，通过Socket传输。创建一个Socket，获取ObjectOutputStream对象，然后把需要发送的对象传进去即可，接收时获取ObjectInputStream对象，readObject()方法就可以获得一个返回的对象。 1.4、服务端的实现——反射调用服务端的实现就简单多了，使用一个ServerSocket监听某个端口，循环接收连接请求，如果发来了请求就创建一个线程，在新线程中处理调用。这里创建线程采用线程池： 123456789101112131415public class RpcServer &#123; private final ExecutorService threadPool; private static final Logger logger = LoggerFactory.getLogger(RpcServer.class); public RpcServer() &#123; int corePoolSize = 5; int maximumPoolSize = 50; long keepAliveTime = 60; BlockingQueue&lt;Runnable&gt; workingQueue = new ArrayBlockingQueue&lt;&gt;(100); ThreadFactory threadFactory = Executors.defaultThreadFactory(); threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.SECONDS, workingQueue, threadFactory); &#125; &#125; 这里简化了一下，RpcServer暂时只能注册一个接口，即对外提供一个接口的调用服务，添加register方法，在注册完一个服务后立刻开始监听： 123456789101112public void register(Object service, int port) &#123; try (ServerSocket serverSocket = new ServerSocket(port)) &#123; logger.info(&quot;服务器正在启动...&quot;); Socket socket; while((socket = serverSocket.accept()) != null) &#123; logger.info(&quot;客户端连接！Ip为：&quot; + socket.getInetAddress()); threadPool.execute(new WorkerThread(socket, service)); &#125; &#125; catch (IOException e) &#123; logger.error(&quot;连接时有错误发生：&quot;, e); &#125;&#125; 这里向工作线程WorkerThread传入了socket和用于服务端实例service。 WorkerThread实现了Runnable接口，用于接收RpcRequest对象，解析并且调用，生成RpcResponse对象并传输回去。run方法如下： 12345678910111213@Overridepublic void run() &#123; try (ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream())) &#123; RpcRequest rpcRequest = (RpcRequest) objectInputStream.readObject(); Method method = service.getClass().getMethod(rpcRequest.getMethodName(), rpcRequest.getParamTypes()); Object returnObject = method.invoke(service, rpcRequest.getParameters()); objectOutputStream.writeObject(RpcResponse.success(returnObject)); objectOutputStream.flush(); &#125; catch (IOException | ClassNotFoundException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) &#123; logger.error(&quot;调用或发送时有错误发生：&quot;, e); &#125;&#125; 其中，通过class.getMethod方法，传入方法名和方法参数类型即可获得Method对象。如果你上面RpcRequest中使用String数组来存储方法参数类型的话，这里你就需要通过反射生成对应的Class数组了。通过method.invoke方法，传入对象实例和参数，即可调用并且获得返回值。 1.5、测试服务端侧，我们已经在上面实现了一个HelloService的实现类HelloServiceImpl的实现类了，我们只需要创建一个RpcServer并且把这个实现类注册进去就行了： 1234567public class TestServer &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); RpcServer rpcServer = new RpcServer(); rpcServer.register(helloService, 9000); &#125;&#125; 服务端开放在9000端口。 客户端方面，我们需要通过动态代理，生成代理对象，并且调用，动态代理会自动帮我们向服务端发送请求的： 123456789public class TestClient &#123; public static void main(String[] args) &#123; RpcClientProxy proxy = new RpcClientProxy(&quot;127.0.0.1&quot;, 9000); HelloService helloService = proxy.getProxy(HelloService.class); HelloObject object = new HelloObject(12, &quot;This is a message&quot;); String res = helloService.hello(object); System.out.println(res); &#125;&#125; 我们这里生成了一个HelloObject对象作为方法的参数。 首先启动服务端，再启动客户端，服务端输出： 123服务器正在启动...客户端连接！Ip为：127.0.0.1接收到：This is a message 客户端输出： 1这是调用的返回值，id=12 2、注册多个服务上一节中，我们使用 JDK 序列化和 Socket 实现了一个最基本的 RPC 框架，服务端测试时是这样的： 1234567public class TestServer &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); RpcServer rpcServer = new RpcServer(); rpcServer.register(helloService, 9000); &#125;&#125; 在注册完 helloService 后，服务器就自行启动了，也就是说，一个服务器只能注册一个服务，这个设计非常不好（毕竟是简易实现）。这一节，我们就将服务的注册和服务器启动分离，使得服务端可以提供多个服务。 2.1、服务注册表我们需要一个容器，这个容器很简单，就是保存一些本地服务的信息，并且在获得一个服务名字的时候能够返回这个服务的信息。创建一个 ServiceRegistry 接口： 123456package top.guoziyang.rpc.registry;public interface ServiceRegistry &#123; &lt;T&gt; void register(T service); Object getService(String serviceName);&#125; 一目了然，一个register注册服务信息，一个getService获取服务信息。 我们新建一个默认的注册表类 DefaultServiceRegistry 来实现这个接口，提供服务注册服务，如下： 12345678910111213141516171819202122232425262728293031public class DefaultServiceRegistry implements ServiceRegistry &#123; private static final Logger logger = LoggerFactory.getLogger(DefaultServiceRegistry.class); private final Map&lt;String, Object&gt; serviceMap = new ConcurrentHashMap&lt;&gt;(); private final Set&lt;String&gt; registeredService = ConcurrentHashMap.newKeySet(); @Override public synchronized &lt;T&gt; void register(T service) &#123; String serviceName = service.getClass().getCanonicalName(); if(registeredService.contains(serviceName)) return; registeredService.add(serviceName); Class&lt;?&gt;[] interfaces = service.getClass().getInterfaces(); if(interfaces.length == 0) &#123; throw new RpcException(RpcError.SERVICE_NOT_IMPLEMENT_ANY_INTERFACE); &#125; for(Class&lt;?&gt; i : interfaces) &#123; serviceMap.put(i.getCanonicalName(), service); &#125; logger.info(&quot;向接口: &#123;&#125; 注册服务: &#123;&#125;&quot;, interfaces, serviceName); &#125; @Override public synchronized Object getService(String serviceName) &#123; Object service = serviceMap.get(serviceName); if(service == null) &#123; throw new RpcException(RpcError.SERVICE_NOT_FOUND); &#125; return service; &#125;&#125; 我们将服务名与提供服务的对象的对应关系保存在一个 ConcurrentHashMap 中，并且使用一个 Set 来保存当前有哪些对象已经被注册。 在注册服务时，默认采用这个对象实现的接口的完整类名作为服务名，例如某个对象 A 实现了接口 X 和 Y，那么将 A 注册进去后，会有两个服务名 X 和 Y 对应于 A 对象。这种处理方式也就说明了某个接口只能有一个对象提供服务。 2.2、其他处理为了降低耦合度，我们不会把 ServiceRegistry 和某一个 RpcServer 绑定在一起，而是在创建 RpcServer 对象时，传入一个 ServiceRegistry 作为这个服务的注册表。 那么 RpcServer 这个类现在就变成了这样： 123456789101112131415161718192021222324252627282930313233public class RpcServer &#123; private static final Logger logger = LoggerFactory.getLogger(RpcServer.class); private static final int CORE_POOL_SIZE = 5; private static final int MAXIMUM_POOL_SIZE = 50; private static final int KEEP_ALIVE_TIME = 60; private static final int BLOCKING_QUEUE_CAPACITY = 100; private final ExecutorService threadPool; private RequestHandler requestHandler = new RequestHandler(); private final ServiceRegistry serviceRegistry; public RpcServer(ServiceRegistry serviceRegistry) &#123; this.serviceRegistry = serviceRegistry; BlockingQueue&lt;Runnable&gt; workingQueue = new ArrayBlockingQueue&lt;&gt;(BLOCKING_QUEUE_CAPACITY); ThreadFactory threadFactory = Executors.defaultThreadFactory(); threadPool = new ThreadPoolExecutor(CORE_POOL_SIZE, MAXIMUM_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, workingQueue, threadFactory); &#125; public void start(int port) &#123; try (ServerSocket serverSocket = new ServerSocket(port)) &#123; logger.info(&quot;服务器启动……&quot;); Socket socket; while((socket = serverSocket.accept()) != null) &#123; logger.info(&quot;消费者连接: &#123;&#125;:&#123;&#125;&quot;, socket.getInetAddress(), socket.getPort()); threadPool.execute(new RequestHandlerThread(socket, requestHandler, serviceRegistry)); &#125; threadPool.shutdown(); &#125; catch (IOException e) &#123; logger.error(&quot;服务器启动时有错误发生:&quot;, e); &#125; &#125;&#125; 在创建 RpcServer 时需要传入一个已经注册好服务的 ServiceRegistry，而原来的 register 方法也被改成了 start 方法，因为服务的注册已经不由 RpcServer 处理了，它只需要启动就行了。 而在每一个请求处理线程（RequestHandlerThread）中也就需要传入 ServiceRegistry 了，这里把处理线程和处理逻辑分成了两个类：RequestHandlerThread 只是一个线程，从ServiceRegistry 获取到提供服务的对象后，就会把 RpcRequest 和服务对象直接交给 RequestHandler 去处理，反射等过程被放到了 RequestHandler 里。 RequesthandlerThread.java：处理线程，接受对象等 1234567891011121314151617181920212223242526272829public class RequestHandlerThread implements Runnable &#123; private static final Logger logger = LoggerFactory.getLogger(RequestHandlerThread.class); private Socket socket; private RequestHandler requestHandler; private ServiceRegistry serviceRegistry; public RequestHandlerThread(Socket socket, RequestHandler requestHandler, ServiceRegistry serviceRegistry) &#123; this.socket = socket; this.requestHandler = requestHandler; this.serviceRegistry = serviceRegistry; &#125; @Override public void run() &#123; try (ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream())) &#123; RpcRequest rpcRequest = (RpcRequest) objectInputStream.readObject(); String interfaceName = rpcRequest.getInterfaceName(); Object service = serviceRegistry.getService(interfaceName); Object result = requestHandler.handle(rpcRequest, service); objectOutputStream.writeObject(RpcResponse.success(result)); objectOutputStream.flush(); &#125; catch (IOException | ClassNotFoundException e) &#123; logger.error(&quot;调用或发送时有错误发生：&quot;, e); &#125; &#125;&#125; RequestHandler.java：通过反射进行方法调用 1234567891011121314151617181920212223public class RequestHandler &#123; private static final Logger logger = LoggerFactory.getLogger(RequestHandler.class); public Object handle(RpcRequest rpcRequest, Object service) &#123; Object result = null; try &#123; result = invokeTargetMethod(rpcRequest, service); logger.info(&quot;服务:&#123;&#125; 成功调用方法:&#123;&#125;&quot;, rpcRequest.getInterfaceName(), rpcRequest.getMethodName()); &#125; catch (IllegalAccessException | InvocationTargetException e) &#123; logger.error(&quot;调用或发送时有错误发生：&quot;, e); &#125; return result; &#125; private Object invokeTargetMethod(RpcRequest rpcRequest, Object service) throws IllegalAccessException, InvocationTargetException &#123; Method method; try &#123; method = service.getClass().getMethod(rpcRequest.getMethodName(), rpcRequest.getParamTypes()); &#125; catch (NoSuchMethodException e) &#123; return RpcResponse.fail(ResponseCode.METHOD_NOT_FOUND); &#125; return method.invoke(service, rpcRequest.getParameters()); &#125;&#125; 在这种情况下，客户端完全不需要做任何改动。 2.3、测试服务端的测试 123456789public class TestServer &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); ServiceRegistry serviceRegistry = new DefaultServiceRegistry(); serviceRegistry.register(helloService); RpcServer rpcServer = new RpcServer(serviceRegistry); rpcServer.start(9000); &#125;&#125; 3、Netty传输和通用序列化接口本节我们会将传统的 BIO 方式传输换成效率更高的 NIO 方式，当然不会使用 Java 原生的 NIO，而是采用更为简单的 Netty。本节还会实现一个通用的序列化接口，为多种序列化支持做准备，并且，本节还会自定义传输的协议。 3.1、Netty 服务端与客户端首先就需要在 pom.xml 中加入 Netty 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt;\t&lt;version&gt;$&#123;netty-version&#125;&lt;/version&gt;&lt;/dependency&gt; netty 的最新版本可以在 mavenrepository查到，注意使用 netty 4 而不是 netty 5。 为了保证通用性，我们可以把 Server 和 Client 抽象成两个接口，分别是 RpcServer 和 RpcClient： 1234567public interface RpcServer &#123; void start(int port);&#125;public interface RpcClient &#123; Object sendRequest(RpcRequest rpcRequest);&#125; 而原来的 RpcServer 和 RpcClient 类实际上是上述两个接口的 Socket 方式实现类，改成 SocketServer 和 SocketClient 并实现上面两个接口即可，几乎不需要做什么修改。 我们的任务，就是要实现 NettyServer 和 NettyClient。 这里提一个改动，就是在 DefaultServiceRegistry.java 中，将包含注册信息的 serviceMap 和 registeredService 都改成了 static ，这样就能保证全局唯一的注册信息，并且在创建 RpcServer 时也就不需要传入了。 NettyServer的实现很传统 12345678910111213141516171819202122232425262728293031323334353637public class NettyServer implements RpcServer &#123; private static final Logger logger = LoggerFactory.getLogger(NettyServer.class); @Override public void start(int port) &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .handler(new LoggingHandler(LogLevel.INFO)) .option(ChannelOption.SO_BACKLOG, 256) .option(ChannelOption.SO_KEEPALIVE, true) .childOption(ChannelOption.TCP_NODELAY, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new CommonEncoder(new JsonSerializer())); pipeline.addLast(new CommonDecoder()); pipeline.addLast(new NettyServerHandler()); &#125; &#125;); ChannelFuture future = serverBootstrap.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; logger.error(&quot;启动服务器时有错误发生: &quot;, e); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 了解过 Netty 的同学可能知道，Netty 中有一个很重要的设计模式——责任链模式，责任链上有多个处理器，每个处理器都会对数据进行加工，并将处理后的数据传给下一个处理器。代码中的 CommonEncoder、CommonDecoder和NettyServerHandler 分别就是编码器，解码器和数据处理器。因为数据从外部传入时需要解码，而传出时需要编码，类似计算机网络的分层模型，每一层向下层传递数据时都要加上该层的信息，而向上层传递时则需要对本层信息进行解码。 而 NettyClient 的实现也很类似： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class NettyClient implements RpcClient &#123; private static final Logger logger = LoggerFactory.getLogger(NettyClient.class); private String host; private int port; private static final Bootstrap bootstrap; public NettyClient(String host, int port) &#123; this.host = host; this.port = port; &#125; static &#123; EventLoopGroup group = new NioEventLoopGroup(); bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.SO_KEEPALIVE, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new CommonDecoder()) .addLast(new CommonEncoder(new JsonSerializer())) .addLast(new NettyClientHandler()); &#125; &#125;); &#125; @Override public Object sendRequest(RpcRequest rpcRequest) &#123; try &#123; ChannelFuture future = bootstrap.connect(host, port).sync(); logger.info(&quot;客户端连接到服务器 &#123;&#125;:&#123;&#125;&quot;, host, port); Channel channel = future.channel(); if(channel != null) &#123; channel.writeAndFlush(rpcRequest).addListener(future1 -&gt; &#123; if(future1.isSuccess()) &#123; logger.info(String.format(&quot;客户端发送消息: %s&quot;, rpcRequest.toString())); &#125; else &#123; logger.error(&quot;发送消息时有错误发生: &quot;, future1.cause()); &#125; &#125;); channel.closeFuture().sync(); AttributeKey&lt;RpcResponse&gt; key = AttributeKey.valueOf(&quot;rpcResponse&quot;); RpcResponse rpcResponse = channel.attr(key).get(); return rpcResponse.getData(); &#125; &#125; catch (InterruptedException e) &#123; logger.error(&quot;发送消息时有错误发生: &quot;, e); &#125; return null; &#125;&#125; 在静态代码块中就直接配置好了 Netty 客户端，等待发送数据时启动，channel 将 RpcRequest 对象写出，并且等待服务端返回的结果。注意这里的发送是非阻塞的，所以发送后会立刻返回，而无法得到结果。这里通过 AttributeKey 的方式阻塞获得返回结果： 12AttributeKey&lt;RpcResponse&gt; key = AttributeKey.valueOf(&quot;rpcResponse&quot;);RpcResponse rpcResponse = channel.attr(key).get(); 通过这种方式获得全局可见的返回结果，在获得返回结果 RpcResponse 后，将这个对象以 key 为 rpcResponse 放入 ChannelHandlerContext 中，这里就可以立刻获得结果并返回，我们会在 NettyClientHandler 中看到放入的过程。 3.2、自定义协议与编解码器在传输过程中，我们可以在发送的数据上加上各种必要的数据，形成自定义的协议，而自动加上这个数据就是编码器的工作，解析数据获得原始数据就是解码器的工作。 我们定义的协议是这样的： 1234567+---------------+---------------+-----------------+-------------+| Magic Number | Package Type | Serializer Type | Data Length || 4 bytes | 4 bytes | 4 bytes | 4 bytes |+---------------+---------------+-----------------+-------------+| Data Bytes || Length: $&#123;Data Length&#125; |+---------------------------------------------------------------+ 首先是 4 字节魔数，表识一个协议包。接着是 Package Type，标明这是一个调用请求还是调用响应，Serializer Type 标明了实际数据使用的序列化器，这个服务端和客户端应当使用统一标准；Data Length 就是实际数据的长度，设置这个字段主要防止粘包，最后就是经过序列化后的实际数据，可能是 RpcRequest 也可能是 RpcResponse 经过序列化后的字节，取决于 Package Type。 规定好协议后，我们就可以来看看 CommonEncoder 了： 123456789101112131415161718192021222324public class CommonEncoder extends MessageToByteEncoder &#123; private static final int MAGIC_NUMBER = 0xCAFEBABE; private final CommonSerializer serializer; public CommonEncoder(CommonSerializer serializer) &#123; this.serializer = serializer; &#125; @Override protected void encode(ChannelHandlerContext ctx, Object msg, ByteBuf out) throws Exception &#123; out.writeInt(MAGIC_NUMBER); if(msg instanceof RpcRequest) &#123; out.writeInt(PackageType.REQUEST_PACK.getCode()); &#125; else &#123; out.writeInt(PackageType.RESPONSE_PACK.getCode()); &#125; out.writeInt(serializer.getCode()); byte[] bytes = serializer.serialize(msg); out.writeInt(bytes.length); out.writeBytes(bytes); &#125;&#125; CommonEncoder 继承了MessageToByteEncoder 类，见名知义，就是把 Message（实际要发送的对象）转化成 Byte 数组。CommonEncoder 的工作很简单，就是把 RpcRequest 或者 RpcResponse 包装成协议包。 根据上面提到的协议格式，将各个字段写到管道里就可以了，这里serializer.getCode() 获取序列化器的编号，之后使用传入的序列化器将请求或响应包序列化为字节数组写入管道即可。 而 CommonDecoder 的工作就更简单了： 1234567891011121314151617181920212223242526272829303132333435public class CommonDecoder extends ReplayingDecoder &#123; private static final Logger logger = LoggerFactory.getLogger(CommonDecoder.class); private static final int MAGIC_NUMBER = 0xCAFEBABE; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; int magic = in.readInt(); if(magic != MAGIC_NUMBER) &#123; logger.error(&quot;不识别的协议包: &#123;&#125;&quot;, magic); throw new RpcException(RpcError.UNKNOWN_PROTOCOL); &#125; int packageCode = in.readInt(); Class&lt;?&gt; packageClass; if(packageCode == PackageType.REQUEST_PACK.getCode()) &#123; packageClass = RpcRequest.class; &#125; else if(packageCode == PackageType.RESPONSE_PACK.getCode()) &#123; packageClass = RpcResponse.class; &#125; else &#123; logger.error(&quot;不识别的数据包: &#123;&#125;&quot;, packageCode); throw new RpcException(RpcError.UNKNOWN_PACKAGE_TYPE); &#125; int serializerCode = in.readInt(); CommonSerializer serializer = CommonSerializer.getByCode(serializerCode); if(serializer == null) &#123; logger.error(&quot;不识别的反序列化器: &#123;&#125;&quot;, serializerCode); throw new RpcException(RpcError.UNKNOWN_SERIALIZER); &#125; int length = in.readInt(); byte[] bytes = new byte[length]; in.readBytes(bytes); Object obj = serializer.deserialize(bytes, packageClass); out.add(obj); &#125;&#125; CommonDecoder 继承自 ReplayingDecoder ，与 MessageToByteEncoder 相反，它用于将收到的字节序列还原为实际对象。主要就是一些字段的校验，比较重要的就是取出序列化器的编号，以获得正确的反序列化方式，并且读入 length 字段来确定数据包的长度（防止粘包），最后读入正确大小的字节数组，反序列化成对应的对象。 3.3、序列化接口序列化器接口（CommonSerializer）如下： 1234567891011121314151617public interface CommonSerializer &#123; byte[] serialize(Object obj); Object deserialize(byte[] bytes, Class&lt;?&gt; clazz); int getCode(); static CommonSerializer getByCode(int code) &#123; switch (code) &#123; case 1: return new JsonSerializer(); default: return null; &#125; &#125;&#125; 主要就是四个方法，序列化，反序列化，获得该序列化器的编号，以及根据编号获取序列化器，这里我已经写了一个示例的 JSON 序列化器，Kryo 序列化器会在后面讲解。 作为一个比较简单的例子，我写了一个 JSON 的序列化器： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class JsonSerializer implements CommonSerializer &#123; private static final Logger logger = LoggerFactory.getLogger(JsonSerializer.class); private ObjectMapper objectMapper = new ObjectMapper(); @Override public byte[] serialize(Object obj) &#123; try &#123; return objectMapper.writeValueAsBytes(obj); &#125; catch (JsonProcessingException e) &#123; logger.error(&quot;序列化时有错误发生: &#123;&#125;&quot;, e.getMessage()); e.printStackTrace(); return null; &#125; &#125; @Override public Object deserialize(byte[] bytes, Class&lt;?&gt; clazz) &#123; try &#123; Object obj = objectMapper.readValue(bytes, clazz); if(obj instanceof RpcRequest) &#123; obj = handleRequest(obj); &#125; return obj; &#125; catch (IOException e) &#123; logger.error(&quot;反序列化时有错误发生: &#123;&#125;&quot;, e.getMessage()); e.printStackTrace(); return null; &#125; &#125; /* 这里由于使用JSON序列化和反序列化Object数组，无法保证反序列化后仍然为原实例类型 需要重新判断处理 */ private Object handleRequest(Object obj) throws IOException &#123; RpcRequest rpcRequest = (RpcRequest) obj; for(int i = 0; i &lt; rpcRequest.getParamTypes().length; i ++) &#123; Class&lt;?&gt; clazz = rpcRequest.getParamTypes()[i]; if(!clazz.isAssignableFrom(rpcRequest.getParameters()[i].getClass())) &#123; byte[] bytes = objectMapper.writeValueAsBytes(rpcRequest.getParameters()[i]); rpcRequest.getParameters()[i] = objectMapper.readValue(bytes, clazz); &#125; &#125; return rpcRequest; &#125; @Override public int getCode() &#123; return SerializerCode.valueOf(&quot;JSON&quot;).getCode(); &#125;&#125; JSON 序列化工具我使用的是 Jackson，在 pom.xml 中添加依赖即可。序列化和反序列化都比较循规蹈矩，把对象翻译成字节数组，和根据字节数组和 Class 反序列化成对象。这里有一个需要注意的点，就是在 RpcRequest 反序列化时，由于其中有一个字段是 Object 数组，在反序列化时序列化器会根据字段类型进行反序列化，而 Object 就是一个十分模糊的类型，会出现反序列化失败的现象，这时就需要 RpcRequest 中的另一个字段 ParamTypes 来获取到 Object 数组中的每个实例的实际类，辅助反序列化，这就是 handleRequest() 方法的作用。 上面提到的这种情况不会在其他序列化方式中出现，因为其他序列化方式是转换成字节数组，会记录对象的信息，而 JSON 方式本质上只是转换成 JSON 字符串，会丢失对象的类型信息。 3.4、NettyServerHandler 和 NettyClientHandlerNettyServerHandler 和 NettyClientHandler 都分别位于服务器端和客户端责任链的尾部，直接和 RpcServer 对象或 RpcClient 对象打交道，而无需关心字节序列的情况。 NettyServerhandler 用于接收 RpcRequest，并且执行调用，将调用结果返回封装成 RpcResponse 发送出去。 123456789101112131415161718192021222324252627282930313233public class NettyServerHandler extends SimpleChannelInboundHandler&lt;RpcRequest&gt; &#123; private static final Logger logger = LoggerFactory.getLogger(NettyServerHandler.class); private static RequestHandler requestHandler; private static ServiceRegistry serviceRegistry; static &#123; requestHandler = new RequestHandler(); serviceRegistry = new DefaultServiceRegistry(); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, RpcRequest msg) throws Exception &#123; try &#123; logger.info(&quot;服务器接收到请求: &#123;&#125;&quot;, msg); String interfaceName = msg.getInterfaceName(); Object service = serviceRegistry.getService(interfaceName); Object result = requestHandler.handle(msg, service); ChannelFuture future = ctx.writeAndFlush(RpcResponse.success(result)); future.addListener(ChannelFutureListener.CLOSE); &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; logger.error(&quot;处理过程调用时有错误发生:&quot;); cause.printStackTrace(); ctx.close(); &#125;&#125; 处理方式和 Socket 中的逻辑基本一致，不做讲解。 NettyClientHandler 1234567891011121314151617181920212223public class NettyClientHandler extends SimpleChannelInboundHandler&lt;RpcResponse&gt; &#123; private static final Logger logger = LoggerFactory.getLogger(NettyClientHandler.class); @Override protected void channelRead0(ChannelHandlerContext ctx, RpcResponse msg) throws Exception &#123; try &#123; logger.info(String.format(&quot;客户端接收到消息: %s&quot;, msg)); AttributeKey&lt;RpcResponse&gt; key = AttributeKey.valueOf(&quot;rpcResponse&quot;); ctx.channel().attr(key).set(msg); ctx.channel().close(); &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; logger.error(&quot;过程调用时有错误发生:&quot;); cause.printStackTrace(); ctx.close(); &#125;&#125; 这里只需要处理收到的消息，即 RpcResponse 对象，由于前面已经有解码器解码了，这里就直接将返回的结果放入 ctx 中即可。 3.5、测试这里我们主要测试 Netty 方式。 NettyTestServer 如下： 1234567891011public class NettyTestServer &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); ServiceRegistry registry = new DefaultServiceRegistry(); registry.register(helloService); NettyServer server = new NettyServer(); server.start(9999); &#125;&#125; NettyTestClient如下： 123456789101112public class NettyTestClient &#123; public static void main(String[] args) &#123; RpcClient client = new NettyClient(&quot;127.0.0.1&quot;, 9999); RpcClientProxy rpcClientProxy = new RpcClientProxy(client); HelloService helloService = rpcClientProxy.getProxy(HelloService.class); HelloObject object = new HelloObject(12, &quot;This is a message&quot;); String res = helloService.hello(object); System.out.println(res); &#125;&#125; 注意这里 RpcClientProxy 通过传入不同的 Client（SocketClient、NettyClient）来切换客户端不同的发送方式。 执行后可以获得与之前类似的结果。 4、Kryo序列化上一节我们实现了一个通用的序列化框架，使得序列化方式具有了较高的扩展性，并且实现了一个基于 JSON 的序列化器。 但是，我们也提到过，这个基于 JSON 的序列化器有一个毛病，就是在某个类的属性反序列化时，如果属性声明为 Object 的，就会造成反序列化出错，通常会把 Object 属性直接反序列化成 String 类型，就需要其他参数辅助序列化。并且，JSON 序列化器是基于字符串（JSON 串）的，占用空间较大且速度较慢。 另外，我们在用过的RPC通信框架中，很少会发现使用JDK提供的序列化，主要是因为JDK默认的序列化存在着如下一些缺陷：无法跨语言、易被攻击、序列化后的流太大、序列化性能太差等。 这一节我们就来实现一个基于 Kryo 的序列化器。那么，什么是 Kryo？ Kryo 是一个快速高效的 Java 对象序列化框架，主要特点是高性能、高效和易用。最重要的两个特点，一是基于字节的序列化，对空间利用率较高，在网络传输时可以减小体积；二是序列化时记录属性对象的类型信息，这样在反序列化时就不会出现之前的问题了。 4.1、实现接口首先添加 kryo 的依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.2&lt;/version&gt;&lt;/dependency&gt; 我们在上一节定义了一个通用的序列化接口： 12345678910111213141516171819public interface CommonSerializer &#123; byte[] serialize(Object obj); Object deserialize(byte[] bytes, Class&lt;?&gt; clazz); int getCode(); static CommonSerializer getByCode(int code) &#123; switch (code) &#123; case 0: return new KryoSerializer(); case 1: return new JsonSerializer(); default: return null; &#125; &#125;&#125; 根据接口，我们的主要任务就是实现其中的主要两个方法，serialize() 和 deserialize() ，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class KryoSerializer implements CommonSerializer &#123; private static final Logger logger = LoggerFactory.getLogger(KryoSerializer.class); private static final ThreadLocal&lt;Kryo&gt; kryoThreadLocal = ThreadLocal.withInitial(() -&gt; &#123; Kryo kryo = new Kryo(); kryo.register(RpcResponse.class); kryo.register(RpcRequest.class); kryo.setReferences(true); kryo.setRegistrationRequired(false); return kryo; &#125;); @Override public byte[] serialize(Object obj) &#123; try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream))&#123; Kryo kryo = kryoThreadLocal.get(); kryo.writeObject(output, obj); kryoThreadLocal.remove(); return output.toBytes(); &#125; catch (Exception e) &#123; logger.error(&quot;序列化时有错误发生:&quot;, e); throw new SerializeException(&quot;序列化时有错误发生&quot;); &#125; &#125; @Override public Object deserialize(byte[] bytes, Class&lt;?&gt; clazz) &#123; try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream)) &#123; Kryo kryo = kryoThreadLocal.get(); Object o = kryo.readObject(input, clazz); kryoThreadLocal.remove(); return o; &#125; catch (Exception e) &#123; logger.error(&quot;反序列化时有错误发生:&quot;, e); throw new SerializeException(&quot;反序列化时有错误发生&quot;); &#125; &#125; @Override public int getCode() &#123; return SerializerCode.valueOf(&quot;KRYO&quot;).getCode(); &#125;&#125; 这里 Kryo 可能存在线程安全问题，文档上是推荐放在 ThreadLocal 里，一个线程一个 Kryo。在序列化时，先创建一个 Output 对象（Kryo 框架的概念），接着使用 writeObject 方法将对象写入 Output 中，最后调用 Output 对象的 toByte() 方法即可获得对象的字节数组。反序列化则是从 Input 对象中直接 readObject，这里只需要传入对象的类型，而不需要具体传入每一个属性的类型信息。 最后 getCode 方法中事实上是把序列化的编号写在一个枚举类 SerializerCode 里了： 12345public enum SerializerCode &#123; KRYO(0), JSON(1); private final int code;&#125; 4.2、替换序列化器并测试我们只需要把 NettyServer 和 NettyClient 责任链中的 CommonEncoder 传入的参数改成 KryoSerializer 即可使用 Kryo 序列化。 12- pipeline.addLast(new CommonEncoder(new JsonSerializer()));+ pipeline.addLast(new CommonEncoder(new KryoSerializer())); 5、基于 Nacos 的服务器注册与发现我们目前实现的框架看起来工作的还不错，但是有一个问题：我们的服务端地址是固化在代码中的，也就是说，对于一个客户端，它只会去寻找那么一个服务提供者，如果这个提供者挂了或者换了地址，那就没有办法了。 在分布式架构中，有一个重要的组件，就是服务注册中心，它用于保存多个服务提供者的信息，每个服务提供者在启动时都需要向注册中心注册自己所拥有的服务。这样客户端在发起 RPC 时，就可以直接去向注册中心请求服务提供者的信息，如果拿来的这个挂了，还可以重新请求，并且在这种情况下可以很方便地实现负载均衡。 常见的注册中心有 Eureka、Zookeeper 和 Nacos。 获得 Nacos Nacos 是阿里开发的一款服务注册中心，在 SpringCloud Alibaba 逐步替代原始的 SpringCloud 的过程中，Nacos 逐步走红，所以我们就是用 Nacos 作为我们的注册中心。 下载解压的过程略过。注意 Nacos 是依赖数据库的，所以我们需要在配置文件中配置 Mysql 的信息。 为了简单，我们先以单机模式运行： 1sh startup.sh -m standalone 启动后可以访问 Nacos 的web UI，地址 http://127.0.0.1:8848/nacos/index.html 默认的用户名和密码都是 nacos 5.1、在项目中使用 Nacos引入 nacos-client 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.nacos&lt;/groupId&gt; &lt;artifactId&gt;nacos-client&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt; 这里我们修正之前的概念，第二节把本地保存服务的类称为 ServiceRegistry，现在更改为 ServiceProvider，而 ServiceRegistry 作为远程注册表（Nacos）使用，对应的类名也有修改。 这里我们实现一个接口 ServiceRegistry： 1234public interface ServiceRegistry &#123; void register(String serviceName, InetSocketAddress inetSocketAddress); InetSocketAddress lookupService(String serviceName);&#125; 两个方法很好理解，register 方法将服务的名称和地址注册进服务注册中心，lookupService 方法则是根据服务名称从注册中心获取到一个服务提供者的地址。 接口有了，我们就可以写实现类了，我们实现一个 Nacos 作为注册中心的实现类：NacosServiceRegistry，我们也可以使用 ZooKeeper 作为注册中心，实现接口就可以 1234567891011121314151617181920212223242526272829303132333435363738public class NacosServiceRegistry implements ServiceRegistry &#123; private static final Logger logger = LoggerFactory.getLogger(NacosServiceRegistry.class); private static final String SERVER_ADDR = &quot;127.0.0.1:8848&quot;; private static final NamingService namingService; static &#123; try &#123; namingService = NamingFactory.createNamingService(SERVER_ADDR); &#125; catch (NacosException e) &#123; logger.error(&quot;连接到Nacos时有错误发生: &quot;, e); throw new RpcException(RpcError.FAILED_TO_CONNECT_TO_SERVICE_REGISTRY); &#125;m,. &#125; @Override public void register(String serviceName, InetSocketAddress inetSocketAddress) &#123; try &#123; namingService.registerInstance(serviceName, inetSocketAddress.getHostName(), inetSocketAddress.getPort()); &#125; catch (NacosException e) &#123; logger.error(&quot;注册服务时有错误发生:&quot;, e); throw new RpcException(RpcError.REGISTER_SERVICE_FAILED); &#125; &#125; @Override public InetSocketAddress lookupService(String serviceName) &#123; try &#123; List&lt;Instance&gt; instances = namingService.getAllInstances(serviceName); Instance instance = instances.get(0); return new InetSocketAddress(instance.getIp(), instance.getPort()); &#125; catch (NacosException e) &#123; logger.error(&quot;获取服务时有错误发生:&quot;, e); &#125; return null; &#125;&#125; Nacos 的使用很简单，通过 NamingFactory 创建 NamingService 连接 Nacos（连接的时候没有找到修改用户名密码的方式……是不需要吗），连接的过程写在了静态代码块中，在类加载时自动连接。namingService 提供了两个很方便的接口，registerInstance 和 getAllInstances 方法，前者可以直接向 Nacos 注册服务，后者可以获得提供某个服务的所有提供者的列表。所以接口的这两个方法只需要包装一下就好了。 在 lookupService 方法中，通过 getAllInstance 获取到某个服务的所有提供者列表后，需要选择一个，这里就涉及了负载均衡策略，这里我们先选择第 0 个，后面某节会详细讲解负载均衡。 5.2、注册服务我们修改 RpcServer 接口，新增一个方法 publishService，用于向 Nacos 注册服务： 1&lt;T&gt; void publishService(Object service, Class&lt;T&gt; serviceClass); 接着只需要实现这个方法即可，以 NettyServer 的实现为例，NettyServer 在创建时需要创建一个 ServiceRegistry 了： 123456public NettyServer(String host, int port) &#123; this.host = host; this.port = port; serviceRegistry = new NacosServiceRegistry(); serviceProvider = new ServiceProviderImpl();&#125; 接着实现 publishService 方法即可： 123456789public &lt;T&gt; void publishService(Object service, Class&lt;T&gt; serviceClass) &#123; if(serializer == null) &#123; logger.error(&quot;未设置序列化器&quot;); throw new RpcException(RpcError.SERIALIZER_NOT_FOUND); &#125; serviceProvider.addServiceProvider(service); serviceRegistry.register(serviceClass.getCanonicalName(), new InetSocketAddress(host, port)); start();&#125; publishService 需要将服务保存在本地的注册表，同时注册到 Nacos 上。我这里的实现是注册完一个服务后直接调用 start() 方法，这是个不太好的实现……导致一个服务端只能注册一个服务，之后可以多注册几个然后再手动调用 start() 方法。 5.3、发现服务客户端的修改就更简单了，以 NettyClient 为例，在过去创建 NettyClient 时，需要传入 host 和 port，现在这个 host 和 port 是通过 Nacos 获取的，sendRequest 修改如下： 12345678910public Object sendRequest(RpcRequest rpcRequest) &#123; if(serializer == null) &#123; logger.error(&quot;未设置序列化器&quot;); throw new RpcException(RpcError.SERIALIZER_NOT_FOUND); &#125; AtomicReference&lt;Object&gt; result = new AtomicReference&lt;&gt;(null); try &#123; InetSocketAddress inetSocketAddress = serviceRegistry.lookupService(rpcRequest.getInterfaceName()); Channel channel = ChannelProvider.get(inetSocketAddress, serializer);... 重点是最后两句，过去是直接使用传入的 host 和 port 直接构造 channel，现在是首先从 ServiceRegistry 中获取到服务的地址和端口，再构造。 5.4、测试NettyTestClient 如下： 1234567891011public class NettyTestClient &#123; public static void main(String[] args) &#123; RpcClient client = new NettyClient(); client.setSerializer(new ProtobufSerializer()); RpcClientProxy rpcClientProxy = new RpcClientProxy(client); HelloService helloService = rpcClientProxy.getProxy(HelloService.class); HelloObject object = new HelloObject(12, &quot;This is a message&quot;); String res = helloService.hello(object); System.out.println(res); &#125;&#125; 构造 RpcClient 时不再需要传入地址和端口。 NettyTestServer 如下： 12345678public class NettyTestServer &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); NettyServer server = new NettyServer(&quot;127.0.0.1&quot;, 9999); server.setSerializer(new ProtobufSerializer()); server.publishService(helloService, HelloService.class); &#125;&#125; 我这里是把 start 写在了 publishService 中，实际应当分离，否则只能注册一个服务。 分别启动，可以看到和之前相同的结果。 这里如果通过修改不同的端口，启动两个服务的话，会看到即使客户端多次调用，也只是由同一个服务端提供服务，这是因为在 NacosServiceRegistry 中，我们直接选择了服务列表的第 0 个，这个会在之后讲解负载均衡时作出修改。 6、自动注销服务和负载均衡策略6.1、自动注销服务上一节我们实现了服务的自动注册和发现，但是有些细心的同学就可能会发现，如果你启动完成服务端后把服务端给关闭了，并不会自动地注销 Nacos 中对应的服务信息，这样就导致了当客户端再次向 Nacos 请求服务时，会获取到已经关闭的服务端信息，最终就有可能因为连接不到服务器而调用失败。 那么我们就需要一种办法，在服务端关闭之前自动向 Nacos 注销服务。但是有一个问题，我们不知道什么时候服务器会关闭，也就不知道这个方法调用的时机，就没有办法手工去调用。这时，我们就需要钩子。 钩子是什么呢？是在某些事件发生后自动去调用的方法。那么我们只需要把注销服务的方法写到关闭系统的钩子方法里就行了。 首先先写向 Nacos 注销所有服务的方法，这部分被放在了 NacosUtils 中作为一个静态方法，NacosUtils 是一个 Nacos 相关的工具类： 123456789101112131415public static void clearRegistry() &#123; if(!serviceNames.isEmpty() &amp;&amp; address != null) &#123; String host = address.getHostName(); int port = address.getPort(); Iterator&lt;String&gt; iterator = serviceNames.iterator(); while(iterator.hasNext()) &#123; String serviceName = iterator.next(); try &#123; namingService.deregisterInstance(serviceName, host, port); &#125; catch (NacosException e) &#123; logger.error(&quot;注销服务 &#123;&#125; 失败&quot;, serviceName, e); &#125; &#125; &#125;&#125; 所有的服务名称都被存储在 NacosUtils 类中的 serviceNames 中，在注销时只需要用迭代器迭代所有服务名，调用 deregisterInstance 即可。 接着就是钩子了，新建一个类，ShutdownHook： 1234567891011121314151617181920public class ShutdownHook &#123; private static final Logger logger = LoggerFactory.getLogger(ShutdownHook.class); private final ExecutorService threadPool = ThreadPoolFactory.createDefaultThreadPool(&quot;shutdown-hook&quot;); private static final ShutdownHook shutdownHook = new ShutdownHook(); public static ShutdownHook getShutdownHook() &#123; return shutdownHook; &#125; public void addClearAllHook() &#123; logger.info(&quot;关闭后将自动注销所有服务&quot;); Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; NacosUtil.clearRegistry(); threadPool.shutdown(); &#125;)); &#125;&#125; 使用了单例模式创建其对象，在 addClearAllHook 中，Runtime 对象是 JVM 虚拟机的运行时环境，调用其 addShutdownHook 方法增加一个钩子函数，创建一个新线程调用 clearRegistry 方法完成注销工作。这个钩子函数会在 JVM 关闭之前被调用。 这样在 RpcServer 启动之前，只需要调用 addClearAllHook，就可以注册这个钩子了。例如在 NettyServer 中： 123 ChannelFuture future = serverBootstrap.bind(host, port).sync();+ ShutdownHook.getShutdownHook().addClearAllHook(); future.channel().closeFuture().sync(); 启动服务端后再关闭，就会发现 Nacos 中的注册信息都被注销了。 6.2、负载均衡策略负载均衡大家应该都熟悉，在上一节中客户端在 lookupService 方法中，从 Nacos 获取到的是所有提供这个服务的服务端信息列表，我们就需要从中选择一个，这便涉及到客户端侧的负载均衡策略。我们新建一个接口：LoadBalancer： 123public interface LoadBalancer &#123; Instance select(List&lt;Instance&gt; instances);&#125; 接口中的 select 方法用于从一系列 Instance 中选择一个。这里我就实现两个比较经典的算法：随机和转轮。 随机算法顾名思义，就是随机选一个，毫无技术含量： 12345678public class RandomLoadBalancer implements LoadBalancer &#123; @Override public Instance select(List&lt;Instance&gt; instances) &#123; return instances.get(new Random().nextInt(instances.size())); &#125;&#125; 而转轮算法大家也应该了解，按照顺序依次选择第一个、第二个、第三个……这里就需要一个变量来表示当前选到了第几个： 12345678910111213public class RoundRobinLoadBalancer implements LoadBalancer &#123; private int index = 0; @Override public Instance select(List&lt;Instance&gt; instances) &#123; if(index &gt;= instances.size()) &#123; index %= instances.size(); &#125; return instances.get(index++); &#125;&#125; index 就表示当前选到了第几个服务器，并且每次选择后都会自增一。 最后在 NacosServiceRegistry 中集成就可以了，这里选择外部传入的方式传入 LoadBalancer： 12345678910111213141516171819public class NacosServiceDiscovery implements ServiceDiscovery &#123; private final LoadBalancer loadBalancer; public NacosServiceDiscovery(LoadBalancer loadBalancer) &#123; if(loadBalancer == null) this.loadBalancer = new RandomLoadBalancer(); else this.loadBalancer = loadBalancer; &#125; public InetSocketAddress lookupService(String serviceName) &#123; try &#123; List&lt;Instance&gt; instances = NacosUtil.getAllInstance(serviceName); Instance instance = loadBalancer.select(instances); return new InetSocketAddress(instance.getIp(), instance.getPort()); &#125; catch (NacosException e) &#123; logger.error(&quot;获取服务时有错误发生:&quot;, e); &#125; return null; &#125;&#125; 而这个负载均衡策略，也可以在创建客户端时指定，例如无参构造 NettyClient 时就用默认的策略，也可以有参构造传入策略，具体的实现留给大家。 7、服务端自动注册服务到目前为止，客户端看起来挺完美了，但是在服务端，我们却需要手动创建服务对象，并且手动进行注册，如果服务端提供了很多服务，这个操作就会变得很繁琐。本节就会介绍如何基于注解进行服务的自动注册。 本节需要一些反射知识。 7.1、定义注解首先我们需要定义两个注解：Service 和 ServiceScan： Service.java 1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface Service &#123; public String name() default &quot;&quot;;&#125; ServiceScan.java 1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface ServiceScan &#123; public String value() default &quot;&quot;;&#125; @Service 放在一个类上，标识这个类提供一个服务，@ServiceScan 放在启动的入口类上（main 方法所在的类），标识服务的扫描的包的范围。Service 注解的值定义为该服务的名称，默认值是该类的完整类名，而 ServiceScan 的值定义为扫描范围的根包，默认值为入口类所在的包，扫描时会扫描该包及其子包下所有的类，找到标记有 Service 的类，并注册。 7.2、工具类 ReflectUtil这个类是一系列工具方法，不做讲解，只说用途，感兴趣的可以研究研究具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public class ReflectUtil &#123; public static String getStackTrace() &#123; StackTraceElement[] stack = new Throwable().getStackTrace(); return stack[stack.length - 1].getClassName(); &#125; public static Set&lt;Class&lt;?&gt;&gt; getClasses(String packageName) &#123; Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); boolean recursive = true; String packageDirName = packageName.replace(&#x27;.&#x27;, &#x27;/&#x27;); Enumeration&lt;URL&gt; dirs; try &#123; dirs = Thread.currentThread().getContextClassLoader().getResources( packageDirName); // 循环迭代下去 while (dirs.hasMoreElements()) &#123; // 获取下一个元素 URL url = dirs.nextElement(); // 得到协议的名称 String protocol = url.getProtocol(); // 如果是以文件的形式保存在服务器上 if (&quot;file&quot;.equals(protocol)) &#123; // 获取包的物理路径 String filePath = URLDecoder.decode(url.getFile(), &quot;UTF-8&quot;); // 以文件的方式扫描整个包下的文件 并添加到集合中 findAndAddClassesInPackageByFile(packageName, filePath, recursive, classes); &#125; else if (&quot;jar&quot;.equals(protocol)) &#123; // 如果是jar包文件 // 定义一个JarFile JarFile jar; try &#123; // 获取jar jar = ((JarURLConnection) url.openConnection()) .getJarFile(); // 从此jar包 得到一个枚举类 Enumeration&lt;JarEntry&gt; entries = jar.entries(); // 同样的进行循环迭代 while (entries.hasMoreElements()) &#123; // 获取jar里的一个实体 可以是目录 和一些jar包里的其他文件 如META-INF等文件 JarEntry entry = entries.nextElement(); String name = entry.getName(); // 如果是以/开头的 if (name.charAt(0) == &#x27;/&#x27;) &#123; // 获取后面的字符串 name = name.substring(1); &#125; // 如果前半部分和定义的包名相同 if (name.startsWith(packageDirName)) &#123; int idx = name.lastIndexOf(&#x27;/&#x27;); // 如果以&quot;/&quot;结尾 是一个包 if (idx != -1) &#123; // 获取包名 把&quot;/&quot;替换成&quot;.&quot; packageName = name.substring(0, idx) .replace(&#x27;/&#x27;, &#x27;.&#x27;); &#125; // 如果可以迭代下去 并且是一个包 if ((idx != -1) || recursive) &#123; // 如果是一个.class文件 而且不是目录 if (name.endsWith(&quot;.class&quot;) &amp;&amp; !entry.isDirectory()) &#123; // 去掉后面的&quot;.class&quot; 获取真正的类名 String className = name.substring( packageName.length() + 1, name .length() - 6); try &#123; // 添加到classes classes.add(Class .forName(packageName + &#x27;.&#x27; + className)); &#125; catch (ClassNotFoundException e) &#123; // log // .error(&quot;添加用户自定义视图类错误 找不到此类的.class文件&quot;); e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException e) &#123; // log.error(&quot;在扫描用户定义视图时从jar包获取文件出错&quot;); e.printStackTrace(); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return classes; &#125; private static void findAndAddClassesInPackageByFile(String packageName, String packagePath, final boolean recursive, Set&lt;Class&lt;?&gt;&gt; classes) &#123; // 获取此包的目录 建立一个File File dir = new File(packagePath); // 如果不存在或者 也不是目录就直接返回 if (!dir.exists() || !dir.isDirectory()) &#123; // log.warn(&quot;用户定义包名 &quot; + packageName + &quot; 下没有任何文件&quot;); return; &#125; // 如果存在 就获取包下的所有文件 包括目录 File[] dirfiles = dir.listFiles(new FileFilter() &#123; // 自定义过滤规则 如果可以循环(包含子目录) 或则是以.class结尾的文件(编译好的java类文件) public boolean accept(File file) &#123; return (recursive &amp;&amp; file.isDirectory()) || (file.getName().endsWith(&quot;.class&quot;)); &#125; &#125;); // 循环所有文件 for (File file : dirfiles) &#123; // 如果是目录 则继续扫描 if (file.isDirectory()) &#123; findAndAddClassesInPackageByFile(packageName + &quot;.&quot; + file.getName(), file.getAbsolutePath(), recursive, classes); &#125; else &#123; // 如果是java类文件 去掉后面的.class 只留下类名 String className = file.getName().substring(0, file.getName().length() - 6); try &#123; // 添加到集合中去 //classes.add(Class.forName(packageName + &#x27;.&#x27; + className)); //经过回复同学的提醒，这里用forName有一些不好，会触发static方法，没有使用classLoader的load干净 classes.add(Thread.currentThread().getContextClassLoader().loadClass(packageName + &#x27;.&#x27; + className)); &#125; catch (ClassNotFoundException e) &#123; // log.error(&quot;添加用户自定义视图类错误 找不到此类的.class文件&quot;); e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 主要就是 getClasses 方法，传入一个包名，用于扫描该包及其子包下所有的类，并将其 Class 对象放入一个 Set 中返回。 7.3、扫描服务由于扫描服务这一步是一个比较公共的方法，无论是 Socket 还是 Netty 的服务端都需要这个方法，于是我对项目做了一点重构，使用了一个抽象类 AbstractRpcServer 实现了 RpcServer 接口，而 NettyServer 和 SocketServer 继承自 AbstractRpcServer，将 scanServices 方法放在抽象类中，而 start 方法则由具体实现类来实现。 scanServices 方法如下： 123456789101112131415161718192021222324252627282930313233343536373839public void scanServices() &#123; String mainClassName = ReflectUtil.getStackTrace(); Class&lt;?&gt; startClass; try &#123; startClass = Class.forName(mainClassName); if(!startClass.isAnnotationPresent(ServiceScan.class)) &#123; logger.error(&quot;启动类缺少 @ServiceScan 注解&quot;); throw new RpcException(RpcError.SERVICE_SCAN_PACKAGE_NOT_FOUND); &#125; &#125; catch (ClassNotFoundException e) &#123; logger.error(&quot;出现未知错误&quot;); throw new RpcException(RpcError.UNKNOWN_ERROR); &#125; String basePackage = startClass.getAnnotation(ServiceScan.class).value(); if(&quot;&quot;.equals(basePackage)) &#123; basePackage = mainClassName.substring(0, mainClassName.lastIndexOf(&quot;.&quot;)); &#125; Set&lt;Class&lt;?&gt;&gt; classSet = ReflectUtil.getClasses(basePackage); for(Class&lt;?&gt; clazz : classSet) &#123; if(clazz.isAnnotationPresent(Service.class)) &#123; String serviceName = clazz.getAnnotation(Service.class).name(); Object obj; try &#123; obj = clazz.newInstance(); &#125; catch (InstantiationException | IllegalAccessException e) &#123; logger.error(&quot;创建 &quot; + clazz + &quot; 时有错误发生&quot;); continue; &#125; if(&quot;&quot;.equals(serviceName)) &#123; Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; oneInterface: interfaces)&#123; publishService(obj, oneInterface.getCanonicalName()); &#125; &#125; else &#123; publishService(obj, serviceName); &#125; &#125; &#125;&#125; 我们首先需要获得要扫描的包的范围，就需要获取到 ServiceScan 注解的值，而我们前面说过，这个注解是加在启动类上的，那么，我们怎么知道启动类是哪一个呢？答案是通过调用栈。方法的调用和返回是通过方法调用栈来实现的，当调用一个方法时，该方法入栈，该方法返回时，该方法出站，控制回到栈顶的方法。那么，main 方法一定位于调用栈的最底端，在 ReflectUtils 中，我写了一个 getStackTrace 方法（名字起得不好），用于获取 main 所在的类。通过 Class 对象的 isAnnotationPresent 方法来判断该类是否有 ServiceScan 注解。如果有，通过startClass.getAnnotation(ServiceScan.class).value(); 获取注解的值。 当获得扫描的范围后，就可以通过ReflectUtil.getClasses(basePackage) 获取到所有的 Class 了，逐个判断是否有 Service 注解，如果有的话，通过反射创建该对象，并且调用 publishService 注册即可。 7.4、开启自动注册并测试以 NettyServer 为例，在 NettyServer 的构造方法最后，调用 scanServices 方法，即可自动注册所有服务： 12345678public NettyServer(String host, int port, Integer serializer) &#123; this.host = host; this.port = port; serviceRegistry = new NacosServiceRegistry(); serviceProvider = new ServiceProviderImpl(); this.serializer = CommonSerializer.getByCode(serializer); scanServices(); &#125; 不要忘了在 HelloServiceImpl 类上加上 @service 注解： 1234567@Servicepublic class HelloServiceImpl implements HelloService &#123; @Override public String hello(String name) &#123; return &quot;Hello, &quot; + name; &#125;&#125; 并且在服务器启动类上加上注解： 1234567@ServiceScanpublic class NettyTestServer &#123; public static void main(String[] args) &#123; NettyServer server = new NettyServer(&quot;127.0.0.1&quot;, 9999, CommonSerializer.PROTOBUF_SERIALIZER); server.start(); &#125;&#125; 直接使用启动类所在的包作为扫描根包。 启动类变得无比简洁！启动后应该能看到和之前相同的结果。","categories":["Spring Cloud"]},{"title":"恭王府花园游记","path":"/2022/11/12/恭王府花园游记/","content":"阳光很好的周六下午，去了恭王府，相比故宫一片金碧辉煌井然有序的宫殿建筑群，颐和园一片水边山边的园林图景，我更喜欢这一处人家院落里营造的园林风景，一如我一直喜爱的苏州园林！真的很喜欢恭王府的后花园了！以后要和喜欢的朋友一起去，在春天花开的时候，在夏天绿树浓荫的时候……希望会有吧。 在西二旗地铁站的时候，也许是周末，北京的地铁站永远不缺熙攘的人群，在一辆地铁驶来后，站台人流散去，呈现一片寂寥之感，我的眼前飘来一朵蒲公英，我开心的走向前用手接起，白色的绒球下面是一粒黑色羽片状的种子，想象它是冬奥会场飘出来的一朵，忽然很开心，把它小心翼翼的揣兜里。 是在北海北下的，出站便能看见文物研究所，往三座桥胡同北走，便到了恭王府的所在，藏在胡同里的恭王府是寂静的，从正门进去，会穿过很多垂花门和庭院，院子里种满松柏，两侧亦有屋舍，设了一些展览，是高山流水漆器展和山西忻州剪纸作品展览。 后院是一个很大的花园，杂以山石草木，屋舍绮花，是我喜欢的所在了！ 园子里堆了许多假山，便有了高低错落之感，人行其中，攀石寻径，若隐若现。假山上砌了亭子，坐在亭子上可一览周遭景致，遇友人来，对弈于此，秋爽高风，岂不快乐！另有一处假山，两边是山径游廊，顺游廊而上，可到一高处，名“邀月亭”，若逢日落西山，新月初霁，可揽月于此，我歌月徘徊，我舞影零乱。 园子西侧有一处水池，池中有湖心亭，湖畔有是一面有实墙的走廊，走廊穿以漏景窗，可坐倚栏杆上，看绿波游鱼。邀月亭假山后面，又有一处山房，名蝠厅，柱子上画成了竹子的形状，门窗尽绿，和旁边簇拥的竹子相映成趣，很有红楼梦里潇湘馆的感觉！我恍惚看见书中的“黛玉”正倚在廊边。 东面是梧桐院和听雨轩，先说梧桐院，果真是一处隐藏起来的院子，因为被假山掩遮，闲静时可偃卧此间。听雨轩，适合一个人的时候，逢着雨落，听着屋外雨打蕉叶三两声，细细的让人落入空茫…… 听雨轩过去是一处圆拱门，种满了竹子，有种曲径通幽的感觉，另有一处所在，名“怡神所”，四面屋绕，中间植以绿竹，一走进便有遮天蔽日的浓荫，适合夏日消暑，清寂之极，稍有一点风便能听见竹叶沙沙作响，果然闻之叫人心旷神怡！园中诸多景致，想见四时各异，待日后一一赏玩。 回前门经过佛楼，佛楼前有一排转经筒，来往的游人把它拨动。我知道西藏那边是有的，藏人对神山圣水的拜谒使我敬重。我也去拨动那转经筒，希望等到属于我的好运！ ] ] ] ] ] ]","categories":["山水行"]},{"path":"/explore/index.html","content":"自序 风轻轻的吹拂着，初秋的阳光失了炽热的温度，明亮且温暖的洒向人间，一草一木，一径一墙。我沿着鹅卵石铺砌的林间小路，走向木椅而坐，我喜欢这样的白日，心空空，极尽清寂，高树在半空哗啦啦的摇曳，地面的光影婆娑起舞。远处行人自顾行走，遥遥看见我，只当我于此小憩，全无在意。经过了一个夏天的麦冬草颜色转为深碧，三三两两开出一茎茎紫红色的小花，阳光下梧桐的叶片脉络可见。我闭目凝神起来，在没有五色的世界，树声、鸟声、远处的市声愈发明朗起来，前尘往事，几多悲欢，几多周折，历历而来。 遥想东晋的王右军兰亭曲水流觞之后，酒兴未阑，泼墨写下“夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外”，诚然如斯，人生天地，俯仰之间，不知何事镇日纷纷，萦我怀抱，半梦半醒。 阅读、文字、摄影、古典文化迷、千山万水身、心漪千重、卉木萋萋、烟火人间、清欢玄想 遥想 2023 年 10 月 21 日莫听穿林打叶声，何妨吟啸且徐行。2023 年 10 月 22 日昔者庄周梦为蝴蝶，栩栩然蝴蝶也，蘧然觉，则周也，不知周与蝴蝶。余今亦梦蝶，游戏荚豆花处。 找到我 小红书https://www.xiaohongshu.com/user/profile/5dbc56e20000000001005c42 Bilibilihttps://space.bilibili.com/323246904 豆瓣https://www.douban.com/people/144948848/?_i=6300582lRiL1YZ 公众号：愔颂邮箱&#x6c;&#x69;&#108;&#x69;&#x73;&#104;&#97;&#110;&#x63;&#104;&#117;&#97;&#x6e;&#51;&#52;&#64;&#x31;&#54;&#51;&#46;&#99;&#x6f;&#109;"},{"title":"乐读所在","path":"/wiki/gallery/乐读所在.html","content":"此《书城十二图》册由清代邹一桂绘。全册共十二开，描绘各式山水园林中的读书寓所，依次为：梅花书屋、红杏书楼、碧梧书馆、青莲书社、密筠书舍、绿天书坞、天香书院、万松书岩、秋林书圃、迟云书阁、听泉书榭、印月书堂。每幅上并有汪由敦行书题乾隆帝御制诗。此册现藏于台北故宫博物院。 碧梧书馆 迟云书阁 红杏书楼 绿天书坞 梅花书屋 密筠书舍 青帘书社 秋林书圃 天香书院 听泉书榭 万松书院 印月书堂"},{"title":"四方烟火中的古建大观","path":"/wiki/gallery/building.html","content":"故宫圆明园, 建筑, 古建筑, 中国, 中国式建筑, 红, 屋檐, 斗拱, 树天堂的寺庙, 天坛公园, 北京, 中国, 建造, 古建筑, 天坛, 结构圆明园, 大觉寺, 石门, 红色, 中国古建筑, 建筑, 户易乐摄影, 北京紫禁城, 北京, 冬天, 古建筑, 雪, 结构, 景山公园, 中国 客家围楼中国, 福建, 客家, 少数民族, 民居, 土楼, 中国古典建筑, 中国古代建筑 闽南红砖古厝中国, 泉州, 古建筑, 寺庙, 旅行"},{"path":"/friends/index.html","content":"友圈李小沐苏末了摸鱼也免费xaoxuuBeaCoxbokebohaoyu 收藏图床站内图片在线存储书格https://www.shuge.org/ 图书书格https://www.shuge.org/ 鸠摩搜书https://www.jiumodiary.com/ 地图"},{"title":"开韶集胜记","path":"/wiki/gallery/开韶集胜记.html","content":"此《开韶集胜》册由清代董诰绘。春天是一年的美好开始。画册内容皆以 “春” 字为题，描绘美好春光，共十二开：春社延宾、春泉漱玉、春原叱犊、春林生翠、春港红酣、春畬新绿、春帆细雨、春岫归樵、春园婴戏、春溪渔乐、春坞青帘、春湖香雪。此为清嘉庆八年绘本，现藏于台北故宫博物院。 春季是四季之首。阴阳之气开始转变，万物随阳气上升而萌芽生长。春，代表着温暖、生长，也寓意一年的美好开始。 春社延宾 春泉漱玉 春林生翠 春湖香雪 春港红酣 春帆细雨 春田新绿 春坞青帘 春溪渔乐 春岫归樵 春园婴戏 春原叱犊"},{"title":"古典山水画卷","path":"/wiki/gallery/scenery.html","content":"千里江山图王希孟（宋）51.5×1191.5 厘米长卷 绢本设色故宫博物院 《千里江山图》卷无画家落款，通过卷后蔡京题跋可知，画家名为“希孟”，绘制此画时年仅十八岁，并曾得到过宋徽宗指点。画作是一幅设色浓重的“青绿山水”，即以石青、石绿等矿物质颜料为主描写山水景观的绘画技法，其发展可追溯到隋唐时代。《千里江山图》以宋人察物明理的思想表现出更加细腻的画风，全图用极其工整细腻的笔致表现“千里江山”的波澜壮阔，既有高山流水，亦有曲径通幽，江山之间还有房舍屋宇点缀其中，山间渔村、茅屋、草舍更添生活之美。学界对于《千里江山图》及有关宋徽宗朝青绿山水的研究讨论众多。有学者认为该画描绘的是江南水乡，并进一步提出认为画中的人文景观和湿地生态是北宋末年“丰亨豫大”绘画审美观的精彩展现。也有学者认为该画是出世、避世的场所展现。还有学者认为该画是道家思想的现实反映，亦有学者提出该画是以《诗经》为文本起源的宋代招隐画卷等等。 踏歌图马远（宋）193.5×111 厘米立轴 绢本设色故宫博物院 “宿雨清畿甸，朝阳丽帝城。丰年人乐业，垅上踏歌行。”临安城外，雨过天晴，一片空茫的云雾将画面分为上下两部分：上部奇峰突起，郁郁林木掩映着皇宫殿宇，为天上之景；下部有湍流巨石，麦田阡陌，垅上农人踏歌而行，为人间之景。将天上与人间巧妙并置，申发出天子与民同乐的主题。“踏歌”即用足踏节奏而作歌，是宋时在平民中甚为流行的娱乐形式，绘人们踏歌之欢乐与满足，亦是对理想太平盛世的映射。此作采用边角式构图，又通过精心布局使画面达到视觉上的平衡。凝重的卧石与峻峭的秀峰以大斧劈皴绘出，其间又有柔和的水纹、舒展的柳枝与细软的稻叶，刚柔并济，动静相兼。 四景山水图刘松年（宋）41.3×69 厘米&#x2F;页页改卷 绢本设色故宫博物院 临安是南宋都城所在，当时的士绅贵族多在西湖畔建造庭园别墅，嬉游享乐，生活优裕闲适。此卷分为四段，分别描绘私家园林的四时之景，以及人们在其中的四季活动。春日杨柳葱茏，主人似欲出门踏青，僮仆备马携担，在门外等候；夏日花木丛生，主人于庭中静坐，享受阵阵清凉；秋日霜叶渐染，主人端坐塌床之上，侍童汲水煮茶；冬日素裹银装，桥上的主人骑驴张伞，正欲外出游乐访友。此作画风清润古朴，山石以小斧劈皴绘之，点景人物细腻生动，极富生活气息，营造出不同季节的景致与风韵；亭台楼阁使用传统的界画用笔，建筑与山水巧妙相融，为古代建筑史与园林史的研究提供了重要的图像资料。 花坞醉归图佚名（宋）23.8×25.3 厘米团扇 绢本设色上海博物馆 “犬吠水声中，桃花带露浓。”图绘山乡一隅，取恬淡安逸的乡村日常情景入画，极富生活气息。融融的春山间是宁静的溪谷，桃花灼灼，落英缤纷。两间茅屋坐落于堤岸，酒旗招展，门户洞开。一妇人挑水拾级而上，对面细犬相迎。近处的小桥上有一主一仆，主人于驴上作酩酊之态，仿佛刚在花间会友把酒言欢；侍从担挑酒食器物跟随其后，神色稍显倦怠。此图尺幅虽小，但笔墨细腻，构图饱满，只用寥寥点景人物，便展现了丰富又生动的故事情节，引人入胜。山村的春日，在混着酒香与花影的溪水中，也在醉客蹒跚的步履里。 溪山行旅图范宽（宋）206.3×103.3 厘米立轴 绢本设色台北故宫博物院 巍峨的山体几乎占满整幅画面，巨峰壁立，气势撼人。谷间一瀑倾泻而下，飞流百尺。山腰下部籍云雾留白，前景溪水奔流，一队商旅正缘溪赶路。山溪上部，一位行脚僧正转过巨石，要沿着栈桥去往旁边山头的寺庙。画家将近景放低缩小，中景远景拔高拉大，使主峰呈现出丰碑式的宏伟气势，让观者拥有如临其境的欣赏体验。当我们仰望它时，不禁悲慨于自身的渺小和短促，却也能感触到自然的宁静与深沉。山体使用大面积的雨点皴与积墨法，墨色厚重沉郁，既展现了秦岭地区独特的岩石地貌，又形成了“如行夜山”之效果，使山势更加雄强硬朗。宋画多穷款，此作右下方的树荫草叶间却隐有“范宽”二字，十分难得。 松芝群鹿图（传）牟仲甫（宋）23.9×24.3 厘米团扇 绢本设色台北故宫博物院 郊野的坡地上生有几棵苍松老树，其旁有较低矮的灵芝与翠竹，松荫下，一公一母两鹿并立，它们虽身体朝向相反，目光却凝视着同方向，仿佛被右侧的某物所惊动。两鹿线条流畅，体态浑圆，画家巧妙地捕捉到鹿蹄抬起的瞬间，表现其将行又止的姿态，尤为生动，活灵活现。与一侧以较粗重的墨线勾勒出的草木不同，两只鹿以极细的淡墨稍加绘形，在背部、脊骨处上赭色与灰色，点染白斑，其上排布极浅的细密短线，表现皮毛的质感。鹿与松一淡一深，一动一静，一密一疏，形成微妙的视觉对比，颇有意趣。 橙黄橘绿图（传）赵令穰（宋）24.2×24.9 厘米团扇 绢本设色台北故宫博物院 “一年好景君须记，最是橙黄橘绿时。”宋人善以诗意入画。江南入秋的好时节，天高气爽，景色清丽。潺潺的溪流蜿蜒在云雾迷蒙的平野，汀渚之间有三两水鸟或飞或歇。沿溪两岸，橙橘遍植，除了树木主干略用墨线勾勒，其余的叶片与果实皆用色直接点染，平铺排布，虽形状不追求精准，却别有一番朦胧柔美的意趣。青绿重色之法传至宋代，逐渐演变为小青绿，其以薄染为主，参以墨笔，气象更为淡雅，此图便是典型。远远望去，枝桠上一粒粒黄果绿实，多像漂浮着的点点星尘，它们随着流水远道而来，宛若天上的银河淌入人间。 万壑松风图李唐（宋）187.5×139.8 厘米挂轴 绢本水墨台北故宫博物院 图绘巉岩峭壁，飞瀑鸣泉，山中烟云缭绕，松林繁茂，似有清风穿过崖谷与林木，在画轴上发出簌簌声响。此图为李唐南渡临安前的画作，与北宋画院中影响极大的郭熙一派不甚相似，而更近范宽等人的风格：中景巨峰耸立，如一座巍峨丰碑，与《溪山行旅图》的构图异曲同工；山石以短条状的皴笔绘出，用笔果断迅疾，突出方直坚硬的质感，与范宽的“雨点皴”一脉相承。画面以墨为主调，又加以赭石、花青、石绿等淡色作点缀，层层渲染，使整体气象更加浑厚苍郁。因此，此图可看作是画家在崇古之风下对青绿山水的延续与更新。 月夜看潮图李嵩（宋）22.3×22 厘米团扇 绢本设色台北故宫博物院 图中绘秋夜观赏钱塘江潮的情景，高悬的明月下，由远及近的夜潮卷涌成一直线奔驰而来，简括的笔法写出成片蓝灰色的远山，配合一艘帆船展现钱塘江粼粼江涛，一排临江的楼阁台榭，四周通透无壁，空间布置精确入微，观景极佳。宋人热衷于观潮，并且八月中秋左右数日观钱塘大潮被列入到皇室的行事历中，据学者分析，画面中高台建筑应是在南宋宫中观赏大潮的最佳点“天开图画台”。 后赤壁赋图乔仲常（宋）29.5×560.4 厘米长卷 纸本水墨纳尔逊阿特金斯艺术博物馆 此长卷以苏轼《后赤壁赋》内容为本，将赋文分为九段，分别题于画上，异时同图依次描绘，是现知最早的“赤壁”题材传世之作。走笔用线简括洒脱，不事渲染，气韵清空。首段绘苏轼与两客在江边向渔人买鱼，以淡墨绘出月光下的朦胧人影，细腻生动；二段绘苏轼携酒走出家门，妻儿在其后送别；三段绘主客三人复游赤壁，对江饮酒；四段绘苏轼独自一人摄衣登山；五段绘丛林草木；六段绘山石水波；七段绘苏轼与客放舟江上，与一只仙鹤擦身而过；八段绘苏轼归家就寝，在梦中得见仙鹤所化的道士；九段绘其惊寤，却不见道士身影，全卷就此作结。此作绘制详略得当，虚实相生，构思流畅精妙，是诗画合一的典范。 鹊华秋色图此《鹊华秋色图》卷由元代赵孟頫绘。此画是元贞元年他自济南路职位南返后，为友人周密描绘其祖籍地景色之作。画卷构图简洁，在一广阔水泽坡岸秋景间，右为尖耸的华不注山，左侧是浑圆的鹊山。画境表现出恬静悠闲的田园风味。此卷现藏于台北故宫博物院。 此幅为画史上认定为文人画风式青绿设色山水。两座主峰以花青杂以石青，呈深蓝色。这与州渚的浅淡、树叶的各种深浅不一的青色，成同色调的变化；斜坡、近水边处，染赭，屋顶、树干、树叶又以红、黄、赭。笔墨与设色相辅相成，更添生趣，自然佳景秀丽宜人。 升平万国图卷此《升平万国图》卷由清代周鲲绘。此卷描绘清乾隆年间常熟湖山胜迹、街市和社会风情，南门坛上、陈家市、书院弄、虞山十八景中的昆承双塔等地标，画太平盛世，村郭连绵，各行各业欢乐的生活情形。此图为清乾隆十年所进，歌颂皇帝免天下赋税。此卷现藏于台北故宫博物院。 清乾隆十年 (1745) 六月，下谕隔年普免全国钱粮，减赋蠲税。为让皇帝清楚了解常熟的风俗景物，细绘常熟秀丽的山川景色、丰饶物产，反映出太平盛世景象。"},{"title":"古人词抄","path":"/wiki/poem/古人词抄.html","content":"此卷系作者读书所录文字思想，于时光悠悠处，细看白鸟绕南山，小池澹澹碧。 第一辑 漱玉词漱玉之名，源于李清照僦居济南处庭前漱玉泉，泉水清冽，至今汩汩。 渔家傲 天接云涛连晓雾天接云涛连晓雾。星河欲转千帆舞。仿佛梦魂归帝所。闻天语。殷勤问我归何处。 我报路长嗟日暮。学诗谩有惊人句。九万里风鹏正举。风休住。蓬舟吹取三山去。 渔家傲 雪里已知春信至雪里已知春信至。寒梅点缀琼枝腻。香脸半开娇旖旎。当庭际。玉人浴出新妆洗。 造化可能偏有意。故教明月玲珑地。共赏金尊沈绿蚁。莫辞醉。此花不与群花比。 如梦令 常记溪亭日暮（一题作酒兴） 常记溪亭日暮，沉醉不知归路。兴尽晚回舟，误入藕花深处。争渡，争渡，惊起一滩鸥鹭。 如梦令 昨夜雨疏风骤（一题作春晚） 昨夜雨疏风骤，浓睡不消残酒。试问卷帘人，却道海棠依旧。知否？知否？应是绿肥红瘦。 如梦令 谁伴明窗独坐谁伴明窗独坐，我共影儿俩个。灯尽欲眠时，影也把人抛躲。无那，无那，好个凄凉的我。 浣溪沙 绣面芙蓉一笑开绣面芙蓉一笑开。斜飞宝鸭衬香腮。眼波才动被人猜。 一面风情深有韵，半笺娇恨寄幽怀。月移花影约重来。 浣溪沙 髻子伤春慵更梳髻子伤春慵更梳。晚风庭院落梅初。淡云来往月疏疏。 玉鸭熏炉闲瑞脑，朱樱斗帐掩流苏。通犀还解辟寒无。 一剪梅 红藕香残玉簟秋红藕香残玉簟秋。轻解罗裳，独上兰舟。云中谁寄锦书来，雁字回时，月满西楼。花自飘零水自流。一种相思，两处闲愁。此情无计可消除，才下眉头，却上心头。 蝶恋花 泪湿罗衣脂粉满泪湿罗衣脂粉满，四叠阳关，唱到千千遍。人道山长山又断，萧萧微雨闻孤馆。惜别伤离方寸乱，忘了临行，酒盏深和浅。好把音书凭过雁，东莱不似蓬莱远。 鹧鸪天 暗淡轻黄体性柔暗淡轻黄体性柔，情疏迹远只香留。 何须浅碧轻红色，自是花中第一流。 梅定妒，菊应羞，画栏开处冠中秋。 骚人可煞无情思，何事当年不见收。 临江仙 庭院深深深几许欧阳公作《蝶恋花》，有“深深深几许”之句，予酷爱之。用其语作“庭院深深”数阕，其声即旧《临江仙》也。 庭院深深深几许？云窗雾阁常扃。柳梢梅萼渐分明。春归秣陵树，人老建康城。感月吟风多少事，如今老去无成。谁怜憔悴更凋零。试灯无意思，踏雪没心情。 醉花阴 薄雾浓云愁永昼薄雾浓云愁永昼，瑞脑消金兽。佳节又重阳，玉枕纱橱，半夜凉初透。 东篱把酒黄昏后，有暗香盈袖。莫道不销魂，帘卷西风，人比黄花瘦。 武陵春 风住尘香花已尽风住尘香花已尽，日晚倦梳头。物是人非事事休，欲语泪先流。 闻说双溪春尚好，也拟泛轻舟，只恐双溪舴艋舟，载不动许多愁。 声声慢 寻寻觅觅寻寻觅觅，冷冷清清，凄凄惨惨戚戚。乍暖还寒时候，最难将息。三杯两盏淡酒，怎敌他、晚来风急？雁过也，正伤心，却是旧时相识。 满地黄花堆积。憔悴损，如今有谁堪摘？守着窗儿，独自怎生得黑？梧桐更兼细雨，到黄昏、点点滴滴。这次第，怎一个愁字了得！ 清平乐 年年雪里年年雪里。常插梅花醉。挼尽梅花无好意。赢得满衣清泪。 今年海角天涯。萧萧两鬓生华。看取晚来风势，故应难看梅花。 点绛唇 蹴罢秋千蹴罢秋千，起来慵整纤纤手。露浓花瘦，薄汗轻衣透。 见客入来，袜刬金钗溜。和羞走，倚门回首，却把青梅嗅。 点绛唇 寂寞深闺寂寞深闺，柔肠一寸愁千缕。惜春春去。几点催花雨。 倚遍阑干，只是无情绪。人何处。连天衰草，望断归来路。 满庭芳 小阁藏春小阁藏春，闲窗锁昼，画堂无限深幽。篆香烧尽，日影下帘钩。手种江梅渐好，又何必、临水登楼。无人到，寂寥浑似，何逊在扬州。 从来，知韵胜，难堪雨藉，不耐风揉。更谁家横笛，吹动浓愁。莫恨香消雪减，须信道、扫迹情留。难言处，良宵淡月，疏影尚风流。 御街行 藤床纸帐朝眠起藤床纸帐朝眠起。说不尽、无佳思。沈香断续玉炉寒，伴我情怀如水。笛里三弄，梅心惊破，多少春情意。 小风疏雨萧萧地。又催下、千行泪。吹箫人去玉楼空，肠断与谁同倚。一枝折得，人间天上，没个人堪寄。 玉楼春 红酥肯放琼苞碎红酥肯放琼苞碎，探著南枝开遍未。不知酝藉几多香，但见包藏无限意。 道人憔悴春窗底，闷损阑干愁不倚。要来小酌便来休，未必明朝风不起。 永遇乐 落日熔金落日熔金，暮云合壁，人在何处。染柳烟浓，吹梅笛怨，春意知几许。元宵佳节，融和天气，次第岂无风雨。来相召、香车宝马，谢他酒朋诗侣。 中州盛日，闺门多暇，记得偏重三五。铺翠冠儿，捻金雪柳，簇带争济楚。如今憔悴，风鬟霜鬓，怕见夜间出去。不如向、帘儿底下，听人笑语。 青玉案 一年春事都来几一年春事都来几。早过了、三之二。绿暗红嫣浑可事。绿杨庭院，暖风帘幕，有个人憔悴。 买花载酒长安市。又争似家山见桃李。不枉东风吹客泪，相思难表，梦魂无据，惟有归来是。 添字丑奴儿 窗前谁种芭蕉树窗前谁种芭蕉树，阴满中庭。阴满中庭。叶叶心心，舒卷有馀清。 伤心枕上三更雨，点滴霖霪。点滴霖霪。愁损北人，不惯起来听。 摊破浣溪沙 病起萧萧两鬓华病起萧萧两鬓华。卧看残月上窗纱。豆蔻连梢煎熟水，莫分茶。 枕上诗书闲处好，门前风景雨来佳。终日向人多藉藉，木犀花。 摊破浣溪沙 揉破黄金万点轻揉破黄金万点轻，剪成碧玉叶层层。风度精神如彦辅，太鲜明。 梅蕊重重何俗甚，丁香千结苦粗生。熏透愁人千里梦，却无情。 第二辑 小山词黄庭坚曰：淮海、小山，真古之伤心人也。余读其词，深情娟丽，真谓痴也。 临江仙 梦后楼台高锁梦后楼台高锁，酒醒帘幕低垂。去年春恨却来时。落花人独立，微雨燕双飞。 记得小苹初见，两重心字罗衣。琵琶弦上说相思。当时明月在，曾照彩云归。 鹧鸪天 彩袖殷勤捧玉钟彩袖殷勤捧玉钟。当年拚却醉颜红。舞低杨柳楼心月，歌尽桃花扇影风。 从别后，忆相逢。几回魂梦与君同。今宵剩把银红照，犹恐相逢是梦中。 生查子 关山魂梦长关山魂梦长，鱼雁音尘少。两鬓可怜青，只为相思老。 归梦碧纱窗，说与人人道。真个别离难，不似相逢好。 清平乐 留人不住留人不住。醉解兰舟去。一棹碧涛春水路。过尽晓莺啼处。 渡头杨柳青青。枝枝叶叶离情。此后锦书休寄，画楼云雨无凭。 木兰花 初心已恨花期远初心已恨花期晚。别后相思长在眼。兰衾犹有旧时香，每到梦回珠泪满。 多应不信人肠断。几夜夜寒谁共暖。欲将恩爱结来生，只恐来生缘又短。 思远人 红叶黄花秋意晚红叶黄花秋意晚，千里念行客。飞云过尽，归鸿无信，何处寄书得。 泪弹不尽临窗滴。就砚旋研墨。渐写到别来，此情深处，红笺为无色。 长相思 长相思长相思。长相思。若问相思甚了期。除非相见时。 长相思。长相思。欲把相思说似谁。浅情人不知。 第三辑 范仲淹词笔苏幕遮 怀旧碧云天，黄叶地，秋色连波，波上寒烟翠。山映斜阳天接水，芳草无情，更在斜阳外。 黯乡魂，追旅思，夜夜除非，好梦留人睡。明月楼高休独倚，酒入愁肠，化作相思泪。 渔家傲 秋思塞下秋来风景异，衡阳雁去无留意。四面边声连角起。千嶂里，长烟落日孤城闭。 浊酒一杯家万里，燕然未勒归无计。羌管悠悠霜满地。人不寐，将军白发征夫泪。 御街行 秋日怀旧纷纷坠叶飘香砌，夜寂静，寒声碎。真珠帘卷玉楼空，天淡银河垂地。年年今夜，月华如练，长是人千里。 愁肠已断无由醉，酒未到，先成泪。残灯明灭枕头敧，谙尽孤眠滋味。都来此事，眉间心上，无计相回避。 第四辑 欧阳修词笔踏莎行 候馆梅残候馆梅残，溪桥柳细。草薰风暖摇征辔。离愁渐远渐无穷，迢迢不断如春水。 寸寸柔肠，盈盈粉泪。楼高莫近危阑倚。平芜尽处是春山，行人更在春山外。 玉楼春 尊前拟把归期说尊前拟把归期说。未语春容先惨咽。人生自是有情痴，此恨不关风与月。 离歌且莫翻新阕。一曲能教肠寸结。直须看尽洛城花，始共春风容易别。 玉楼春 别后不知君远近别后不知君远近。触目凄凉多少闷。渐行渐远渐无书，水阔鱼沈何处问。 夜深风竹敲秋韵。万叶千声皆是恨。故欹单枕梦中寻，梦又不成灯又烬。 南歌子 凤髻金泥带凤髻金泥带，龙纹玉掌梳。走来窗下笑相扶。爱道画眉深浅入时无。 弄笔偎人久，描花试手初。等闲妨了绣功夫。笑问鸳鸯二字怎生书。 临江仙 记得金銮同唱第记得金銮同唱第，春风上国繁华。如今薄宦老天涯。十年岐路，空负曲江花。 闻说阆山通阆苑，楼高不见君家。孤城寒日等闲斜。离愁难尽，红树远连霞。 浪淘沙 把酒祝东风把酒祝东风。且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。 聚散苦匆匆。此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同。 浣溪沙 堤上游人逐画船堤上游人逐画船。拍堤春水四垂天。绿杨楼外出秋千。 白发戴花君莫笑，六么催拍盏频传。人生何处似尊前。 第五辑 姜白石词曲扬州慢 淮左名都淳熙丙申至日，予过维扬。夜雪初霁，荠麦弥望。入其城，则四顾萧条，寒水自碧，暮色渐起，戍角悲吟。予怀怆然，感慨今昔，因自度此曲。千岩老人以为有“黍离”之悲也。 淮左名都，竹西佳处，解鞍少驻初程。过春风十里，尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏，清角吹寒，都在空城。 杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生？ 点绛唇 丁未冬过吴松作燕雁无心，太湖西畔随云去。数峰清苦。商略黄昏雨。 第四桥边，拟共天随住。今何许。凭阑怀古。残柳参差舞。 暗香 旧时月色辛亥之冬，余载雪诣石湖。止既月，授简索句，且征新声，作此两曲，石湖把玩不已，使二妓肆习之，音节谐婉，乃名之曰《暗香》、《疏影》。 旧时月色，算几番照我，梅边吹笛？唤起玉人，不管清寒与攀摘。何逊而今渐老，都忘却春风词笔。但怪得竹外疏花，香冷入瑶席。 江国，正寂寂，叹寄与路遥，夜雪初积。翠尊易泣，红萼无言耿相忆。长记曾携手处，千树压、西湖寒碧。又片片、吹尽也，几时见得？ 鹧鸪天 元夕有所梦 肥水东流无尽期。当初不合种相思。梦中未比丹青见，暗里忽惊山鸟啼。 春未绿，鬓先丝。人间别久不成悲。谁教岁岁红莲夜，两处沉吟各自知。 疏影 苔枝缀玉辛亥之冬，余载雪诣石湖。止既月，授简索句，且征新声，作此两曲，石湖把玩不已，使二妓肆习之，音节谐婉，乃名之曰《暗香》、《疏影》。 苔枝缀玉，有翠禽小小，枝上同宿。客里相逢，篱角黄昏，无言自倚修竹。昭君不惯胡沙远，但暗忆、江南江北。想佩环、月夜归来，化作此花幽独。 犹记深宫旧事，那人正睡里，飞近蛾绿。莫似春风，不管盈盈，早与安排金屋。还教一片随波去，又却怨、玉龙哀曲。等恁时、重觅幽香，已入小窗横幅 鹧鸪天 正月十一日观灯巷陌风光纵赏时。笼纱未出马先嘶。白头居士无呵殿，只有乘肩小女随。 花满市，月侵衣。少年情事老来悲。沙河塘上春寒浅，看了游人缓缓归 齐天乐 蟋蟀丙辰岁，与张功父会饮张达可之堂。闻屋壁间蟋蟀有声，功父约予同赋，以授歌者。功父先成，辞甚美。予裴回末利花间，仰见秋月，顿起幽思，寻亦得此。蟋蟀，中都呼为促织，善斗。好事者或以三二十万钱致一枚，镂象齿为楼观以贮之。 庾郎先自吟愁赋，凄凄更闻私语。露湿铜铺，苔侵石井，都是曾听伊处。哀音似诉。正思妇无眠，起寻机杼。曲曲屏山，夜凉独自甚情绪？ 西窗又吹暗雨。为谁频断续，相和砧杵？候馆迎秋，离宫吊月，别有伤心无数。豳诗漫与。笑篱落呼灯，世间儿女。写入琴丝，一声声更苦。 长亭怨慢 渐吹尽余颇喜自制曲。初率意为长短句，然后协以律，故前后阕多不同。桓大司马云：“昔年种柳，依依汉南。今看摇落，凄怆江潭：树犹如此，人何以堪？”此语余深爱之。 渐吹尽，枝头香絮，是处人家，绿深门户。远浦萦回，暮帆零乱向何许？阅人多矣，谁得似长亭树？树若有情时，不会得青青如此！ 日暮，望高城不见，只见乱山无数。韦郎去也，怎忘得、玉环分付：第一是早早归来，怕红萼无人为主。算空有并刀，难剪离愁千缕。 淡黄柳 空城晓角客居合肥南城赤阑桥之西，巷陌凄凉，与江左异。唯柳色夹道，依依可怜。因度此阕，以纾客怀。 空城晓角，吹入垂杨陌。马上单衣寒恻恻。看尽鹅黄嫩绿，都是江南旧相识。 正岑寂，明朝又寒食。强携酒、小桥宅。怕梨花落尽成秋色。燕燕飞来，问春何在？唯有池塘自碧。 江梅引 人间离别易多时丙辰之冬，予留梁溪，将诣淮南不得，因梦思以述志。 人间离别易多时。见梅枝，忽相思。几度小窗幽梦手同携。今夜梦中无觅处，漫徘徊，寒侵被，尚未知。 湿红恨墨浅封题。宝筝空，无雁飞。俊游巷陌，算空有、古木斜晖。旧约扁舟，心事已成非。歌罢淮南春草赋，又萋萋。漂零客，泪满衣。 第六辑 淮海集淮海集系秦观所作，秦观，北宋时人，字太虚、海若。江苏高邮人。 鹊桥仙 纤云弄巧纤云弄巧，飞星传恨，银汉迢迢暗度。金风玉露一相逢，便胜却人间无数。 柔情似水，佳期如梦，忍顾鹊桥归路。两情若是久长时，又岂在朝朝暮暮。 浣溪沙 漠漠轻寒上小楼漠漠轻寒上小楼。晓阴无赖似穷秋。淡烟流水画屏幽。 自在飞花轻似梦，无边丝雨细如愁。宝帘闲挂小银钩。 满庭芳 山抹微云山抹微云，天连衰草，画角声断谯门。暂停征棹，聊共引离尊。多少蓬莱旧事，空回首、烟霭纷纷。斜阳外，寒鸦万点，流水绕孤村。 销魂。当此际，香囊暗解，罗带轻分。谩赢得、青楼薄幸名存。此去何时见也，襟袖上、空惹啼痕。伤情处，高城望断，灯火已黄昏。 踏莎行 雾失楼台雾失楼台，月迷津渡，桃源望断无寻处。可堪孤馆闭春寒，杜鹃声里斜阳暮。 驿寄梅花，鱼传尺素，砌成此恨无重数。郴江幸自绕郴山，为谁流下潇湘去？ 江城子 西城杨柳弄春柔西城杨柳弄春柔。动离忧。泪难收。犹记多情，曾为系归舟。碧野朱桥当日事，人不见，水空流。 韶华不为少年留。恨悠悠。几时休。飞絮落花时候、一登楼。便做春江都是泪，流不尽，许多愁。 行香子 树绕村庄树绕村庄，水满陂塘。倚东风、豪兴徜徉。小园几许，收尽春光。有桃花红，李花白，菜花黄。 远远围墙，隐隐茅堂。飏青旗、流水桥旁。偶然乘兴、步过东冈。正莺儿啼，燕儿舞，蝶儿忙 八六子 倚危亭倚危亭。恨如芳草，萋萋刬尽还生。念柳外青骢别后，水边红袂分时，怆然暗惊。 无端天与娉婷。夜月一帘幽梦，春风十里柔情。 怎奈向、欢娱渐随流水，素弦声断，翠绡香减，那堪片片飞花弄晚，蒙蒙残雨笼晴。正销凝。黄鹂又啼数声 望海潮 梅英疏淡梅英疏淡，冰澌溶泄，东风暗换年华。金谷俊游，铜驼巷陌，新晴细履平沙。长记误随车。正絮翻蝶舞，芳思交加。柳下桃蹊，乱分春色到人家。 西园夜饮鸣笳。有华灯碍月，飞盖妨花。兰苑未空，行人渐老，重来是事堪嗟。烟暝酒旗斜。但倚楼极目，时见栖鸦。无奈归心，暗随流水到天涯。 南乡子 妙手写徽真妙手写徽真，水剪双眸点绛唇。疑是昔年窥宋玉，东邻，只露墙头一半身。 往事已酸辛，谁记当年翠黛颦？尽道有些堪恨处，无情，任是无情也动人。 临江仙 千里潇湘挼蓝浦千里潇湘挼蓝浦，兰桡昔日曾经。月高风定露华清。微波澄不动，冷浸一天星。 独倚危樯情悄悄，遥闻妃瑟泠泠。新声含尽古今情。曲终人不见，江上数峰青。 江城子 南来飞燕北归鸿南来飞燕北归鸿，偶相逢，惨愁容。绿鬓朱颜重见两衰翁。别后悠悠君莫问，无限事，不言中。 小槽春酒滴珠红，莫匆匆，满金钟。饮散落花流水各西东。后会不知何处是？烟浪远，暮云重。 阮郎归 春风吹雨绕残枝春风吹雨绕残枝，落花无可飞。小池寒绿欲生漪，雨晴还日西。 帘半卷，燕双归，讳愁无奈眉。翻身整顿着残棋，沉吟应劫迟。 鹧鸪天 枝上流莺和泪闻枝上流莺和泪闻，新啼痕间旧啼痕。一春鱼鸟无消息，千里关山劳梦魂。 无一语，对芳尊。安排肠断到黄昏。甫能炙得灯儿了，雨打梨花深闭门。 第七辑 温飞卿诗集商山早行晨起动征铎，客行悲故乡。鸡声茅店月，人迹板桥霜。槲叶落山路，枳花明驿墙。因思杜陵梦，凫雁满回塘。 望江南 梳洗罢梳洗罢，独倚望江楼。过尽千帆皆不是，斜晖脉脉水悠悠。肠断白蘋洲。 菩萨蛮 小山重叠金明灭小山重叠金明灭，鬓云欲度香腮雪。懒起画蛾眉，弄妆梳洗迟。照花前后镜，花面交相映。新帖绣罗襦，双双金鹧鸪。 梦江南 千万恨千万恨，恨极在天涯。山月不知心里事，水风空落眼前花，摇曳碧云斜 第八辑 浣花词浣花词为韦庄所作词，韦庄，字端已。五代蜀人。 菩萨蛮 人人尽说江南好人人尽说江南好，游人只合江南老。春水碧于天，画船听雨眠。垆边人似月，皓腕凝霜雪。未老莫还乡，还乡须断肠。 思帝乡 春日游春日游，杏花吹满头。陌上谁家年少足风流？ 妾拟将身嫁与一生休。纵被无情弃，不能羞。 菩萨蛮 劝君今夜须沉醉劝君今夜须沉醉，尊前莫话明朝事。珍重主人心，酒深情亦深。须愁春漏短，莫诉金杯满。遇酒且呵呵，人生能几何。 女冠子 四月十七四月十七，正是去年今日，别君时。忍泪佯低面，含羞半敛眉。不知魂已断，空有梦相随。除却天边月，没人知。 谒金门 春雨足春雨足，染就一溪新绿。柳外飞来双羽玉，弄晴相对浴。楼外翠帘高轴，倚遍阑干几曲。云淡水平烟树簇，寸心千里目。 第九辑 竹山词竹山词为蒋捷所作，蒋捷，南宋时人。 虞美人 听雨[宋] 蒋捷 少年听雨歌楼上，红烛昏罗帐。壮年听雨客舟中，江阔云低，断雁叫西风。 而今听雨僧庐下，鬓已星星也。悲欢离合总无情，一任阶前，点滴到天明。 一剪梅 舟过吴江[宋] 蒋捷 一片春愁待酒浇。江上舟摇，楼上帘招。秋娘渡与泰娘桥，风又飘飘，雨又萧萧。 何日归家洗客袍？银字笙调，心字香烧。流光容易把人抛，红了樱桃，绿了芭蕉。 声声慢 秋声黄花深巷，红叶低窗，凄凉一片秋声。豆雨声来，中间夹带风声。疏疏二十五点，丽谯门、不锁更声。故人远，问谁摇玉佩，檐底铃声？ 彩角声吹月堕，渐连营马动，四起笳声。闪烁邻灯，灯前尚有砧声。知他诉愁到晓，碎哝哝、多少蛩声！诉未了，把一半、分与雁声。 贺新郎 兵后寓吴深阁帘垂绣，记家人、软语灯边，笑涡红透。万叠城头哀怨角，吹落霜花满袖。影厮伴、东奔西走。望断乡关知何处?羡寒鸦、到著黄昏后。一点点，归杨柳。 相看只有山如旧。叹浮云、本是无心，也成苍狗。明日枯荷包冷饭，又过前头小阜。趁未发、且尝村酒。醉探枵囊毛锥在，问邻翁、要写牛经否?翁不应，但摇手。 第十辑 刘辰翁词笔柳梢青·春感铁马蒙毡，银花洒泪，春入愁城。笛里番腔，街头戏鼓，不是歌声。那堪独坐青灯。想故国、高台月明。辇下风光，山中岁月，海上心情。 山花子·此处情怀欲问天此处情怀欲问天，相期相就复何年。行过章江三十里，泪依然。早宿半程芳草路，犹寒欲雨暮春天。小小桃花三两处，得人怜。 唐多令·明月满沧洲明月满沧洲。长江一意流。更何人、横笛危楼。天地不知兴废事，三十万、八千秋。 落叶女墙头。铜驼无恙不。看青山、白骨堆愁。除却月宫花树下，尘坱莽、欲何游。 第十一辑 山谷词山谷词为江西诗派开山之祖黄庭坚所作，黄庭坚，字鲁直，号山谷道人。文学家，书法家。 清平乐·春归何处春归何处。寂寞无行路。若有人知春去处。唤取归来同住。 春无踪迹谁知。除非问取黄鹂。百啭无人能解，因风飞过蔷薇 虞美人·宜州见梅作天涯也有江南信。梅破知春近。夜阑风细得香迟。不道晓来开遍、向南枝。 玉台弄粉花应妒。飘到眉心住。平生个里愿杯深。去国十年老尽、少年心。 鹧鸪天·座中有眉山隐客史应之和前韵即席答之黄菊枝头生晓寒。人生莫放酒杯干。风前横笛斜吹雨，醉里簪花倒著冠。 身健在，且加餐。舞裙歌板尽清欢。黄花白发相牵挽，付与时人冷眼看 菩萨蛮·半烟半雨溪桥畔半烟半雨溪桥畔，渔翁醉着无人唤。疏懒意何长，春风花草香。 江山如有待，此意陶潜解。问我去何之，君行到自知。 赠衡阳妓陈湘鸳鸯翡翠，小小思珍偶。眉黛敛秋波，尽湖南、山明水秀。娉娉袅袅，恰近十三余。春未透，花枝瘦，正是愁时候。 寻花载酒，肯落谁人后？只恐远归来，绿成阴、青梅如豆。心期得处，每自不由人，长亭柳，君知否，千里犹回首。 第十二辑 稼轩长短句辛弃疾，字幼安，号稼轩，善为词，可谓篇篇佳作，余读其词，拊手频频。选之一二示下。 鹧鸪天·代人赋晚日寒鸦一片愁。柳塘新绿却温柔。若教眼底无离恨，不信人间有白头。 肠已断，泪难收。相思重上小红楼。情知已被山遮断，频倚阑干不自由。 鹧鸪天·陌上柔桑破嫩芽陌上柔桑破嫩芽，东邻蚕种已生些。平冈细草鸣黄犊，斜日寒林点暮鸦。 山远近，路横斜，青旗沽酒有人家。城中桃李愁风雨，春在溪头荠菜花。 水调歌头·壬子三山被召陈端仁给事饮饯席上作长恨复长恨，裁作短歌行。何人为我楚舞，听我楚狂声？余既滋兰九畹，又树蕙之百亩，秋菊更餐英。门外沧浪水，可以濯吾缨。一杯酒，问何似，身后名？人间万事，毫发常重泰山轻。悲莫悲生离别，乐莫乐新相识，儿女古今情。富贵非吾事，归与白鸥盟。 第十三辑 东坡词放眼北宋文坛，惟东坡一人而已。其造诣所及之处，可谓全矣。 浣溪沙·细雨斜风作晓寒元丰七年十二月二十四日，从泗州刘倩叔游南山 细雨斜风作晓寒，淡烟疏柳媚晴滩。入淮清洛渐漫漫。雪沫乳花浮午盏，蓼茸蒿笋试春盘。人间有味是清欢。 西江月·顷在黄州顷在黄州，春夜行蕲水中，过酒家饮，酒醉，乘月至一溪桥上，解鞍，由肱醉卧少休。及觉已晓，乱山攒拥，流水锵然，疑非尘世也。书此语桥柱上。 照野弥弥浅浪，横空隐隐层霄。障泥未解玉骢骄，我欲醉眠芳草。可惜一溪风月，莫教踏碎琼瑶。解鞍欹枕绿杨桥，杜宇一声春晓。 蝶恋花·暮春别李公择簌簌无风花自堕。寂寞园林，柳老樱桃过。落日有情还照坐，山青一点横云破。路尽河回人转舵。系缆渔村，月暗孤灯火。凭仗飞魂招楚些，我思君处君思我。 临江仙·夜饮东坡醒复醉夜饮东坡醒复醉，归来仿佛三更。家童鼻息已雷鸣。敲门都不应，倚杖听江声。长恨此身非我有，何时忘却营营。夜阑风静縠纹平。小舟从此逝，江海寄余生。 临江仙·送钱穆父一别都门三改火，天涯踏尽红尘。依然一笑作春温。无波真古井，有节是秋筠。惆怅孤帆连夜发，送行淡月微云。尊前不用翠眉颦。人生如逆旅，我亦是行人。 定风波·南海归赠王定国侍人寓娘常羡人间琢玉郎，天应乞与点酥娘。尽道清歌传皓齿，风起，雪飞炎海变清凉。 万里归来颜愈少，微笑，笑时犹带岭梅香。试问岭南应不好，却道：此心安处是吾乡。 南乡子·和杨元素时移守密州东武望余杭，云海天涯两渺茫。何日功成名遂了，还乡，醉笑陪公三万场。不用诉离觞，痛饮从来别有肠。今夜送归灯火冷，河塘，堕泪羊公却姓杨。 第十四辑 元曲摘选折桂令·春情元代：徐再思 平生不会相思，才会相思，便害相思。身似浮云，心如飞絮，气若游丝。空一缕余香在此，盼千金游子何之。证候来时，正是何时？灯半昏时，月半明时。 人月圆·山中书事元代：张可久 兴亡千古繁华梦，诗眼倦天涯。孔林乔木，吴宫蔓草，楚庙寒鸦。数间茅舍，藏书万卷，投老村家。山中何事？松花酿酒，春水煎茶。 第十五辑 唐代曲子词菩萨蛮【唐】李白 平林漠漠烟如织，寒山一带伤心碧。暝色入高楼，有人楼上愁。 玉阶空伫立，宿鸟归飞急。何处是归程？长亭更短亭。 忆秦娥【唐】李白 箫声咽，秦娥梦断秦楼月。秦楼月，年年柳色，灞陵伤别。 乐游原上清秋节，咸阳古道音尘绝。音尘绝，西风残照，汉家陵阙。","categories":[null]},{"title":"木心诗二三句","path":"/wiki/poem/木心诗二三句.html","content":"木心的诗，感觉语言上特别有古意与情味，其诗描写故景往事者读之令人口齿含香，即刻诵之。今日夏初梅雨，摘之二三，与时消夏。 少年朝食 清早阳光 照明高墙一角 喜鹊喀喀叫 天井花坛葱茏 丫鬟悄声报用膳 紫檀圆桌 四碟端陈 姑苏酱鸭 平湖糟蛋 撕蒸笋 豆干末子拌马兰头 莹白的暖暖香粳米粥 没有比粥更温柔的了 东坡、剑南皆嗜粥 念予毕生流离红尘 就找不到一个似粥温柔之人 吁，予仍频忆江南古镇 梁昭明太子读书于我家后园 窗前的银杏树是六朝之前的 昔南塘春半、风和马嘶 日长无事蝴蝶飞 大卫 莫倚偎我 我习于冷 志于成冰 莫倚偎我 别走近我 我正升焰 万木俱焚 别走近我 清嘉录 **其一** 平明舟出山庄 万枝垂柳，烟雨迷茫 回眺岸上土屋亦如化境 舟子挽纤行急 误窜层网中，遂致勃谿 登岸相劝，几为乡人窘 偿以百钱，始悻悻散 行百余里，滩险日暮 约去港口数里以泊 江潮大来，荻芦如雪 肃肃与风相抟 是夕正望，月似紫铜盘 水势益长，澎湃声起 俄闻金山蒲牢动，漏下矣 **其二** 梅雨时备缸瓮收旧雨水 供烹茶，曰梅水 梅天多雨，雨水极佳 贮之味经年不变 人于初交黄梅时收雨 以其甘滑胜山泉 ​ 从前慢 记得早先少年时 大家诚诚恳恳 说一句是一句 清早上火车站 长街黑暗无行人 卖豆浆的小店冒着热气 从前的日色变得慢 车、马、邮件都慢 一生只够爱一个人 从前的锁也好看 钥匙精美有样子 你锁了，人家就懂了 魏玛早春 有一株树 曾见一株这样的树 冬季晴和了几天 不觉彤云叆叇 万千乌鸦出林 聒鸣飞旋 乡民谓之噪雪 称彤云为酿雪 风凛冽 行人匆匆回家 曾见一株树在这样的时日 枝头齐茁蓓蕾 淡绛的星星点点密布楂条 长势迅速梢端尤累累若不胜载 际此霙雪纷纷下 无数花苞仰雪绽放 雪片愈大愈紧 群花朵朵舒展 树高十米 干围一点五米 叶如樟似杨 顶冠直径十余米 花状类乎扶桑之樱 色与雪同 吐香清馥 冬季中下几遭雪 发几度花 霰霙之夕 寂然不应 初雪之顷无气息 四野积雪丰厚 便闲幽馨流播 昼夜氤氲 雪销 花凋谢 植物志上没有这株树的学名 中国洞庭湖之南湘省 洞口县 水口山 树在那里已两百多年 爪哇国 从前的人真有趣 他们要形容荒唐 便说“一错错到了爪哇国” 他们以为爪哇是最远的了 你想明朝人有多可爱 如偈 晚晴风光好 大梦觉犹眠 每忆儿时景 莲叶何田田 谑庵片简 隆恩寺无他奇 独大会明堂百余丈 可玩月 径下有云深庵 五月，啖其樱桃 八月，落其苹果 樱桃人啖后百鸟俱来 绿羽翠翎者，白身朱咮者 嘈嘈各呈妙音 京师五月 石榴花正开 照眼鲜明 居人每与夹竹桃列中庭 榴竹之间，配以鱼缸 朱鳞数尾游漾其中 几于家家如此 我 我是一个在黑暗中大雪纷飞的人哪 浣花溪归 出成都门 左万里桥 西折，溪流纤秀长曲 如连环，如玦色 如鉴，如琅玕 窈然深碧 潆回城下 皆浣花溪之委也 夏日山居 遍地悬铃木 树叶杂花横生 紫檀，木兰，石榴 扇形的棕榈 油润润的乌柏 朝暾初升 小丘上阳光已很强烈 芬芳的雾闪着兰晕 林薮蓊郁，群峦后 终年积雪的巍巍高峰","categories":[null]},{"title":"入夏记","path":"/wiki/life/入夏记.html","content":"时间来到二零二四年的五月，我在杭州，"},{"title":"杭州","path":"/wiki/life/杭州.html","content":"杭州这座城市，作为江南地域风光的代表，因为一句 “人间天堂，最忆杭州” ，永远不乏四处慕名而来的人。杭州地处富庶的吴越之邦，近在市区，便能看见重重青山和一带长堤，一片西湖水，三面环山的西湖宛若隐世之所，然而却又在城市之中。 自唐宋以来，来来往往的大诗人给它写过诗篇。 白居易说“江南忆，最忆是杭州”。 苏东坡说“欲把西湖比西子，淡妆浓抹总相宜”。 最喜欢柳永的一句：“重湖叠巘清嘉，有三秋桂子，十里荷花，羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃”，把西湖的山水景物一下子具象化起来。 后来晚明的张岱也为西湖写过《西湖梦寻》一书，书中细陈西湖名胜古迹。其中《湖心亭看雪》一篇，“雾凇沆砀，天与云与山与水，上下一白，湖中影子，惟长堤一痕，湖心亭一点，与余舟一芥，舟中人两三粒而已”，更是将雪后西湖之景描绘的让世人神往。 更多的，《白蛇传》中许仙和白素贞的断桥相遇，许嵩的《断桥残雪》，以及古来好评的“西湖十景”，将汉字的美抒写到极致。 古评西湖十景： 苏堤春晓、曲院风荷、平湖秋月、断桥残雪、花港观鱼、柳浪闻莺、三潭印月、双峰插云、雷峰夕照、南屏晚钟 新评西湖十景： 云栖竹径、满陇桂雨、虎跑梦泉、龙井问茶、九溪烟树、 吴山天风、阮墩环碧、黄龙吐翠、玉皇飞云、宝石流霞 三评西湖十景： 灵隐禅踪、六和听涛、岳墓栖霞、湖滨晴雨、钱祠表忠、万松书缘、杨堤景行、三台云水、梅坞春早、北街梦寻 三十六景了，然而未必你去了杭州就都能看到，比如“断桥残雪”，得是一个大雪天才行，近年来杭州很少下雪，更莫说大雪了；比如“曲院风荷”，得在夏天才有荷花；比如“雷峰夕照”，得赶上一个大晴天，还得在夕阳西下的那十来分钟里去到长桥一带；比如“满陇桂雨”，得是金秋九月桂花开放的时节；比如“龙井问茶”，最好是清明节前后采茶的那段时间。 虽然上述景点都在西湖边，然而也未必能把西湖绕上一圈，更别说西湖群山里的景点。 在我二三年来到杭州后，陆陆续续把杭城走了个遍，熟悉之后，西湖也不怎么去了。 以下是给读者推荐一些去处： 1、西湖游船体验，可乘坐摇橹船，有坐船的感觉，价格偏贵。也可坐大船，更平稳。 路线：北段换湖心岛一圈；南段环小瀛洲（三潭印月） 其实大运河也可以体验游船，如武林门码头到拱宸桥。 2、曲院风荷 很惬意的一处林子，临着北山街，有跨虹桥，可眺望外西湖里西湖的风光。 3、西湖漫步 推荐东岸，有修葺好的沿湖步道，离地铁站和商圈近，交通方便。有精力可走苏堤。"},{"title":"鲤城记","path":"/wiki/life/鲤城记.html","content":"泉州，古称刺桐，相传五代时期，此处满城种满刺桐花，有刺桐城之名，因其城郭形似鲤鱼，又名鲤城。 在开始鲤城游记之前，先来讲讲这座城市的历史，早在西晋末年，中原大乱，北方士族纷纷南渡，落户泉州。在老城区，走进巷子里，你可以看到家家门楣上写着“衍派”、“传芳”之类，大概在述说着祖上的出处和风光。宋元时期的泉州，作为海上丝绸之路的起点，迎来了文化贸易的极大发展，种种宗教文化在此生根发芽，在泉州古城区，你可以看到佛教的开元寺、承天寺，道教的玄妙观，孔子的文庙，关羽岳飞的武庙，伊斯兰教的清净寺，基督教的天主教堂，信奉妈祖的天后宫…… 宋代的朱熹说：“此地古称佛国，满街都是圣人” 泉州文旅说：“泉州，半城烟火半城仙” 在泉州申遗成功后，泉州这座城市逐渐成为热门旅游目的地，为大众所熟知。除了古城区的宗庙林立，泉州有着闽南特色的红砖古厝建筑群，以其“红砖白石双坡曲，出砖入石燕尾脊，雕梁画栋皇宫式”的特色尽显艳丽恢弘，还有备受推崇的簪花文化，展现了闽南沿海渔女的风貌。 一、泉州古城西街来泉州，必逛西街，在进入西街处的十字路口，可以看到一座白色的钟楼，同时也是红绿灯指示牌O(∩_∩)O哈哈~ 西街上一路美食、文创店铺、簪花体验馆。走一圈下来，卖的最多的本地的吃食，面线糊、四果汤、炸醋肉、肉粽、姜母鸭、烤鸡腿，大家自行品尝吧。 橘若，二楼有个天台，可以屋顶拍照，以东西塔作背景。 西街路上，有条小巷子，泉府小西埕。 赶上一个洛江区的旅游推广活动，看着海报地图制作精美，随手拍了一张，给大家放上瞅瞅，洛江区最有名的当属洛阳桥吧，这座北宋修建的石桥历经了千年。 开元寺开元寺，这座泉州最盛的寺庙，也在西街边上。相传唐代富商黄守恭梦见桑树长出莲花，遂舍桑园建寺，初名“莲花寺”。后开元年间，唐玄宗诏天下诸州各建一寺，以年号为名，遂改称开元寺。 开元寺的牌匾，上书“桑莲法界”。 东西塔，东塔为镇国塔，西塔为仁寿塔，石块垒塔，浮雕着各式佛门人物。塔檐下天然的避雨，成了飞鸟喜欢的落脚地，特别是鸽子和麻雀。坐在塔下的石凳上，仰观着这座千年石塔，时不时一只鸽子扑棱棱的飞过，落到塔檐上。 寺院里的植物：长在树干上的菠萝蜜，香蕉树、菩提树、开着的凤凰花。 承天寺承天寺，又叫月台寺。相比热闹西街上的开元寺，承天寺显得安静很多，这边游客稀少，是个静谧的所在。 大殿两侧的走廊垂着帘子，别有一番天地，那天下午日落时分走在这长廊里，听见大殿诵经的真言和断续的撞钟声。 庭院里的植物：刚长出的龙眼，可能到七八月份才能成熟，开三瓣花的石榴，被遗落树干上的鸡蛋花。 泉南堂 关岳庙 文庙 天后宫 土地庙 西湖公园 二、红砖古厝梧林传统村落"},{"title":"读《琅嬛文集》录","path":"/wiki/poem/读《琅嬛文集》录.html","content":"近来读张岱《琅嬛文集》，遇见许多赏心悦目的文字，摘录此篇分享给大家。 故知世间山川、云物、水火、草木、色声、香味，莫不有冰雪之气；其所以恣人挹取受用之不尽者，莫深于诗文。 山之有空翠，气之有沆瀣，月之有烟霜，竹之有苍蒨，食味之有生鲜，古铜之有青绿，玉石之有胞浆，诗之有冰雪，皆是物也。苏长公曰：“子由近作《栖贤僧堂记》，读之惨凉，觉崩崖飞瀑，逼人寒栗。”噫！此岂可与俗人道哉！笔墨之中，崖瀑何从来哉！ ​ ——一卷冰雪文序 余幼遵大父教，不读朱注。凡看经书，未尝敢以各家注疏横据胸中。正襟危坐，朗诵白文数十余过，其意义忽然有省。间有不能强解者，无意无义，贮之胸中。或一年，或二年，或读他书，或听人议论，或见山川云物鸟兽虫鱼，触目惊心，忽于此书有悟，取而出之，名曰《四书遇》。盖“遇”之云者，谓不于其家，不于其寓，直于途次之中邂逅遇之也。古人见道旁蛇斗而悟草书，见公孙大娘舞剑器而笔法大进，盖真有以遇之也。古人精思静悟，钻研已久，而石火电光，忽然灼露，其机神摄合，政不知从何处着想也。举子十年攻苦于风檐寸晷之中，构成七艺，而主司以醉梦之余，忽然相投，如磁引铁，如珀摄刍，相悦以解。直欲以全副精神注之，其所遇之奥窍，真有不可得而自解者矣。推而究之，色声香味触发中间无不有遇之一窍，特留以待深心明眼之人，邂逅相遇，遂成莫逆耳。 余遭乱离两载，东奔西走，身无长物，委弃无余。独于此书，收之箧底，不遗只字。 ​ ——四书遇序 鸡鸣枕上，夜气方回，因想余生平：繁华靡丽，过眼皆空，五十年来，总成一梦。今当黍熟黄粱，车旋蚁穴，当作如何消受？遥思往事，忆即书之，持向佛前，一一忏悔。不次岁月，异年谱也；不分门类，别志林也。偶拈一则，如游旧径，如见故人，城郭人民，翻用自喜，真所谓痴人前不得说梦矣。 昔有西陵脚夫，为人担酒，失足破其瓮，念无以偿，痴坐伫想曰：“得是梦便好。”一寒士乡试中式，方赴鹿鸣宴，恍然犹忆非真，自啮其臂曰：“莫是梦否？”一梦耳，惟恐其非梦，又惟恐其是梦，其为痴人则一也。余今大梦将寤，犹事雕虫，又是一番梦呓。因叹慧业文人，名心难化，政如邯郸梦断，漏尽钟鸣，卢生遗表，犹思摹拓二王，以流传后世。则其名根一点，坚固如佛家舍利，劫火猛烈，犹烧之不失也。 ​ ——陶庵梦忆序 周又新先生每啜茶，辄道白门闵文水。尝曰：“恨不令宗子见。”一日，文水至越，访又新先生，携茶具，急至予舍。余时在武陵不值，后归，甚懊丧。 戊寅余至白门。甫登岸，即往桃叶渡访文水。时日晡矣，余至文水家，文水亦他出。余坐久。余意文水一少年好事者，及至，则瞿瞿一老子，与余叙款曲，愕愕如野鹿不可接。方欲纵谈，而老子忽起曰：“余杖忘某所，去取杖。”起席竟去。余曰：“今日岂可空去？”待其返，更定矣。老子睨余曰：“客尚在耶？客尚在何为者？”余曰：“周又老尝道闵先生精饮事，愿借余沥以解渴思。”文水喜，即自起当炉，茶旋煮，速如风雨。导至一室，幽窗净几，荆溪壶及成宣窑瓷瓯十余具，皆精绝。余问老子曰：“此茶何产？”老子曰：“阆苑茶也。”余再啜之，曰：“莫绐余，是阆苑制法而味不似。”老子昵笑曰：“客知是何产？”余再啜之，曰：“何其似罗嵑甚也？”老子吐舌曰：“奇！奇！”余问水曰：“何水？”老子曰：“惠水。”余又曰：“莫绐余，惠水至此千里，岂有水之圭角毫芒不动，生磊若是乎？”老子曰：“不复敢隐。舍间取水，必俟惠山人静夜分涸其井，淘洗数次，至黎明，涓流初满，载以大瓮，藉以文石。舟非风则勿行，水体不劳，水性不熟，故与他泉特异。”又吐舌曰：“奇奇！”言未毕，老子自去。少顷，一壶满斟余曰：“客啜此。”余曰：“香扑烈甚，味浑厚，此春茶也，向瀹者的是秋采。”老子大笑曰：“余年七十，精饮事五十余年，未尝见客之赏鉴若此之精也。五十年知己，无出客右，岂周又老谆谆向余道山阴有张宗老者，得非客乎？”余又大笑。遂相好如生平欢，饮啜无虚日。 ​ ——茶史序 盖闻地有高人，品格与山川并重；亭遗古迹，梅花偕姓氏俱香。名流虽已代迁，胜事自须人补。在孤山逸老，高洁韵同秋水，孤清操比寒梅。疏影横斜，远映西湖清浅；暗香浮动，长陪夜月黄昏。今乃人去山空，依然水流花放。瑶葩洒雪，乱点冢上苔痕；玉树迷烟，恍堕林间鹤羽。兹来韵友，欲步先贤，补种千梅，重开孤屿。凌寒三友，蚤结九里松篁；破腊一枝，远谢六桥桃柳。[插图]想水边半树，点缀冰花；待披雪后横枝，低昂铁干。美人来自林下，高士卧于山中。白石苍厓，拟筑草亭招素鹤；浓山淡水，闲锄明月种梅花。有志竟成，无约不践。将与罗浮争艳，还期庾岭分香。实为林处士之功臣，亦是苏东坡之胜友。吾辈常劳梦想，应有宿缘。哦《曲江诗》，便见孤芳风韵；读《广平赋》，尚思铁石心肠。共策灞水之驴，且向段桥踏雪；遥期漆园之蝶，群来林墓寻梅。莫负佳期，用追芳躅。 ​ ——补孤山种梅序 昔有一僧人与一士子同宿夜航船，士子高谈阔论，僧畏慑，卷足而寝。僧听其语有破绽，乃曰：“请问相公，澹台灭明是一个人，是两个人？”士子曰：“是两个人。”僧曰：“这等，尧舜是一个人两个人？”士子曰：“自然是一个人。”僧人乃笑曰：“这等说起来，且待小僧伸伸脚。 ​ ——夜航船序 余生不辰，阔别西湖二十八载。然西湖无日不入吾梦中，而梦中之西湖实未尝一日别余也。前甲午、丁酉，两至西湖，如涌金门、商氏之楼外楼、祁氏之偶居、钱氏余氏之别墅，及余家之寄园一带湖庄，仅存瓦砾。则是余梦中所有者，反为西湖所无。及至断桥一望，凡昔日之歌楼舞榭，弱柳夭桃，如洪水渰没，百不存一矣。余乃急急走避，谓余为西湖而来，今所见若此，反不若保吾梦中之西湖为得计也。因想余梦与李供奉异，供奉之梦天姥也，如神女名姝，梦所未见，其梦也幻。余之梦西湖也，如家园眷属，梦所故有，其梦也真。 今余僦居他氏，已二十二载，梦中犹在故居。旧役小傒，今已白头，梦中仍是总角。夙习未除，故态难脱。而今而后，余但向蝶庵岑寂，蘧榻纡徐，惟吾旧梦是保，一派西湖景色，犹端然未动也。儿曹诘问，偶为言之，总是梦中说梦，非魇即呓也。余犹山中人归自海上，盛称海错之美，乡人竞来共舐其眼。嗟嗟，金齑瑶柱，过舌即空，则舐眼亦何救其馋哉？第作《梦寻》七十二则，留之后世，以作西湖之影。 ​ ——西湖梦寻序 幸生胜地，鞋靸间饶有山川，喜作闲人，酒席间只谈风月。野航恰受，不逾两三；便榼随行，各携一二。僧上凫下，觞止茗生。谈笑杂以诙谐，陶写赖此丝竹。兴来即出，可趁樵风：日暮辄归，不因剡雪。愿邀同志，用续前游。 ​ ——游山小启 会稽佳山水，甲于天下，而霞蔚云蒸，尤聚于山阴道上。故随足所至，皆胜地名山。王右军卜居兹土，于千岩万壑中，独取兰亭一席地。其景物风华，定当妙绝千古。且余少时见兰亭墨刻，岩峦奇峭，亭榭巍峨，曲水流觞，浴鹅涤砚。开卷视之，不禁神往。 ​ ——古兰亭辩 蜀人张岱，陶庵其号也。少为纨绔子弟，极爱繁华，好精舍，好美婢，好娈童，好鲜衣，好美食，好骏马，好华灯，好烟火，好梨园，好鼓吹，好古董，好花鸟，兼以茶淫橘虐，书蠹诗魔。劳碌半生，皆成梦幻。年至五十，国破家亡，避迹山居，所存者破床碎几，折鼎病琴，与残书数帙，缺砚一方而已。布衣蔬食，常至断炊。回首二十年前，真如隔世。 初字宗子，人称石公，即字石公。好著书，其所成者有《石匮书》《张氏家谱》《义烈传》《琅嬛文集》《明易》《大易用》《史阙》《四书遇》《梦忆》《说铃》《昌谷解》《快园道古》《傒囊十集》《西湖梦寻》《一卷冰雪文》行世。 生于万历丁酉八月二十五日卯时，鲁国相大涤翁之树子也。母曰陶宜人。幼多痰疾，养于外大母马太夫人者十年。外太祖云谷公宦两广，藏生牛黄丸盈数簏，自余囡地以至十有六岁，食尽之而厥疾始瘳。六岁时，大父雨若翁携余之武林，遇眉公先生跨一角鹿，为钱唐游客，对大父曰：“闻文孙善属对，吾面试之”，指屏上李白骑鲸图曰：“太白骑鲸，采石江边捞夜月。”余应曰：“眉公跨鹿，钱唐县里打秋风。”眉公大笑起跃曰：“那得灵隽若此，吾小友也。”欲进余以千秋之业，岂料余之一事无成也哉？ 甲申以后，悠悠忽忽，既不能觅死，又不能聊生，白发婆娑，犹视息人世。恐一旦溘先朝露，与草木同腐，因思古人如王无功、陶靖节、徐文长皆自作墓铭，余亦效颦为之。甫构思，觉人与文俱不佳，辍笔者再。虽然，第言吾之癖错，则亦可传也已。曾营生圹于项王里之鸡头山，友人李研斋题其圹曰：“呜呼，有明著述鸿儒陶庵张长公之圹。”伯鸾高士，冢近要离，余故有取于项里也。明年，年跻七十，死与葬，其日月尚不知也，故不书。 铭曰：穷石崇，斗金谷。盲卞和，献荆玉。老廉颇，战涿鹿。赝龙门，开史局。馋东坡，饿孤竹。五羖大夫，焉肯自鬻。空学陶潜，枉希梅福。必也寻三外野人，方晓我之衷曲。 ​ ——自为墓志铭 昔日东坡思栗里，良穗怀新，写尽澄心纸。今见平畴如绿绮，翻来白浪潮头起。野老豚蹄心更侈，篝满瓯窭，奢愿还无已。处处军输如吸髓，敢云畎亩忘庚癸。 ​ ——平畴麦浪 何必微萤量数斛，遇夜嬉游，囊火燃山谷。怎比渔灯千万簇，星星炤出田畴绿。疑是天河成反覆，遍野疏星，连住招摇宿。此际神槎乘博陆，支机石冷空杼柚。 ​ ——孤村渔火 雪巘晴光如缺列，闪烁凝晴，入眼翻成瞥。一幅鹅绫无缁涅，条条水道如轨辙。山入秋湖皆小垤，滋蔓难图，迢递如瓜瓞。余到洛伽心胆裂，银狮蹴起潮头雪。 ​ ——三山霁雪","categories":[null]},{"title":"HBase","path":"/wiki/interview/HBase.html","content":"一. HBase概述HBase（Hadoop Database）是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。HBase 本质上是一个数据模型，可以提供快速随机访问海量结构化数据。利用 Hadoop 的文件系统（HDFS）提供的容错能力。它是 Hadoop 的生态系统，使用 HBase 在 HDFS 读取消费&#x2F;随机访问数据，是 Hadoop 文件系统的一部分。HBase 是一个面向列的数据库，在表中它由行排序。 Hbase的适用场景为 写密集型应用，每天写入量巨大，而相对读数量较小的应用，比如IM的历史消息，游戏的日志等等。 不需要复杂查询条件来查询数据的应用，HBase只支持基于rowkey的查询，对于HBase来说，单条记录或者小范围的查询是可以接受的，大范围的查询由于分布式的原因，可能在性能上有点影响，而对于像SQL的join等查询，HBase无法支持。 对性能和可靠性要求非常高的应用，由于HBase本身没有单点故障，可用性非常高。 二. HBase逻辑模型1.表(table)： 表的作用将存储在HBase的数据组织起来。 2.行健(rowkey): 行的唯一标示类似于主键; 按照字典序进行排序储存;最大长度是64KB，但是建议长度是10-100byte。 3.列族(column family): 在行中的数据都是根据列族分组； 在HBase想要使用列，必须要指定列族，列必须要归属于某一个列族; 4.列族需要在表定义的时候预先给出，而列不需要； 列族是存储，权限控制，调优的最小单元。 5.时间戳（timestamp）： 时间戳是给定值的一个版本号标识，每一个值都会对应一个时间戳，时间戳是和每一个值同时写入HBase存储系统中的。在默认情况下，时间戳表示数据服务在写入数据时的系统时间。 在实际的HDFS存储中，以及存储每个字段数据所对应的完成的键值对： ｛row key，column family，column name，timestamp｝－&gt;value 如上图key3行Address字段下t2时间戳下的数值Shanghai，存储的完整键值对是： ｛key3，PersonalInfo，Address，t2｝-&gt;Shanghai 也就是说Hbase真实是不存在行列的概念，只有键值对的概念。其行的概念是通过相邻的键值对的数据比较构建出来的。所以其物理上不是二维表的概念。 三. HBase物理模型对于HBase 的物理模型包括Table的分割、Region的拆分、Region的分布，Region的构成。 Table的分割 是指Table 中的所有行都按照 rowkey 的字典序排列，Table 在行的方向上分割为多个Region，一个Region在同一时刻只能被同一个RegionServer管理，RegionServer可以管理多个Region（一对多）， Region的拆分 Region是按大小分割的，新创建的表只有一个region（数据为空），随着数据增多，region不断增大，当增大到一个阀值时，region就会拆分为两个新的region，之后会有越来越多的region。 Region 的分布 Region 是 HBase 中分布式存储和负载均衡的最小单元，不同的Region分布到不同的RegionServer上。 Region的构成 Region虽然是分布式分布式存储的最小单元，但并不是存储的最小单元，Store是存储的最小单元，Region由一个或者多个Store组成，每个Store会保存一个Column Family；每个Store又由一个MemStore或0至多个StoreFile组成；MemStore存储在内存中，StoreFile存储在HDFS中。 如上图所示，Hbase一张表由一个或多个Region组成,一个ReginServer可以存储一或多个Region,一个Region只能由一个机器存储。记录之间按照Row Key的字典序排列。Region按大小分割的，每个表一开始只有一个Region，随着数据不断插 入表，Region不断增大，当增大到一个阀值的时候，Region就会等分会 两个新的Hregion。当table中的行不断增多，就会有越来越多的 Region。 四. Hbase架构HBase采用Master&#x2F;Slave架构搭建集群，它隶属于Hadoop生态系统，由一下类型节点组成：HMaster节点、HRegionServer节点、ZooKeeper集群，而在底层，它将数据存储于HDFS中，因而涉及到HDFS的NameNode、DataNode等，总体结构如下： 1、Client 提供了访问HBase的一系列API接口，如Java Native API、Rest风格http API、Thrift API、scala等，并维护cache来加快对HBase的访问。 2、Zookeeper （1）保证任何时候，集群中只有一个master （2）存贮所有Region的寻址入口。 （3）实时监控Region server的上线和下线信息，并实时通知Master （4）存储HBase的schema和table元数据 3、Master （1）为Region server分配region （2）负责Region server的负载均衡 （3）发现失效的RegionServer并重新分配其上的region （4）管理用户对table的增删改操作 4、RegionServer Region server维护region，处理对这些region的IO请求，向HDFS文件系统中读写数据。一个RegionServer由多个region组成，一个region由多个store组成，一个store对应一个CF（列族），而一个store包括位于内存中的memstore和位于磁盘的storefile，每个storefile以HFile格式保存在HDFS上。写操作先写入memstore，当memstore中的数据达到某个阈值时，RegionServer会启动flashcache进程写入storefile，每次写入形成单独的一个storefile。 五.HBase RowKey设计rowkey是一个二进制码流，可以是任意字符串，最大长度64kb ，实际应用中一般为10-100bytes，以 byte[] 形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w&#x3D;10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。 rowkey散列原则如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。 rowkey唯一原则必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。 热点问题HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。 下面是一些常见的避免热点的方法以及它们的优缺点： 加盐这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。 哈希哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。 反转第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题。 hbase的rowkey设计决定了数据的分区和查询的方式，是使用hbase前一定要想清楚的，以下简单列举了设计hbase rowkey时需要考虑的问题 1. rowkey是唯一的吗？rowkey相同的记录在hbase里被认为是同一条数据的多个版本，查询时默认返回最新版本的数据，所以通常rowkey都需要保证唯一，除非用到多版本特性 最佳设计实践： 123rowkey就好比数据库的里的主键，他唯一确定了一条记录，它可以是一个字段也可以是多个字段拼接起来:每个用户只有一条记录： [userid]每个用户有多条交易记录：[userid][orderid] 2. 满足查询场景吗？rowkey的设计限制了数据的查询方式，hbase只有两种查询方式： 根据完整的rowkey查询（get）类似传统DB的sql:select * from table where rowkey &#x3D; ‘abcde’这种查询方式需要知道完整的rowkey，即组成rowkey的所有字段的值都是确定的 根据rowkey的范围查询（scan):类似传统DB的sql:select * from table where ‘abc’ &lt; rowkey &lt;’abcx’这种查询方式需要知道数据rowkey左边的值，就好像一本英文字典，你可以查询pre开头的所有单词，也可以查询prefi开头的所有单词，但是没办法查询中间是efi或结尾是ix的所有单词，除非翻阅整个字典。 最佳设计实践： 在有限的查询方式下如何实现复杂查询： 12345671.再建另外一张表作为索引表，应用双写2.使用filter，在服务端过滤掉不需要的数据3.使用二级索引4.如何实现倒序（新的数据排在前面，如：order by orderTime desc）： 使用反向scan：scan.setReverse(true) #反向scan的性能比正常scan要差，如果倒序的场景占大头可以设计上就把数据倒序： [hostname][log-event][timestamp] =&gt; [hostname][log-event][Long.MAX_VALUE – timestamp] 3. 数据足够分散，会产生热点吗？散列的目的是数据可以分散到不同的分区，不至于产生热点，把某一台服务器累死，其他服务器闲置，充分发挥分布式和并发的优势 最佳设计实践： 1234567891.md5 [userId][orderid] =&gt; [md5(userid).subStr(0,4)][userId][orderid] 2.反转 [userId][orderid] =&gt; [reverse(userid)][orderid]3.取模 [timestamp][hostname][log-event] =&gt; [bucket][timestamp][hostname][log-event] long bucket = timestamp % numBuckets;4.增加随机数 [userId][orderid] =&gt; [userId][orderid][random(100)] 4. rowkey可以再短点吗?短的rowkey可以减少数据量 ，提高查询写入性能 最佳设计实践： 12341. 使用long或int型代替String 如： &#x27;2015122410&#x27; =&gt; Long(2015122410)2. 使用编码代替名称 如：’淘宝‘ =&gt; tb 5. scan时会不会查询出不需要的数据？假设有以下场景：table1的rowkey是: colume1+ colume2+ colume3现在需要查询colume1&#x3D; host1 的所有数据： scan ‘table1’,{startkey&#x3D;&gt; ‘host1’,endkey&#x3D;&gt; ‘host2’}此时如果有一条记录colume1&#x3D;host12，这条记录也会被查询出来：因为:‘host1’ &lt; ‘host12’ &lt; ‘host2’但显然这条记录不是我们想要的 最佳设计实践： 12341. 字段定长 [colume1][colume2] =&gt; [rpad(colume1,&#x27;x&#x27;,20)][colume2]2. 添加分隔符 [colume1][colume2] =&gt; [colume1][_][colume2] 常见设计实例：日志类、时间序列数据查询场景：1.查询某台机器某个指标某段时间内的数据[hostname][log-event][timestamp] 2.查询某台机器某个指标最新的几条数据timestamp &#x3D; Long.MAX_VALUE – timestamp[hostname][log-event][timestamp] 3.数据只有时间一个维度或某一个维度数据量特别大long bucket &#x3D; timestamp % numBuckets;[bucket][timestamp][hostname][log-event] 交易类数据查询场景：1.查询某个卖家某段时间内的交易记录[seller id][timestmap][order number] 2.查询某个买家某段时间内的交易记录[buyer id][timestmap][order number] 3.根据订单号查询[order number] 4.同时满足1，2，3三张表：一张买家维度表，rowkey为：[buyer id][timestmap][order number]一张卖家维度表，rowkey为：[seller id][timestmap][order number]一张订单索引表，rowkey为：[order number]"},{"title":"MySQL","path":"/wiki/interview/MySQL.html","content":"在线文章一、图解MySQL 站内汇总一、SQL语法基础 常见面试题1. 能说下myisam 和 innodb的区别吗？myisam引擎是5.1版本之前的默认引擎，支持全文检索、压缩、空间函数等，但是不支持事务和行级锁，所以一般用于有大量查询少量插入的场景来使用，而且myisam不支持外键，并且索引和数据是分开存储的。 innodb是基于聚簇索引建立的，和myisam相反它支持事务、外键，并且通过MVCC来支持高并发，索引和数据存储在一起。 2. 说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么？索引按照数据结构来说主要包含B+树和Hash索引。 假设我们有张表，结构如下： 123456create table user( id int(11) not null, age int(11) not null, primary key(id), key(age)); B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。 这是主键聚簇索引存储的结构，那么非聚簇索引的结构是什么样子呢？非聚簇索引(二级索引)保存的是主键id值，这一点和myisam保存的是数据地址是不同的。 最终，我们一张图看看InnoDB和Myisam聚簇和非聚簇索引的区别 3. 那你知道什么是覆盖索引和回表吗？覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。 而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。 以上面的user表来举例，我们再增加一个name字段，然后做一些查询试试。 12explain select * from user where age=1; //查询的name无法从索引数据获取explain select id,age from user where age=1; //可以直接从索引获取 4. 锁的类型有哪些呢mysql锁分为共享锁和排他锁，也叫做读锁和写锁。 读锁是共享的，可以通过lock in share mode实现，这时候只能读不能写。 写锁是排他的，它会阻塞其他的写锁和读锁。从颗粒度来区分，可以分为表锁和行锁两种。 表锁会锁定整张表并且阻塞其他用户对该表的所有读写操作，比如alter修改表结构的时候会锁表。 行锁又可以分为乐观锁和悲观锁，悲观锁可以通过for update实现，乐观锁则通过版本号实现。 5. 你能说下事务的基本特性和隔离级别吗？事务基本特性ACID分别是： 原子性：指的是一个事务中的操作要么全部成功，要么全部失败。 一致性：指的是数据库总是从一个一致性的状态转换到另外一个一致性的状态。比如A转账给B100块钱，假设中间sql执行过程中系统崩溃A也不会损失100块，因为事务没有提交，修改也就不会保存到数据库。 隔离性：指的是一个事务的修改在最终提交前，对其他事务是不可见的。 持久性：指的是一旦事务提交，所做的修改就会永久保存到数据库中。 而隔离性有4个隔离级别，分别是： read uncommit 读未提交，可能会读到其他事务未提交的数据，也叫做脏读。 用户本来应该读取到id&#x3D;1的用户age应该是10，结果读取到了其他事务还没有提交的事务，结果读取结果age&#x3D;20，这就是脏读。 read commit 读已提交，两次读取结果不一致，叫做不可重复读。 不可重复读解决了脏读的问题，他只会读取已经提交的事务。 用户开启事务读取id&#x3D;1用户，查询到age&#x3D;10，再次读取发现结果&#x3D;20，在同一个事务里同一个查询读取到不同的结果叫做不可重复读。 repeatable read 可重复复读，这是mysql的默认级别，就是每次读取结果都一样，但是有可能产生幻读。 serializable 串行，一般是不会使用的，他会给每一行读取的数据加锁，会导致大量超时和锁竞争的问题。 6. 那ACID靠什么保证的呢？A原子性由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性一般由代码层面来保证 I隔离性由MVCC来保证 D持久性由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 7. 那你说说什么是幻读，什么是MVCC？要说幻读，首先要了解MVCC，MVCC叫做多版本并发控制，实际上就是保存了数据在某个时间节点的快照。 我们每行数据实际上隐藏了两列，创建时间版本号，过期(删除)时间版本号，每开始一个新的事务，版本号都会自动递增。 还是拿上面的user表举例子，假设我们插入两条数据，他们实际上应该长这样。 id name create_version delete_version 1 张三 1 2 李四 2 这时候假设小明去执行查询，此时current_version&#x3D;3 1select * from user where id&lt;=3; 同时，小红在这时候开启事务去修改id&#x3D;1的记录，current_version&#x3D;4 1update user set name=&#x27;张三三&#x27; where id=1; 执行成功后的结果是这样的 id name create_version delete_version 1 张三 1 2 李四 2 1 张三三 4 如果这时候还有小黑在删除id&#x3D;2的数据，current_version&#x3D;5，执行后结果是这样的。 id name create_version delete_version 1 张三 1 2 李四 2 5 1 张三三 4 由于MVCC的原理是查找创建版本小于或等于当前事务版本，删除版本为空或者大于当前事务版本，小明的真实的查询应该是这样 1select * from user where id&lt;=3 and create_version&lt;=3 and (delete_version&gt;3 or delete_version is null); 所以小明最后查询到的id&#x3D;1的名字还是’张三’，并且id&#x3D;2的记录也能查询到。这样做是为了保证事务读取的数据是在事务开始前就已经存在的，要么是事务自己插入或者修改的。 明白MVCC原理，我们来说什么是幻读就简单多了。举一个常见的场景，用户注册时，我们先查询用户名是否存在，不存在就插入，假定用户名是唯一索引。 小明开启事务current_version&#x3D;6查询名字为’王五’的记录，发现不存在。 小红开启事务current_version&#x3D;7插入一条数据，结果是这样： id Name create_version delete_version 1 张三 1 2 李四 2 3 王五 7 小明执行插入名字’王五’的记录，发现唯一索引冲突，无法插入，这就是幻读。 8. 那你知道什么是间隙锁吗？间隙锁是可重复读级别下才会有的锁，结合MVCC和间隙锁可以解决幻读的问题。我们还是以user举例，假设现在user表有几条记录 id Age 1 10 2 20 3 30 当我们执行： 123456789begin;select * from user where age=20 for update;begin;insert into user(age) values(10); #成功insert into user(age) values(11); #失败insert into user(age) values(20); #失败insert into user(age) values(21); #失败insert into user(age) values(30); #失败 只有10可以插入成功，那么因为表的间隙mysql自动帮我们生成了区间(左开右闭) 1(negative infinity，10],(10,20],(20,30],(30,positive infinity) 由于20存在记录，所以(10,20]，(20,30]区间都被锁定了无法插入、删除。 如果查询21呢？就会根据21定位到(20,30)的区间(都是开区间)。 需要注意的是唯一索引是不会有间隙索引的。 9. 你们数据量级多大？分库分表怎么做的？首先分库分表分为垂直和水平两个方式，一般来说我们拆分的顺序是先垂直后水平。 垂直分库 基于现在微服务拆分来说，都是已经做到了垂直分库了 垂直分表 如果表字段比较多，将不常用的、数据较大的等等做拆分 水平分表 首先根据业务场景来决定使用什么字段作为分表字段(sharding_key)，比如我们现在日订单1000万，我们大部分的场景来源于C端，我们可以用user_id作为sharding_key，数据查询支持到最近3个月的订单，超过3个月的做归档处理，那么3个月的数据量就是9亿，可以分1024张表，那么每张表的数据大概就在100万左右。 比如用户id为100，那我们都经过hash(100)，然后对1024取模，就可以落到对应的表上了。 10. 那分表后的ID怎么保证唯一性的呢？因为我们主键默认都是自增的，那么分表之后的主键在不同表就肯定会有冲突了。有几个办法考虑： 设定步长，比如1-1024张表我们设定1024的基础步长，这样主键落到不同的表就不会冲突了。 分布式ID，自己实现一套分布式ID生成算法或者使用开源的比如雪花算法这种 分表后不使用主键作为查询依据，而是每张表单独新增一个字段作为唯一主键使用，比如订单表订单号是唯一的，不管最终落在哪张表都基于订单号作为查询依据，更新也一样。 11. 分表后非sharding_key的查询怎么处理呢？ 可以做一个mapping表，比如这时候商家要查询订单列表怎么办呢？不带user_id查询的话你总不能扫全表吧？所以我们可以做一个映射关系表，保存商家和用户的关系，查询的时候先通过商家查询到用户列表，再通过user_id去查询。 打宽表，一般而言，商户端对数据实时性要求并不是很高，比如查询订单列表，可以把订单表同步到离线（实时）数仓，再基于数仓去做成一张宽表，再基于其他如es提供查询服务。 数据量不是很大的话，比如后台的一些查询之类的，也可以通过多线程扫表，然后再聚合结果的方式来做。或者异步的形式也是可以的。 123456789101112131415161718192021List&lt;Callable&lt;List&lt;User&gt;&gt;&gt; taskList = Lists.newArrayList();for (int shardingIndex = 0; shardingIndex &lt; 1024; shardingIndex++) &#123; taskList.add(() -&gt; (userMapper.getProcessingAccountList(shardingIndex)));&#125;List&lt;ThirdAccountInfo&gt; list = null;try &#123; list = taskExecutor.executeTask(taskList);&#125; catch (Exception e) &#123; //do something&#125;public class TaskExecutor &#123; public &lt;T&gt; List&lt;T&gt; executeTask(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws Exception &#123; List&lt;T&gt; result = Lists.newArrayList(); List&lt;Future&lt;T&gt;&gt; futures = ExecutorUtil.invokeAll(tasks); for (Future&lt;T&gt; future : futures) &#123; result.add(future.get()); &#125; return result; &#125;&#125; 12. 说说mysql主从同步怎么做的吧？首先先了解mysql主从同步的原理 master提交完事务后，写入binlog slave连接到master，获取binlog master创建dump线程，推送binglog到slave slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中 slave再开启一个sql线程读取relay log事件并在slave执行，完成同步 slave记录自己的binglog 由于mysql默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已经处理，这样会产生一个问题就是假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。由此产生两个概念。 全同步复制 主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。 半同步复制 和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。 13. 那主从的延迟怎么解决呢？ 针对特定的业务场景，读写请求都强制走主库 读请求走从库，如果没有数据，去主库做二次查询"},{"title":"JVM篇","path":"/wiki/interview/JVM篇.html","content":"JVM内存划分1、JVM运行时数据区域堆、方法区（元空间）、虚拟机栈、本地方法栈、程序计数器 Heap(堆)： 对象的实例以及数组的内存都是要在堆上进行分配的，堆是线程共享的一块区域，用来存放对象实例，也是垃圾回收（GC）的主要区域；开启逃逸分析后，某些未逃逸的对象可以通过标量替换的方式在栈中分配 堆细分：新生代、老年代，对于新生代又分为：Eden区和Surviver1和Surviver2区； 方法区： 对于JVM的方法区也可以称之为永久区，它储存的是已经被java虚拟机加载的类信息、常量、静态变量；Jdk1.8以后取消了方法区这个概念，称之为元空间（MetaSpace）； 当应用中的 Java 类过多时，比如 Spring 等一些使用动态代理的框架生成了很多类，如果占用空间超出了我们的设定值，就会发生元空间溢出 虚拟机栈： 虚拟机栈是线程私有的，他的生命周期和线程的生命周期是一致的。里面装的是一个一个的栈帧，每一个方法在执行的时候都会创建一个栈帧，栈帧中用来存放（局部变量表、操作数栈 、动态链接 、返回地址）；在Java虚拟机规范中，对此区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将会抛出StackOverflowError异常；如果虚拟机栈动态扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 局部变量表：局部变量表是一组变量值存储空间，用来存放方法参数、方法内部定义的局部变量。底层是变量槽（variable slot） 操作数栈：是用来记录一个方法在执行的过程中，字节码指令向操作数栈中进行入栈和出栈的过程。大小在编译的时候已经确定了，当一个方法刚开始执行的时候，操作数栈中是空发的，在方法执行的过程中会有各种字节码指令往操作数栈中入栈和出栈。 动态链接：因为字节码文件中有很多符号的引用，这些符号引用一部分会在类加载的解析阶段或第一次使用的时候转化成直接引用，这种称为静态解析；另一部分会在运行期间转化为直接引用，称为动态链接。 返回地址（returnAddress）：类型（指向了一条字节码指令的地址）JIT即时编译器（Just In Time Compiler），简称 JIT 编译器:为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，比如锁粗化等 本地方法栈： 本地方法栈和虚拟机栈类似，不同的是虚拟机栈服务的是Java方法，而本地方法栈服务的是Native方法。在HotSpot虚拟机实现中是把本地方法栈和虚拟机栈合二为一的，同理它也会抛出StackOverflowError和OOM异常。 PC程序计数器： PC，指的是存放下一条指令的位置的一个指针。它是一块较小的内存空间，且是线程私有的。由于线程的切换，CPU在执行的过程中，需要记住原线程的下一条指令的位置，所以每一个线程都需要有自己的PC。 2、堆内存分配策略 对象优先分配在Eden区，如果Eden区没有足够的空间进行分配时，虚拟机执行一次MinorGC。而那些无需回收的存活对象，将会进到 Survivor 的 From 区（From 区内存不足时，直接进入 Old 区）。 大对象直接进入老年代（需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄（Age Count）计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，直到达到阀值（默认15次），对象进入老年区。（动态对象年龄判定：程序从年龄最小的对象开始累加，如果累加的对象大小，大于幸存区的一半，则将当前的对象 age 作为新的阈值，年龄大于此阈值的对象则直接进入老年代） 每次进行Minor GC或者大对象直接进入老年区时，JVM会计算所需空间大小如小于老年区的剩余值大小，则进行一次Full GC。 3、创建一个对象的步骤步骤：类加载检查、分配内存、初始化零值、设置对象头、执行init方法 ①类加载检查： 虚拟机遇到 new 指令时，⾸先去检查是否能在常量池中定位到这个类的符号引⽤，并且检查这个符号引⽤代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执⾏相应的类加载过程。 ②分配内存： 在类加载检查通过后，接下来虚拟机将为新⽣对象分配内存，分配⽅式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配⽅式由 Java 堆是否规整决定，⽽Java堆是否规整⼜由所采⽤的垃圾收集器是否带有压缩整理功能决定。 ③初始化零值： 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值，这⼀步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使⽤，程序能访问到这些字段的数据类型所对应的零值。 ④设置对象头： 初始化零值完成之后，虚拟机要对对象进⾏必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运⾏状态的不同，如是否启⽤偏向锁等，对象头会有不同的设置⽅式。 ⑤执⾏ init ⽅法： 从虚拟机的视⻆来看，⼀个新的对象已经产⽣了，但从Java 程序的视⻆来看， ⽅法还没有执⾏，所有的字段都还为零。所以⼀般来说（除循环依赖），执⾏ new 指令之后会接着执⾏ ⽅法，这样⼀个真正可⽤的对象才算产⽣出来。 4、对象引用普通的对象引用关系就是强引用。 软引用用于维护一些可有可无的对象。只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。 弱引用对象相比软引用来说，要更加无用一些，它拥有更短的生命周期，当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 虚引用是一种形同虚设的引用，在现实场景中用的不是很多，它主要用来跟踪对象被垃圾回收的活动。 JVM类加载过程过程：加载、验证、准备、解析、初始化 加载阶段： 1.通过一个类的全限定名来获取定义此类的二进制字节流。 2.将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 3.在Java堆中生成一个代表这个类的java.lang.class对象，作为方法区这些数据的访问入口。 验证阶段： 1.文件格式验证（是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理） 2.元数据验证（对字节码描述的信息进行语意分析，以保证其描述的信息符合Java语言规范要求） 3.字节码验证（保证被校验类的方法在运行时不会做出危害虚拟机安全的行为） 4.符号引用验证（虚拟机将符号引用转化为直接引用时，解析阶段中发生） 准备阶段： 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段。将对象初始化为“零”值 解析阶段： 解析阶段时虚拟机将常量池内的符号引用替换为直接引用的过程。 字符串常量池：堆上，默认class文件的静态常量池 运行时常量池：在方法区，属于元空间 初始化阶段： 初始化阶段时加载过程的最后一步，而这一阶段也是真正意义上开始执行类中定义的Java程序代码。 1、双亲委派机制每⼀个类都有⼀个对应它的类加载器。系统中的 ClassLoder 在协同⼯作的时候会默认使⽤ 双亲委派模型 。即在类加载的时候，系统会⾸先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，⾸先会把该请求委派该⽗类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当⽗类加载器⽆法处理时，才由⾃⼰来处理。当⽗类加载器为null时，会使⽤启动类加载器 BootstrapClassLoader 作为⽗类加载器。 使用好处： 此机制保证JDK核心类的优先加载；使得Java程序的稳定运⾏，可以避免类的重复加载，也保证了 Java 的核⼼ API 不被篡改。如果不⽤没有使⽤双亲委派模型，⽽是每个类加载器加载⾃⼰的话就会出现⼀些问题，⽐如我们编写⼀个称为 java.lang.Object 类的话，那么程序运⾏的时候，系统就会出现多个不同的Object 类。 破坏双亲委派机制： 可以⾃⼰定义⼀个类加载器，重写loadClass方法； Tomcat 可以加载自己目录下的 class 文件，并不会传递给父类的加载器； Java 的 SPI，发起者 BootstrapClassLoader 已经是最上层了，它直接获取了 AppClassLoader 进行驱动加载，和双亲委派是相反的。 2、tomcat的类加载机制步骤： 先在本地cache查找该类是否已经加载过，看看 Tomcat 有没有加载过这个类。 如果Tomcat 没有加载过这个类，则从系统类加载器的cache中查找是否加载过。 如果没有加载过这个类，尝试用ExtClassLoader类加载器类加载，重点来了，这里并没有首先使用 AppClassLoader 来加载类。这个Tomcat 的 WebAPPClassLoader 违背了双亲委派机制，直接使用了 ExtClassLoader来加载类。这里注意 ExtClassLoader 双亲委派依然有效，ExtClassLoader 就会使用 Bootstrap ClassLoader 来对类进行加载，保证了 Jre 里面的核心类不会被重复加载。 比如在 Web 中加载一个 Object 类。WebAppClassLoader → ExtClassLoader → Bootstrap ClassLoader，这个加载链，就保证了 Object 不会被重复加载。 如果 BoostrapClassLoader，没有加载成功，就会调用自己的 findClass 方法由自己来对类进行加载，findClass 加载类的地址是自己本 web 应用下的 class。 加载依然失败，才使用 AppClassLoader 继续加载。 都没有加载成功的话，抛出异常。 总结一下以上步骤，WebAppClassLoader 加载类的时候，故意打破了JVM 双亲委派机制，绕开了 AppClassLoader，直接先使用 ExtClassLoader 来加载类。 JVM垃圾回收1、存活算法和两次标记过程引用计数法： 给对象添加一个引用计数器，每当由一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 优点：实现简单，判定效率也很高 缺点：他很难解决对象之间相互循环引用的问题，基本上被抛弃 可达性分析法： 通过一系列的成为“GC Roots”(活动线程相关的各种引用，虚拟机栈帧引用，静态变量引用，JNI引用)的对象作为起始点，从这些节点ReferenceChains开始向下搜索，搜索所走过的路径成为引用链，当一个对象到GC ROOTS没有任何引用链相连时，则证明此对象时不可用的； 两次标记过程： 对象被回收之前，该对象的finalize()方法会被调用；两次标记，即第一次标记不在“关系网”中的对象。第二次的话就要先判断该对象有没有实现finalize()方法了，如果没有实现就直接判断该对象可回收；如果实现了就会先放在一个队列中，并由虚拟机建立的一个低优先级的线程去执行它，随后就会进行第二次的小规模标记，在这次被标记的对象就会真正的被回收了。 2、垃圾回收算法垃圾回收算法：复制算法、标记清除、标记整理、分代收集 复制算法：(young) 将内存分为⼤⼩相同的两块，每次使⽤其中的⼀块。当这⼀块的内存使⽤完后，就将还存活的对象复制到另⼀块去，然后再把使⽤的空间⼀次清理掉。这样就使每次的内存回收都是对内存区间的⼀半进⾏回收； 优点：实现简单，内存效率高，不易产生碎片 缺点：内存压缩了一半，倘若存活对象多，Copying 算法的效率会大大降低 标记清除：(cms) 标记出所有需要回收的对象，在标记完成后统⼀回收所有被标记的对象 缺点：效率低，标记清除后会产⽣⼤量不连续的碎⽚，需要预留空间给分配阶段的浮动垃圾 标记整理：(old) 标记过程仍然与“标记-清除”算法⼀样，再让所有存活的对象向⼀端移动，然后直接清理掉端边界以外的内存；解决了产生大量不连续碎片问题 分代收集： 根据各个年代的特点选择合适的垃圾收集算法。 新生代采用复制算法，新生代每次垃圾回收都要回收大部分对象，存活对象较少，即要复制的操作比较少，一般将新生代划分为一块较大的 Eden 空间和两个较小的 Survivor 空间(From Space, To Space)，每次使用Eden 空间和其中的一块 Survivor 空间，当进行回收时，将该两块空间中还存活的对象复制到另一块 Survivor 空间中。 老年代的对象存活⼏率是⽐较⾼的，⽽且没有额外的空间对它进⾏分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进⾏垃圾收集。 Safepoint 当发生 GC 时，用户线程必须全部停下来，才可以进行垃圾回收，这个状态我们可以认为 JVM 是安全的（safe），整个堆的状态是稳定的。如果在 GC 前，有线程迟迟进入不了 safepoint，那么整个 JVM 都在等待这个阻塞的线程，造成了整体 GC 的时间变长 MinorGC、MajorGC、FullGCMinorGC 在年轻代空间不足的时候发生， MajorGC 指的是老年代的 GC，出现 MajorGC 一般经常伴有 MinorGC。 FullGC 1、当老年代无法再分配内存的时候；2、元空间不足的时候；3、显示调用 System.gc 的时候。另外，像 CMS 一类的垃圾回收器，在 MinorGC 出现 promotion failure 的时候也会发生 FullGC。 对象优先在 Eden 区分配大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 大对象直接进入老年代大对象是指需要连续内存空间的对象，比如很长的字符串以及数组。老年代直接分配的目的是避免在 Eden 区和 Survivor 区之间出现大量内存复制。 长期存活的对象进入老年代虚拟机给每个对象定义了年龄计数器，对象在 Eden 区出生之后，如果经过一次 Minor GC 之后，将进入 Survivor 区，同时对象年龄变为 1，增加到一定阈值时则进入老年代（阈值默认为 15） 动态对象年龄判定为了能更好地适应不同程序的内存状况，虚拟机并不总是要求对象的年龄必须达到阈值才能进入老年代。如果在 Survivor 区中相同年龄的所有对象的空间总和大于 Survivor 区空间的一半，则年龄大于或等于该年龄的对象直接进入老年代。 空间分配担保在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的空间总和，如果这个条件成立，那么 Minor GC 可以确保是安全的。如果不成立则进行 Full GC。 3、垃圾收集器JDK3：Serial Parnew 关注效率 Serial： Serial 是一个单线程的收集器，它不但只会使用一个 CPU 或一条线程去完成垃圾收集工作，并且在进行垃圾收集的同时，必须暂停其他所有的工作线程，直到垃圾收集结束。适合用于客户端垃圾收集器。 Parnew： ParNew 垃圾收集器其实是 Serial 收集器的多线程版本，也使用复制算法，除了使用多线程进行垃圾收集之外，其余的行为和 Serial 收集器完全一样，ParNew 垃圾收集器在垃圾收集过程中同样也要暂停所有其他的工作线程。 JDK5：parallel Scavenge+（Serial old&#x2F;parallel old）关注吞吐量 parallel Scavenge：(关注吞吐量) Parallel Scavenge收集器关注点是吞吐量（⾼效率的利⽤CPU）。CMS等垃圾收集器的关注点更多的是⽤户线程的停顿时间（提⾼⽤户体验）；高吞吐量可以最高效率地利用 CPU 时间，尽快地完成程序的运算任务，主要适用于在后台运算而不需要太多交互的任务。 Serial old： Serial收集器的⽼年代版本，它同样是⼀个单线程收集器，使用标记-整理算法。主要有两个用途： 在 JDK1.5 之前版本中与新生代的 Parallel Scavenge 收集器搭配使用。 作为年老代中使用 CMS 收集器的后备垃圾收集方案。 parallel old： Parallel Scavenge收集器的⽼年代版本。使⽤多线程和“标记-整理”算法。 JDK8-CMS：（关注最短垃圾回收停顿时间） CMS收集器是一种年老代垃圾收集器，其最主要目标是获取最短垃圾回收停顿时间，和其他年老代使用标记-整理算法不同，它使用多线程的标记-清除算法。最短的垃圾收集停顿时间可以为交互比较高的程序提高用户体验。CMS 工作机制相比其他的垃圾收集器来说更复杂，整个过程分为以下 4 个阶段： 初始标记：只是标记一下 GC Roots 能直接关联的对象，速度很快，STW。 并发标记：进行 ReferenceChains跟踪的过程，和用户线程一起工作，不需要暂停工作线程。 重新标记：为了修正在并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，STW。 并发清除：清除 GC Roots 不可达对象，和用户线程一起工作，不需要暂停工作线程。 由于耗时最长的并发标记和并发清除过程中，垃圾收集线程可以和用户现在一起并发工作，所以总体上来看CMS 收集器的内存回收和用户线程是一起并发地执行。 优点：并发收集、低停顿 缺点：对CPU资源敏感；⽆法处理浮动垃圾；使⽤“标记清除”算法，会导致⼤量空间碎⽚产⽣。 JDK9-G1：（精准控制停顿时间，避免垃圾碎片） 是⼀款⾯向服务器的垃圾收集器,主要针对配备多颗处理器及⼤容量内存的机器.以极⾼概率满⾜GC停顿时间要求的同时,还具备⾼吞吐量性能特征；相比与 CMS 收集器，G1 收集器两个最突出的改进是： 【1】基于标记-整理算法，不产生内存碎片。 【2】可以非常精确控制停顿时间，在不牺牲吞吐量前提下，实现低停顿垃圾回收。 G1 收集器避免全区域垃圾收集，它把堆内存划分为大小固定的几个独立区域，并且跟踪这些区域的垃圾收集进度，同时在后台维护一个优先级列表，每次根据所允许的收集时间，优先回收垃圾最多的区域。区域划分和优先级区域回收机制，确保 G1 收集器可以在有限时间获得最高的垃圾收集效率。 初始标记：Stop The World，仅使用一条初始标记线程对GC Roots关联的对象进行标记 并发标记：使用一条标记线程与用户线程并发执行。此过程进行可达性分析，速度很慢 最终标记：Stop The World，使用多条标记线程并发执行 筛选回收：回收废弃对象，此时也要 Stop The World，并使用多条筛选回收线程并发执行 **JDK11-ZGC:**（在不关注容量的情况获取最小停顿时间5TB&#x2F;10ms） 着色笔技术：加快标记过程 读屏障：解决GC和应用之间并发导致的STW问题 支持 TB 级堆内存（最大 4T， JDK13 最大16TB） 最大 GC 停顿 10ms 对吞吐量影响最大，不超过 15% 4、配置垃圾收集器 首先是内存大小问题，基本上每一个内存区域我都会设置一个上限，来避免溢出问题，比如元空间。 通常，堆空间我会设置成操作系统的 2&#x2F;3，超过 8GB 的堆，优先选用 G1 然后我会对 JVM 进行初步优化，比如根据老年代的对象提升速度，来调整年轻代和老年代之间的比例 依据系统容量、访问延迟、吞吐量等进行专项优化，我们的服务是高并发的，对 STW 的时间敏感 我会通过记录详细的 GC 日志，来找到这个瓶颈点，借用 GCeasy 这样的日志分析工具，定位问题 4、JVM性能调优对应进程的JVM状态以定位问题和解决问题并作出相应的优化 常用命令：jps、jinfo、jstat、jstack、jmap jps：查看java进程及相关信息 jinfo：查看JVM参数 jstat：查看JVM运行时的状态信息，包括内存状态、垃圾回收 jstack：查看JVM线程快照，jstack命令可以定位线程出现长时间卡顿的原因，例如死锁，死循环 jmap：可以用来查看内存信息(配合jhat使用) 5、JDK新特性JDK8 支持 Lamda 表达式、集合的 stream 操作、提升HashMap性能 JDK9 默认G1垃圾回收器 JDK10 其重点在于通过完全GC并行来改善G1最坏情况的等待时间。 JDK11 ZGC (并发回收的策略) 4TB 用于 Lambda 参数的局部变量语法 JDK12 Shenandoah GC (GC 算法)停顿时间和堆的大小没有任何关系，并行关注停顿响应时间。 JDK13 增加ZGC以将未使用的堆内存返回给操作系统，16TB JDK14 删除cms垃圾回收器、弃用ParallelScavenge+SerialOldGC垃圾回收算法组合 将ZGC垃圾回收器应用到macOS和windows平台 线上故障排查1、硬件故障排查如果一个实例发生了问题，根据情况选择，要不要着急去重启。如果出现的CPU、内存飙高或者日志里出现了OOM异常 第一步是隔离，第二步是保留现场，第三步才是问题排查。 隔离 就是把你的这台机器从请求列表里摘除，比如把 nginx 相关的权重设成零。 现场保留 瞬时态和历史态 查看比如 CPU、系统内存等，通过历史状态可以体现一个趋势性问题，而这些信息的获取一般依靠监控系统的协作。 保留信息 （1）系统当前网络连接 使用 ss 命令而不是 netstat 的原因，是因为 netstat 在网络连接非常多的情况下，执行非常缓慢。 后续的处理，可通过查看各种网络连接状态的梳理，来排查 TIME_WAIT 或者 CLOSE_WAIT，或者其他连接过高的问题，非常有用。 （2）网络状态统计 它能够按照各个协议进行统计输出，对把握当时整个网络状态，有非常大的作用。 在一些速度非常高的模块上，比如 Redis、Kafka，就经常发生跑满网卡的情况。表现形式就是网络通信非常缓慢。 （3）进程资源 通过查看进程，能看到打开了哪些文件，可以以进程的维度来查看整个资源的使用情况，包括每条网络连接、每个打开的文件句柄。同时，也可以很容易的看到连接到了哪些服务器、使用了哪些资源。这个命令在资源非常多的情况下，输出稍慢，请耐心等待。 （4）CPU 资源 主要用于输出当前系统的 CPU 和负载，便于事后排查。 （5）I&#x2F;O 资源 一般，以计算为主的服务节点，I&#x2F;O 资源会比较正常，但有时也会发生问题，比如日志输出过多，或者磁盘问题等。此命令可以输出每块磁盘的基本性能信息，用来排查 I&#x2F;O 问题。在第 8 课时介绍的 GC 日志分磁盘问题，就可以使用这个命令去发现。 （6）内存问题 free 命令能够大体展现操作系统的内存概况，这是故障排查中一个非常重要的点，比如 SWAP 影响了 GC，SLAB 区挤占了 JVM 的内存。 （7）其他全局 dmesg 是许多静悄悄死掉的服务留下的最后一点线索。当然，ps 作为执行频率最高的一个命令，由于内核的配置参数，会对系统和 JVM 产生影响，所以我们也输出了一份。 （8）进程快照，最后的遗言（jinfo） 此命令将输出 Java 的基本进程信息，包括环境变量和参数配置，可以查看是否因为一些错误的配置造成了 JVM 问题。 （9）dump 堆信息 jstat 将输出当前的 gc 信息。一般，基本能大体看出一个端倪，如果不能，可将借助 jmap 来进行分析。 （10）堆信息 jmap 将会得到当前 Java 进程的 dump 信息。如上所示，其实最有用的就是第 4 个命令，但是前面三个能够让你初步对系统概况进行大体判断。因为，第 4 个命令产生的文件，一般都非常的大。而且，需要下载下来，导入 MAT 这样的工具进行深入分析，才能获取结果。这是分析内存泄漏一个必经的过程。 （11）JVM 执行栈 jstack 将会获取当时的执行栈。一般会多次取值，我们这里取一次即可。这些信息非常有用，能够还原 Java 进程中的线程情况。 为了能够得到更加精细的信息，我们使用 top 命令，来获取进程中所有线程的 CPU 信息，这样，就可以看到资源到底耗费在什么地方了。 （12）高级替补 有时候，jstack 并不能够运行，有很多原因，比如 Java 进程几乎不响应了等之类的情况。我们会尝试向进程发送 kill -3 信号，这个信号将会打印 jstack 的 trace 信息到日志文件中，是 jstack 的一个替补方案。 对于 jmap 无法执行的问题，也有替补，那就是 GDB 组件中的 gcore，将会生成一个 core 文件。我们可以使用如下的命令去生成 dump： 内存泄漏的现象 稍微提一下 jmap 命令，它在 9 版本里被干掉了，取而代之的是 jhsdb，你可以像下面的命令一样使用。 一般内存溢出，表现形式就是 Old 区的占用持续上升，即使经过了多轮 GC 也没有明显改善。比如ThreadLocal里面的GC Roots，内存泄漏的根本就是，这些对象并没有切断和 GC Roots 的关系，可通过一些工具，能够看到它们的联系。 2、报表异常 | JVM调优有一个报表系统，频繁发生内存溢出，在高峰期间使用时，还会频繁的发生拒绝服务，由于大多数使用者是管理员角色，所以很快就反馈到研发这里。 业务场景是由于有些结果集的字段不是太全，因此需要对结果集合进行循环，并通过 HttpClient 调用其他服务的接口进行数据填充。使用 Guava 做了 JVM 内缓存，但是响应时间依然很长。 初步排查，JVM 的资源太少。接口 A 每次进行报表计算时，都要涉及几百兆的内存，而且在内存里驻留很长时间，有些计算又非常耗 CPU，特别的“吃”资源。而我们分配给 JVM 的内存只有 3 GB，在多人访问这些接口的时候，内存就不够用了，进而发生了 OOM。在这种情况下，没办法，只有升级机器。把机器配置升级到 4C8G，给 JVM 分配 6GB 的内存，这样 OOM 问题就消失了。但随之而来的是频繁的 GC 问题和超长的 GC 时间，平均 GC 时间竟然有 5 秒多。 进一步，由于报表系统和高并发系统不太一样，它的对象，存活时长大得多，并不能仅仅通过增加年轻代来解决；而且，如果增加了年轻代，那么必然减少了老年代的大小，由于 CMS 的碎片和浮动垃圾问题，我们可用的空间就更少了。虽然服务能够满足目前的需求，但还有一些不太确定的风险。 第一，了解到程序中有很多缓存数据和静态统计数据，为了减少 MinorGC 的次数，通过分析 GC 日志打印的对象年龄分布，把 MaxTenuringThreshold 参数调整到了 3（特殊场景特殊的配置）。这个参数是让年轻代的这些对象，赶紧回到老年代去，不要老呆在年轻代里。 第二，我们的 GC 时间比较长，就一块开了参数 CMSScavengeBeforeRemark，使得在 CMS remark 前，先执行一次 Minor GC 将新生代清掉。同时配合上个参数，其效果还是比较好的，一方面，对象很快晋升到了老年代，另一方面，年轻代的对象在这种情况下是有限的，在整个 MajorGC 中占的时间也有限。 第三，由于缓存的使用，有大量的弱引用，拿一次长达 10 秒的 GC 来说。我们发现在 GC 日志里，处理 weak refs 的时间较长，达到了 4.5 秒。这里可以加入参数 ParallelRefProcEnabled 来并行处理Reference，以加快处理速度，缩短耗时。 优化之后，效果不错，但并不是特别明显。经过评估，针对高峰时期的情况进行调研，我们决定再次提升机器性能，改用 8core16g 的机器。但是，这带来另外一个问题。 高性能的机器带来了非常大的服务吞吐量，通过 jstat 进行监控，能够看到年轻代的分配速率明显提高，但随之而来的 MinorGC 时长却变的不可控，有时候会超过 1 秒。累积的请求造成了更加严重的后果。 这是由于堆空间明显加大造成的回收时间加长。为了获取较小的停顿时间，我们在堆上改用了 G1 垃圾回收器，把它的目标设定在 200ms。G1 是一款非常优秀的垃圾收集器，不仅适合堆内存大的应用，同时也简化了调优的工作。通过主要的参数初始和最大堆空间、以及最大容忍的 GC 暂停目标，就能得到不错的性能。修改之后，虽然 GC 更加频繁了一些，但是停顿时间都比较小，应用的运行较为平滑。 到目前为止，也只是勉强顶住了已有的业务，但是，这时候领导层面又发力，要求报表系统可以支持未来两年业务10到100倍的增长，并保持其可用性，但是这个“千疮百孔”的报表系统，稍微一压测，就宕机，那如何应对十倍百倍的压力呢 ? 硬件即使可以做到动态扩容，但是毕竟也有极限。 使用 MAT 分析堆快照，发现很多地方可以通过代码优化，那些占用内存特别多的对象： 1、select * 全量排查，只允许获取必须的数据 2、报表系统中cache实际的命中率并不高，将Guava 的 Cache 引用级别改成弱引用（WeakKeys） 3、限制报表导入文件大小，同时拆分用户超大范围查询导出请求。 每一步操作都使得JVM使用变得更加可用，一系列优化以后，机器相同压测数据性能提升了数倍。 3、大屏异常 | JUC调优有些数据需要使用 HttpClient 来获取进行补全。提供数据的服务提供商有的响应时间可能会很长，也有可能会造成服务整体的阻塞。 接口 A 通过 HttpClient 访问服务 2，响应 100ms 后返回；接口 B 访问服务 3，耗时 2 秒。HttpClient 本身是有一个最大连接数限制的，如果服务 3 迟迟不返回，就会造成 HttpClient 的连接数达到上限，概括来讲，就是同一服务，由于一个耗时非常长的接口，进而引起了整体的服务不可用 这个时候，通过 jstack 打印栈信息，会发现大多数竟然阻塞在了接口 A 上，而不是耗时更长的接口 B，这个现象起初十分具有迷惑性，不过经过分析后，我们猜想其实是因为接口 A 的速度比较快，在问题发生点进入了更多的请求，它们全部都阻塞住的同时被打印出来了。 为了验证这个问题，我搭建了一个demo 工程，模拟了两个使用同一个 HttpClient 的接口。fast 接口用来访问百度，很快就能返回；slow 接口访问谷歌，由于众所周知的原因，会阻塞直到超时，大约 10 s。 利用ab对两个接口进行压测，同时使用 jstack 工具 dump 堆栈。首先使用 jps 命令找到进程号，然后把结果重定向到文件（可以参考 10271.jstack 文件）。 过滤一下 nio 关键字，可以查看 tomcat 相关的线程，足足有 200 个，这和 Spring Boot 默认的 maxThreads 个数不谋而合。更要命的是，有大多数线程，都处于 BLOCKED 状态，说明线程等待资源超时。通过grep fast | wc -l 分析，确实200个中有150个都是blocked的fast的进程。 问题找到了，解决方式就顺利成章了。 1、fast和slow争抢连接资源，通过线程池限流或者熔断处理 2、有时候slow的线程也不是一直slow，所以就得加入监控 3、使用带countdownLaunch对线程的执行顺序逻辑进行控制 4、接口延迟 | SWAP调优有一个关于服务的某个实例，经常发生服务卡顿。由于服务的并发量是比较高的，每多停顿 1 秒钟，几万用户的请求就会感到延迟。 我们统计、类比了此服务其他实例的 CPU、内存、网络、I&#x2F;O 资源，区别并不是很大，所以一度怀疑是机器硬件的问题。 接下来我们对比了节点的 GC 日志，发现无论是 Minor GC，还是 Major GC，这个节点所花费的时间，都比其他实例长得多。 通过仔细观察，我们发现在 GC 发生的时候，vmstat 的 si、so 飙升的非常严重，这和其他实例有着明显的不同。 使用 free 命令再次确认，发现 SWAP 分区，使用的比例非常高，引起的具体原因是什么呢？ 更详细的操作系统内存分布，从 &#x2F;proc&#x2F;meminfo 文件中可以看到具体的逻辑内存块大小，有多达 40 项的内存信息，这些信息都可以通过遍历 &#x2F;proc 目录的一些文件获取。我们注意到 slabtop 命令显示的有一些异常，dentry（目录高速缓冲）占用非常高。 问题最终定位到是由于某个运维工程师删除日志时，定时执行了一句命令： find &#x2F; | grep “xxx.log” 他是想找一个叫做 要被删除 的日志文件，看看在哪台服务器上，结果，这些老服务器由于文件太多，扫描后这些文件信息都缓存到了 slab 区上。而服务器开了 swap，操作系统发现物理内存占满后，并没有立即释放 cache，导致每次 GC 都要和硬盘打一次交道。 解决方式就是关闭 SWAP 分区。 swap 是很多性能场景的万恶之源，建议禁用。在高并发 SWAP 绝对能让你体验到它魔鬼性的一面：进程倒是死不了了，但 GC 时间长的却让人无法忍受。 5、内存溢出 | Cache调优有一次线上遇到故障，重新启动后，使用 jstat 命令，发现 Old 区一直在增长。我使用 jmap 命令，导出了一份线上堆栈，然后使用 MAT 进行分析，通过对 GC Roots 的分析，发现了一个非常大的 HashMap 对象，这个原本是其他同事做缓存用的，但是做了一个无界缓存，没有设置超时时间或者 LRU 策略，在使用上又没有重写key类对象的hashcode和equals方法，对象无法取出也直接造成了堆内存占用一直上升，后来，将这个缓存改成 guava 的 Cache，并设置了弱引用，故障就消失了。 关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。 内存溢出是一个结果，而内存泄漏是一个原因。内存溢出的原因有内存空间不足、配置错误等因素。一些错误的编程方式，不再被使用的对象、没有被回收、没有及时切断与 GC Roots 的联系，这就是内存泄漏。 举个例子，有团队使用了 HashMap 做缓存，但是并没有设置超时时间或者 LRU 策略，造成了放入 Map 对象的数据越来越多，而产生了内存泄漏。 再来看一个经常发生的内存泄漏的例子，也是由于 HashMap 产生的。代码如下，由于没有重写 Key 类的 hashCode 和 equals 方法，造成了放入 HashMap 的所有对象都无法被取出来，它们和外界失联了。所以下面的代码结果是 null。 即使提供了 equals 方法和 hashCode 方法，也要非常小心，尽量避免使用自定义的对象作为 Key。 再看一个例子，关于文件处理器的应用，在读取或者写入一些文件之后，由于发生了一些异常，close 方法又没有放在 finally 块里面，造成了文件句柄的泄漏。由于文件处理十分频繁，产生了严重的内存泄漏问题。 6：CPU飙高 | 死循环我们有个线上应用，单节点在运行一段时间后，CPU 的使用会飙升，一旦飙升，一般怀疑某个业务逻辑的计算量太大，或者是触发了死循环（比如著名的 HashMap 高并发引起的死循环），但排查到最后其实是 GC 的问题。 （1）使用 top 命令，查找到使用 CPU 最多的某个进程，记录它的 pid。使用 Shift + P 快捷键可以按 CPU 的使用率进行排序。 （2）再次使用 top 命令，加 -H 参数，查看某个进程中使用 CPU 最多的某个线程，记录线程的 ID。 （3）使用 printf 函数，将十进制的 tid 转化成十六进制。 （4）使用 jstack 命令，查看 Java 进程的线程栈。 （5）使用 less 命令查看生成的文件，并查找刚才转化的十六进制 tid，找到发生问题的线程上下文。 我们在 jstack 日志搜关键字DEAD，以及中找到了 CPU 使用最多的几个线程id。 可以看到问题发生的根源，是我们的堆已经满了，但是又没有发生 OOM，于是 GC 进程就一直在那里回收，回收的效果又非常一般，造成 CPU 升高应用假死。接下来的具体问题排查，就需要把内存 dump 一份下来，使用 MAT 等工具分析具体原因了。"},{"title":"Spring知识点汇总","path":"/wiki/interview/Spring知识点汇总.html","content":"在线文章一、Spring面试知识 常见面试题1.说说Spring 里用到了哪些设计模式?单例模式：Spring 中的 Bean 默认情况下都是单例的。无需多说。 工厂模式：工厂模式主要是通过 BeanFactory 和 ApplicationContext 来生产 Bean 对象。 代理模式：最常见的 AOP 的实现方式就是通过代理来实现，Spring主要是使用 JDK 动态代理和 CGLIB 代理。 模板方法模式：主要是一些对数据库操作的类用到，比如 JdbcTemplate、JpaTemplate，因为查询数据库的建立连接、执行查询、关闭连接几个过程，非常适用于模板方法。 2.谈谈你对IOC 和 AOP 的理解？他们的实现原理是什么？IOC 叫做控制反转，指的是通过Spring来管理对象的创建、配置和生命周期，这样相当于把控制权交给了Spring，不需要人工来管理对象之间复杂的依赖关系，这样做的好处就是解耦。在Spring里面，主要提供了 BeanFactory 和 ApplicationContext 两种 IOC 容器，通过他们来实现对 Bean 的管理。 AOP 叫做面向切面编程，他是一个编程范式，目的就是提高代码的模块性。Spring AOP 基于动态代理的方式实现，如果是实现了接口的话就会使用 JDK 动态代理，反之则使用 CGLIB 代理，Spring中 AOP 的应用主要体现在 事务、日志、异常处理等方面，通过在代码的前后做一些增强处理，可以实现对业务逻辑的隔离，提高代码的模块化能力，同时也是解耦。Spring主要提供了 Aspect 切面、JoinPoint 连接点、PointCut 切入点、Advice 增强等实现方式。 3. JDK 动态代理和 CGLIB 代理有什么区别？JDK 动态代理主要是针对类实现了某个接口，AOP 则会使用 JDK 动态代理。他基于反射的机制实现，生成一个实现同样接口的一个代理类，然后通过重写方法的方式，实现对代码的增强。 而如果某个类没有实现接口，AOP 则会使用 CGLIB 代理。他的底层原理是基于 asm 第三方框架，通过修改字节码生成成成一个子类，然后重写父类的方法，实现对代码的增强。 4. Spring AOP 和 AspectJ AOP 有什么区别？Spring AOP 基于动态代理实现，属于运行时增强。 AspectJ 则属于编译时增强，主要有3种方式： 编译时织入：指的是增强的代码和源代码我们都有，直接使用 AspectJ 编译器编译就行了，编译之后生成一个新的类，他也会作为一个正常的 Java 类装载到JVM。 编译后织入：指的是代码已经被编译成 class 文件或者已经打成 jar 包，这时候要增强的话，就是编译后织入，比如你依赖了第三方的类库，又想对他增强的话，就可以通过这种方式。 加载时织入：指的是在 JVM 加载类的时候进行织入。 总结下来的话，就是 Spring AOP 只能在运行时织入，不需要单独编译，性能相比 AspectJ 编译织入的方式慢，而 AspectJ 只支持编译前后和类加载时织入，性能更好，功能更加强大。 5. FactoryBean 和 BeanFactory有什么区别？BeanFactory 是 Bean 的工厂， ApplicationContext 的父类，IOC 容器的核心，负责生产和管理 Bean 对象。 FactoryBean 是 Bean，可以通过实现 FactoryBean 接口定制实例化 Bean 的逻辑，通过代理一个Bean对象，对方法前后做一些操作。 6.SpringBean的生命周期说说？SpringBean 生命周期简单概括为4个阶段： 实例化，创建一个Bean对象 填充属性，为属性赋值 初始化 如果实现了xxxAware接口，通过不同类型的Aware接口拿到Spring容器的资源 如果实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation和postProcessAfterInitialization方法 如果配置了init-method方法，则会执行init-method配置的方法 销毁 容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy方法 如果配置了destroy-method方法，则会执行destroy-method配置的方法 7.Spring是怎么解决循环依赖的？首先，Spring 解决循环依赖有两个前提条件： 不全是构造器方式的循环依赖 必须是单例 基于上面的问题，我们知道Bean的生命周期，本质上解决循环依赖的问题就是三级缓存，通过三级缓存提前拿到未初始化完全的对象。 第一级缓存：用来保存实例化、初始化都完成的对象 第二级缓存：用来保存实例化完成，但是未初始化完成的对象 第三级缓存：用来保存一个对象工厂，提供一个匿名内部类，用于创建二级缓存中的对象 假设一个简单的循环依赖场景，A、B互相依赖。 A对象的创建过程： 创建对象A，实例化的时候把A对象工厂放入三级缓存 A注入属性时，发现依赖B，转而去实例化B 同样创建对象B，注入属性时发现依赖A，一次从一级到三级缓存查询A，从三级缓存通过对象工厂拿到A，把A放入二级缓存，同时删除三级缓存中的A，此时，B已经实例化并且初始化完成，把B放入一级缓存。 接着继续创建A，顺利从一级缓存拿到实例化且初始化完成的B对象，A对象创建也完成，删除二级缓存中的A，同时把A放入一级缓存 最后，一级缓存中保存着实例化、初始化都完成的A、B对象 因此，由于把实例化和初始化的流程分开了，所以如果都是用构造器的话，就没法分离这个操作，所以都是构造器的话就无法解决循环依赖的问题了。 8. 为什么要三级缓存？二级不行吗？不可以，主要是为了生成代理对象。 因为三级缓存中放的是生成具体对象的匿名内部类，他可以生成代理对象，也可以是普通的实例对象。 使用三级缓存主要是为了保证不管什么时候使用的都是一个对象。 假设只有二级缓存的情况，往二级缓存中放的显示一个普通的Bean对象，BeanPostProcessor去生成代理对象之后，覆盖掉二级缓存中的普通Bean对象，那么多线程环境下可能取到的对象就不一致了。 9.Spring事务传播机制有哪些？ PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，这也是通常我们的默认选择。 PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘ 10.最后，说说Spring Boot 启动流程吧？这个流程，网上一搜基本都是这张图了，我也不想再画一遍了。那其实主要的流程就几个步骤： 准备环境，根据不同的环境创建不同的Environment 准备、加载上下文，为不同的环境选择不同的Spring Context，然后加载资源，配置Bean 初始化，这个阶段刷新Spring Context，启动应用 最后结束流程"},{"title":"Redis","path":"/wiki/interview/Redis.html","content":"在线文章一、图解Redis 二、缓存-Redis 站内文章一、Redis核心技术与原理"},{"title":"分库分表和分布式事务解决方案","path":"/wiki/interview/分库分表和分布式事务解决方案.html","content":"第一章 为什么要分库分表1.1 互联网大并发场景业务下对于OLTP的挑战面向拥有超高并发，大规模数据存储的互联网OLTP（Online Transaction Processing，在线事务处理）类业务。同时因计算与数据量不断爆发增长，传统的企业级应用急需更强计算能力的在线事务型数据库。 在线业务超高并发，扛不住！ 海量业务数据，存不下！ 复杂分析查询，性能慢！ 单表数据量过大，效率差！ 跨实例联机查询，搞不定！ 1.2 分库分表才是OLTP的最终形态 通过分库分表，将业务数据及访问压力分摊到多台单机数据库实例上，解决在线业务的超高并发难题。 通过水平拆分可线性扩展数据存储空间，提供 PB 级存储能力。高效解决单机数据库存储瓶颈。 针对在线业务提供 Parallel Query 以及 MPP 并行加速能力，可大幅提升在线业务海量数据下复杂分析查询的执行效率。 数据库单表数量过大后，将导致数据库吞吐能力下降，整体性能迟缓。通过分库分表将单表数据水平拆分至各个MySQL中，有效解决单表数据量膨胀问题。 业务通过使用分库分表即可在不同 RDS 实例多个数据库间进行联合查询及事务操作。可有效避免业务端繁琐复杂的硬代码处理方式，大幅提升业务开发效率。 第二章 为什么会引入分布式事务2.1 分布式事务的典型场景这里举一个分布式事务的典型例子——用户下单过程。 当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单。 此时订单系统会生成一条订单。 订单创建成功后，支付系统提供支付功能。 当支付完成后，由积分系统为该用户增加积分。 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 2.2 分布式事务的根源：微服务2.2.1 什么是微服务简而言之，微服务架构是一种将单应用程序作为一套小型服务开发的方法，每种应用程序都在其自己的进程中运行，并与轻量级机制（通常是HTTP资源的API）进行通信。这些服务是围绕业务功能构建的，可以通过全自动部署机制进行独立部署。这些服务的集中化管理已经是最少的，它们可以用不同的编程语言编写，并使用不同的数据存储技术。 2.2.2 微服务的优势 将复杂的业务拆分成多个小的业务，每个业务拆分成一个服务，将复杂的问题简单化。利于分工，降低新人的学习成本。 微服务系统是分布式系统，业务与业务之间完全解耦，随着业务的增加可以根据业务再拆分，具有极强的横向扩展能力。面对搞并发的场景可以将服务集群化部署，加强系统负载能力。 服务间采用 HTTP 协议通信，服务与服务之间完全独立。每个服务可以根据业务场景选取合适的编程语言和数据库。 微服务每个服务都是独立部署的，每个服务的修改和部署对其他服务没有影响。 2.2.3 微服务落地存在的问题虽然微服务现在如火如荼，但对其实践其实仍处于探索阶段。很多中小型互联网公司，鉴于经验、技术实力等问题，微服务落地比较困难。如著名架构师Chris Richardson所言，目前存在的主要困难有如下几方面： 单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。 系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。 微服务数量众多，其测试、部署、监控等都变的更加困难。 随着RPC框架的成熟，第一个问题已经逐渐得到解决。例如dubbo可以支持多种通讯协议，springcloud可以非常好的支持restful调用。对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。 而对于第二个问题，现在还没有通用方案很好的解决微服务产生的事务问题。分布式事务已经成为微服务落地最大的阻碍，也是最具挑战性的一个技术难题。 为此，本文将深入和大家探讨微服务架构下，分布式事务的各种解决方案。 第三章 一些基本概念和基本原理3.1 读写分离这个相对比较好理解一些，就是将数据库分为主从库，一个主库（Master）用于写数据，多个从库（Slaver）进行轮询读取数据的过程，主从库之间通过某种通讯机制进行数据的同步，是一种常见的数据库架构。下面这张图就展示了 “一主二从” 的结构： 当PolarDB-X存储资源MySQL主实例的读请求较多、读压力比较大时，您可以通过读写分离功能对读流量进行分流，减轻存储层的读压力。 PolarDB-X读写分离功能采用了对应用透明的设计。在不修改应用程序任何代码的情况下，只需在控制台中调整读权重，即可实现将读流量按自定义的权重比例在存储资源MySQL主实例与多个存储资源只读实例之间进行分流，而写流量则不做分流全部到指向主实例。 设置读写分离后，从存储资源MySQL主实例读取属于强读（即实时强一致读）；而只读实例上的数据是从主实例上异步复制而来存在毫秒级的延迟，因此从只读实例读取属于弱读（即非强一致性读）。您可以通过Hint指定那些需要保证实时性和强一致性的读SQL到主实例上执行，详情请参见读写分离Hint。 通常可以通过事务读取，这样可以将读请求打到主库上执行，保证实施强一致读。 读写分离仅对显式事务（即需要显式提交或回滚的事务）以外的读请求（即查询请求）有效，写请求和显式事务中的读请求（包括只读事务）均在主实例中执行，不会被分流到只读实例。 读写分离带来的一系列问题 大多数互联网数据操作往往都是读多写少，随着数据的增长，数据库的 “读” 会首先成为瓶颈。如果我们希望能线性地提升数据库的读性能和写性能，就需要让读写尽可能的不相互影响，各自为政。在使用读写分离之前我们应该考虑使用缓存能不能解决问题。然后再考虑对数据库按照 “读” 和 “写” 进行分组。读写分离意味着将一体的结构的进行分散，在数据量大、高并发的情景中要考虑以下这些问题： 如何保证 Master 的高可用，故障转移，熔断限流等。 读写操作的区分规则，代码层面如何处理好读命令和写命令，尽量无感知无业务入侵。 数据一致性的容忍度。虽然是数据同步，但是由于网络的不确定性这仍然是一个不可忽视的问题。 分库 3.2 库级垂直拆分数据库垂直拆分指的是按照业务对数据库中的表进行分组，同组的放到一个新的数据库（逻辑上，并非实例）中。需要从实际业务出发将大业务分割成小业务。比如商城的整个业务中的用户相关表，订单相关表，物流相关表各自独立分类形成用户系统数据库，订单系统数据库，物流系统数据库，如下图： 带来的好处 业务清晰，职责单一 。 易维护，易扩展 。 数据服务化 。 不好的方面 提高了整个应用的复杂度，而且会形成跨库事务。 引发 “木桶效应”，任何一个短板有可能影响整个系统。 部分表关系不能 join只能通过服务相互调用来维系。 甚至由于网络问题引发数据不一致。 在需要进行分库的情况下，通常可优先考虑垂直拆分。 3.3 库级水平拆分在数据库垂直拆分后遇到单机数据库性能瓶颈之后，就可以考虑数据库水平拆分了。 之所以先垂直拆分才水平拆分，是因为垂直拆分后数据业务清晰而且单一，更加方便指定水平的标准。比如我们对商城业务垂直拆分后的 用户系统 进行水平拆分就比对整个商城业务进行水平拆分好找维度，我们可以根据用户注册时间的区间、用户的区域或者用户 ID 的范围、 hash 等条件，然后关联相关表的记录将数据进行拆分，如果放在整个商城业务上你是以用户为准还是以订单为准都不太好考虑。 我们按照每 100 万为区间对用户系统水平拆分如下： 这种拆分的好处在于 单个库的容量可控。 单挑记录保证了数据完整性。 数据关系可以通过 join 维持。 避免了跨库事务 。 缺点： 拆分规则对编码有一定的影响。 不同业务的分区交互需要统筹设计。 3.4 表级垂直拆分表级垂直拆分可以简单来描述，即大宽表拆成多个小表。数据表垂直拆分就是纵向地把表中的列分成多个表，把表从 “宽” 变“窄”。一般遵循以下几个点进行拆分： 冷热分离，把常用的列放在一个表，不常用的放在一个表。 大字段列独立存放。 关联关系的列紧密的放在一起。 我们把用户表中常用的和不常用的而且大字段分离成两张表： 3.5 表级水平拆分表级别的水平拆分原理和库级别水平拆分原理类似，通常是对于一个库来说的，可以根据主键ID进行hash拆分，或者根据某个指定字段进行拆分。比如下图中user表根据name字段作为拆分键进行拆分： 表的水平拆分感觉跟库的水平拆分思想上都是一样的，只不过粒度不同。表结构维持不变。也就是说拆分后数据集的并集等于拆分前的数据集。 3.6 分布式事务众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？ 3.6.1 什么是事务？事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。 事务的四大特性即 ACID 说到事务，就不得不提一下事务著名的四大特性。 原子性（atomicity） ：原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。 一致性（consistency） ：一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。 隔离性（isolation） ：事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。 持久性（durability） ：持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。 注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。 3.6.2 事务的隔离级别这里扩展一下，对事务的隔离性做一个详细的解释。 在事务的四大特性ACID中，要求的隔离性是一种严格意义上的隔离，也就是多个事务是串行执行的，彼此之间不会受到任何干扰。这确实能够完全保证数据的安全性，但在实际业务系统中，这种方式性能不高。因此，数据库定义了四种隔离级别，隔离级别和数据库的性能是呈反比的，隔离级别越低，数据库性能越高，而隔离级别越高，数据库性能越差。 事务并发执行会出现的问题 我们先来看一下在不同的隔离级别下，数据库可能会出现的问题： 更新丢失 当有两个并发执行的事务，更新同一行数据，那么有可能一个事务会把另一个事务的更新覆盖掉。 当数据库没有加任何锁操作的情况下会发生。 脏读 一个事务读到另一个尚未提交的事务中的数据。 该数据可能会被回滚从而失效。 如果第一个事务拿着失效的数据去处理那就发生错误了。 不可重复读 不可重复度的含义：一个事务对同一行数据读了两次，却得到了不同的结果。它具体分为如下两种情况： 虚读：在事务1两次读取同一记录的过程中，事务2对该记录进行了修改，从而事务1第二次读到了不一样的记录。 幻读：事务1在两次查询的过程中，事务2对该表进行了插入、删除操作，从而事务1第二次查询的结果发生了变化。 不可重复读与脏读的区别？ 脏读读到的是尚未提交的数据，而不可重复读读到的是已经提交的数据，只不过在两次读的过程中数据被另一个事务改过了。 3.6.3 数据库的四种隔离级别数据库一共有如下四种隔离级别： Read uncommitted 读未提交 在该级别下，一个事务对一行数据修改的过程中，不允许另一个事务对该行数据进行修改，但允许另一个事务对该行数据读。 因此本级别下，不会出现更新丢失，但会出现脏读、不可重复读。 Read committed 读提交 在该级别下，未提交的写事务不允许其他事务访问该行，因此不会出现脏读；但是读取数据的事务允许其他事务的访问该行数据，因此会出现不可重复读的情况。 Repeatable read 重复读 在该级别下，读事务禁止写事务，但允许读事务，因此不会出现同一事务两次读到不同的数据的情况（不可重复读），且写事务禁止其他一切事务。 Serializable 序列化 该级别要求所有事务都必须串行执行，因此能避免一切因并发引起的问题，但效率很低。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 3.6.4 什么是分布式事务？到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 3.6.5 CAP理论 C (一致性) Consistency： 对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A (可用性) Availability： 非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的。 P (分区容错性) Partition tolerance： 当出现网络分区后，系统能够继续工作。打个比方，这里集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正工作。 CAP 三者是不能共有的，只能同时满足其中两点。基于 AP，我们又有了 BASE 理论。 **基本可用(Basically Available)**：分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。 **软状态(Soft state)**：允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是 CAP 中的不一致。 **最终一致(Eventually consistent)**：最终一致是指经过一段时间后，所有节点数据都将会达到一致。 3.6.6 XA协议首先我们来简要看下分布式事务处理的XA规范： 可知XA规范中分布式事务有AP，RM，TM组成： 其中应用程序(Application Program ，简称AP)：AP定义事务边界（定义事务开始和结束）并访问事务边界内的资源。 资源管理器(Resource Manager，简称RM)：Rm管理计算机共享的资源，许多软件都可以去访问这些资源，资源包含比如数据库、文件系统、打印机服务器等。 事务管理器(Transaction Manager ，简称TM)：负责管理全局事务，分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚、失败恢复等。 Xa主要规定了RM与TM之间的交互，下面来看下XA规范中定义的RM 和 TM交互的接口： 本图来着 参考文章XA规范25页 xa_start负责开启或者恢复一个事务分支，并且管理XID到调用线程 xa_end 负责取消当前线程与事务分支的关联 xa_prepare负责询问RM 是否准备好了提交事务分支 xa_commit通知RM提交事务分支 xa_rollback 通知RM回滚事务分支 XA协议是使用了二阶段协议的，其中： 第一阶段TM要求所有的RM准备提交对应的事务分支，询问RM是否有能力保证成功的提交事务分支，RM根据自己的情况，如果判断自己进行的工作可以被提交，那就就对工作内容进行持久化，并给TM回执OK；否者给TM的回执NO。RM在发送了否定答复并回滚了已经的工作后，就可以丢弃这个事务分支信息了。 第二阶段TM根据阶段1各个RM prepare的结果，决定是提交还是回滚事务。如果所有的RM都prepare成功，那么TM通知所有的RM进行提交；如果有RM prepare回执NO的话，则TM通知所有RM回滚自己的事务分支。 也就是TM与RM之间是通过两阶段提交协议进行交互的。 第四章 如何合理的进行分库分表4.1 当前较流行的分库分表中间件比较常见的包括： *cobar * *TDDL * *atlas * *sharding-jdbc * mycat DRDS DBProxy cobar**** 阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 cobar 集群，cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 Cobar 是提供关系型数据库（MySQL）分布式服务的中间件，它可以让传统的数据库得到良好的线性扩展，并看上去还是一个数据库，对应用保持透明。 Cobar以Proxy的形式位于前台应用和实际数据库之间，对前台的开放的接口是MySQL通信协议，将前台SQL语句变更并按照数据分布规则发到合适的后台数据分库，再合并返回结果，模拟单库下的数据库行为。 Cobar属于中间层方案，在应用程序和MySQL之间搭建一层Proxy。中间层介于应用程序与数据库间，需要做一次转发，而基于JDBC协议并无额外转发，直接由应用程序连接数据库， 性能上有些许优势。这里并非说明中间层一定不如客户端直连，除了性能，需要考虑的因素还有很多，中间层更便于实现监控、数据迁移、连接管理等功能。 Cobar属于阿里B2B事业群，始于2008年，在阿里服役3年多，接管3000+个MySQL数据库的schema,集群日处理在线SQL请求50亿次以上。 由于Cobar发起人的离职，Cobar停止维护。后续的类似中间件，比如MyCAT建立于Cobar之上，包括现在阿里服役的RDRS其中也复用了Cobar-Proxy的相关代码。 TDDL 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 TDDL是Tabao根据自己的业务特点开发了(Tabao Distributed Data Layer, 外号：头都大了)。主要解决了分库分表对应用的透明化以及异构数据库之间的数据复制， 它是一个基于集中式配置的jdbc datasourcce实现，具有主备，读写分离，动态数据库配置等功能。 TDDL并非独立的中间件，只能算作中间层，处于业务层和JDBC层中间，是以Jar包方式提供给应用调用，属于JDBC Shard的思想。 TDDL源码：https://github.com/alibaba/tb_tddl TDDL复杂度相对较高。当前公布的文档较少，只开源动态数据源，分表分库部分还未开源，还需要依赖diamond，不推荐使用。 atlas 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 Atlas是一个位于应用程序与MySQL之间的基于MySQL协议的数据中间层项目，它是在mysql-proxy 0.8.2版本上对其进行优化，360团队基于mysql proxy 把lua用C改写， 它实现了MySQL的客户端和服务端协议，作为服务端与应用程序通讯，同时作为客户端与MySQL通讯。它对应用程序屏蔽了DB的细节。 Altas不能实现分布式分表，所有的字表必须在同一台DB的同一个DataBase里且所有的字表必须实现建好，Altas没有自动建表的功能。 原有版本是不支持分库分表， 目前已经放出了分库分表版本。在网上看到一些朋友经常说在高并 发下会经常挂掉，如果大家要使用需要提前做好测试。 sharding-jdbc 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 sharding-JDBC是当当应用框架ddframe中，从关系型数据库模块dd-rdb中分离出来的数据库水平分片框架，实现透明化数据库分库分表访问。 Sharding-JDBC是继dubbox和elastic-job之后，ddframe系列开源的第3个项目。 Sharding-JDBC直接封装JDBC API，可以理解为增强版的JDBC驱动，旧代码迁移成本几乎为零： 可适用于任何基于Java的ORM框架，如JPA、Hibernate、Mybatis、Spring JDBC Template或直接使用JDBC。 可基于任何第三方的数据库连接池，如DBCP、C3P0、 BoneCP、Druid等。 理论上可支持任意实现JDBC规范的数据库。虽然目前仅支持MySQL，但已有支持Oracle、SQLServer等数据库的计划。 Sharding-JDBC定位为轻量Java框架，使用客户端直连数据库，以jar包形式提供服务，无proxy代理层，无需额外部署，无其他依赖，DBA也无需改变原有的运维方式。 Sharding-JDBC分片策略灵活，可支持等号、between、in等多维度分片，也可支持多分片键。 SQL解析功能完善，支持聚合、分组、排序、limit、or等查询，并支持Binding Table以及笛卡尔积表查询。 mycat 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。 MyCAT是社区爱好者在阿里cobar基础上进行二次开发，解决了cobar当时存 在的一些问题，并且加入了许多新的功能在其中。目前MyCAT社区活 跃度很高， 目前已经有一些公司在使用MyCAT。总体来说支持度比 较高，也会一直维护下去，发展到目前的版本，已经不是一个单纯的MySQL代理了， 它的后端可以支持MySQL, SQL Server, Oracle, DB2, PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。 MyCAT是一个强大的数据库中间件，不仅仅可以用作读写分离，以及分表分库、容灾管理，而且可以用于多租户应用开发、云平台基础设施，让你的架构具备很强的适应性和灵活性， 借助于即将发布的MyCAT只能优化模块，系统的数据访问瓶颈和热点一目了然，根据这些统计分析数据，你可以自动或手工调整后端存储，将不同的表隐射到不同存储引擎上，而整个应用的代码一行也不用改变。 MyCAT是在Cobar基础上发展的版本，两个显著提高：后端由BIO改为NIO，并发量有大幅提高； 增加了对Order By, Group By, Limit等聚合功能 （虽然Cobar也可以支持Order By, Group By, Limit语法，但是结果没有进行聚合，只是简单返回给前端，聚合功能还是需要业务系统自己完成） DRDS DRDS是阿里巴巴自主研发的分布式数据库服务（此项目不开源）,DRDS脱胎于阿里巴巴开源的Cobar分布式数据库引擎，吸收了Cobar核心的Cobar-Proxy源码， 实现了一套独立的类似MySQL-Proxy协议的解析端，能够对传入的SQL进行解析和处理，对应用程序屏蔽各种复杂的底层DB拓扑结构，获得单机数据库一样的使用体验， 同时借鉴了淘宝TDDL丰富的分布式数据库实践经验，实现了对分布式Join支持，SUM&#x2F;MAX&#x2F;COUNT&#x2F;AVG等聚合函数支持以及排序等函数支持， 通过异构索引、小表广播等解决分布式数据库使用场景下衍生出的一系列问题，最终形成了完整的分布式数据库方案。 现在是品牌升级之后叫PolarDB-X。 DBProxy DBProxy是美团点评DBA团队针对公司内部需求，在奇虎360公司开源的Atlas做了很多改进工作，形成了新的高可靠、高可用企业级数据库中间件 其特性主要有：读写分离、负载均衡、支持分表、IP过滤、sql语句黑名单、DBA平滑下线DB、从库流量配置、动态加载配置项 项目的Github地址是https://github.com/Meituan-Dianping/DBProxy 4.2 其他知名度较低的分库分表中间件Heisenberg Baidu. 其优点：分库分表与应用脱离，分库表如同使用单库表一样，减少db连接数压力，热重启配置，可水平扩容，遵守MySQL原生协议，读写分离，无语言限制， mysqlclient, c, java都可以使用Heisenberg服务器通过管理命令可以查看，如连接数，线程池，结点等，并可以调整采用velocity的分库分表脚本进行自定义分库表，相当的灵活。 https://github.com/brucexx/heisenberg（开源版已停止维护） CDS JD. Completed Database Sharding. CDS是一款基于客户端开发的分库分表中间件产品，实现了JDBC标准API，支持分库分表，读写分离和数据运维等诸多共，提供高性能，高并发和高可靠的海量数据路由存取服务，业务系统可近乎零成本进行介入，目前支持MySQL, Oracle和SQL Server. (架构上和Cobar，MyCAT相似，直接采用jdbc对接，没有实现类似MySQL协议，没有NIO,AIO，SQL Parser模块采用JSqlParser, Sql解析器有：druid&gt;JSqlParser&gt;fdbparser.) DDB 网易. Distributed DataBase. DDB经历了三次服务模式的重大更迭：Driver模式-&gt;Proxy模式-&gt;云模式。 Driver模式：基于JDBC驱动访问，提供一个db.jar, 和TDDL类似， 位于应用层和JDBC之间. Proxy模式：在DDB中搭建了一组代理服务器来提供标准的MySQL服务，在代理服务器内部实现分库分表的逻辑。应用通过标准数据库驱动访问DDB Proxy, Proxy内部通过MySQL解码器将请求还原为SQL, 并由DDB Driver执行得到结果。 私有云模式：基于网易私有云开发的一套平台化管理工具Cloudadmin, 将DDB原先Master的功能打散，一部分分库相关功能集成到proxy中，如分库管理、表管理、用户管理等，一部分中心化功能集成到Cloudadmin中，如报警监控，此外，Cloudadmin中提供了一键部署、自动和手动备份，版本管理等平台化功能。 OneProxy 数据库界大牛，前支付宝数据库团队领导楼方鑫开发，基于mysql官方 的proxy思想利用c进行开发的，OneProxy是一款商业收费的中间件， 楼总舍去了一些功能点，专注在性能和稳定性上。测试过说在高并发下很稳定。 Oceanus ： 58同城数据库中间件 Oceanus致力于打造一个功能简单、可依赖、易于上手、易于扩展、易于集成的解决方案，甚至是平台化系统。拥抱开源，提供各类插件机制集成其他开源项目，新手可以在几分钟内上手编程，分库分表逻辑不再与业务紧密耦合，扩容有标准模式，减少意外错误的发生。 Vitess 这个中间件是Youtube生产在使用的，但是架构很复杂。 与以往中间件不同，使用Vitess应用改动比较大要 使用他提供语言的API接口，我们可以借鉴他其中的一些设计思想。 Kingshard Kingshard是前360Atlas中间件开发团队的陈菲利用业务时间 用go语言开发的，目前参与开发的人员有3个左右， 目前来看还不是成熟可以使用的产品，需要在不断完善。 MaxScale与MySQL Route 这两个中间件都算是官方的吧，MaxScale是mariadb (MySQL原作者维护的一个版本)研发的，目前版本不支持分库分表。 MySQL Route是现在MySQL 官方Oracle公司发布出来的一个中间件。 4.3 如何选择拆分键本文将介绍如何在PolarDB-X中选择合适的拆分键。 背景信息 拆分键即分库或分表字段，是水平拆分过程中用于生成拆分规则的数据表字段。PolarDB-X将拆分键值通过拆分函数计算得到一个计算结果，然后根据这个结果将数据分拆到私有定制RDS实例上。 数据表拆分的首要原则是尽可能找到数据所归属的业务逻辑实体，并确定大部分（或核心的）SQL操作或者具备一定并发的SQL都是围绕这个实体进行，然后可使用该实体对应的字段作为拆分键。 示例 业务逻辑实体通常与应用场景相关，下面的一些典型应用场景都有明确的业务逻辑实体（以此类推，其它应用场景也能找到合适的业务逻辑实体），其标识型字段可用来做拆分键。 面向用户的互联网应用，围绕用户维度来做各种操作，那么业务逻辑实体就是用户，可使用用户ID作为拆分键。 侧重于卖家的电商应用，围绕卖家维度来做各种操作，那么业务逻辑实体就是卖家，可使用卖家ID作为拆分键。 游戏类在线应用，围绕玩家维度来做各种操作，那么业务逻辑实体就是玩家，可使用玩家ID作为拆分键。 车联网在线应用，围绕车辆维度来做各种操作，那么业务逻辑实体就是车辆，可使用车辆ID作为拆分键。 税务类在线应用，围绕纳税人来进行前台业务操作，那么业务逻辑实体就是纳税人，可使用纳税人ID作为拆分键。 例如某面向卖家的电商应用，需要对如下单表进行水平拆分： 1CREATE TABLE sample_order ( id INT(11) NOT NULL, sellerId INT(11) NOT NULL, trade_id INT(11) NOT NULL, buyer_id INT(11) NOT NULL, buyer_nick VARCHAR(64) DEFAULT NULL, PRIMARY KEY (id) ) 确定业务逻辑实体为卖家，那么选择字段sellerId作为拆分键，则您可以使用如下分布式DDL语句建表： 1CREATE TABLE sample_order ( id INT(11) NOT NULL, sellerId INT(11) NOT NULL, trade_id INT(11) NOT NULL, buyer_id INT(11) NOT NULL, buyer_nick VARCHAR(64) DEFAULT NULL, PRIMARY KEY (id) ) DBPARTITION BY HASH(sellerId) 如果确实找不到合适的业务逻辑实体作为拆分键，特别是传统企业级应用，那么可以考虑以下方法来选择拆分键。 根据数据分布和访问的均衡度来考虑拆分键，尽量将数据表中的数据相对均匀地分布在不同分表中，PolarDB-X推出了全局强一致二级索引和Parallel Query能够提高在此场景下SQL并发度并缩短响应时间。 按照数字（字符串）类型与时间类型字段相结合作为拆分键，进行分库和分表，适用于日志检索类的应用场景。 例如某日志系统记录了用户的所有操作，现需要对如下日志单表进行水平拆分： 1CREATE TABLE user_log ( userId INT(11) NOT NULL, name VARCHAR(64) NOT NULL, operation VARCHAR(128) DEFAULT NULL, actionDate DATE DEFAULT NULL ) 此时可以选择用户标识与时间字段相结合作为拆分键，并按照一周七天进行分表，则您可以使用如下分布式DDL语句建表： 1CREATE TABLE user_log ( userId INT(11) NOT NULL, name VARCHAR(64) NOT NULL, operation VARCHAR(128) DEFAULT NULL, actionDate DATE DEFAULT NULL ) DBPARTITION BY HASH(userId) TBPARTITION BY WEEK(actionDate) TBPARTITIONS 7 更多拆分键的选择和分表形式，请参见CREATE TABLE和拆分函数概述。 4.4 如何选择分片数DRDS 中的水平拆分有两个层次：分库和分表。每个 RDS 实例上默认会创建8个物理分库，每个物理分库上可以创建一个或多个物理分表。分表数通常也被称为分片数。 一般情况下，建议单个物理分表的容量不超过500万行数据。通常可以预估1到2年的数据增长量，用估算出的总数据量除以总的物理分库数，再除以建议的最大数据量500万，即可得出每个物理分库上需要创建的物理分表数： 物理分库上的物理分表数 &#x3D; 向上取整(估算的总数据量 &#x2F; (RDS 实例数 * 8) &#x2F; 5,000,000) 因此，当计算出的物理分表数等于1时，分库即可，无需再进一步分表，即每个物理分库上一个物理分表；若计算结果大于1，则建议既分库又分表，即每个物理分库上多个物理分表。 例如，某用户预估一张表在2年后的总数据量大概是1亿行，购买了4个 RDS 实例，那么按照上述公式计算： 物理分库上的物理分表数 &#x3D; CEILING(100,000,000 &#x2F; ( 4 * 8 ) &#x2F; 5,000,000) &#x3D; CEILING(0.625) &#x3D; 1 结果为1，那么只分库即可，即每个物理分库上1个物理分表。 若上述例子中仅购买了1个 RDS 实例，那么按照上述公式计算： 物理分库上的物理分表数 &#x3D; CEILING(100,000,000 &#x2F; ( 1 * 8 ) &#x2F; 5,000,000) &#x3D; CEILING(2.5) &#x3D; 3 结果为3，那么建议既分库又分表，即每个物理分库上3个物理分表。 4.5 单个表的容量限制最佳实践分表的大小是有限制的，建议单个分表的数据记录数不宜超过500万。（阿里巴巴最佳实践） 4.6 DRDS 的分库分表，能否更换分库分表的拆分键对于已经被建好的分库分表，DRDS 不支持变更它们的拆分键。如果确实有需要变更表的拆分键，可以采用以下的临时办法： 选择新的分库键并重新建表； 然后将原表的数据进行导入。 4.7 PolarDB-X实例中每一个RDS的分库数，每个分库里的分表数是否有限制单个RDS实例的默认分库数目是8个，不可更改。 每个分库里的分表数目理论上是没有限制的，受限于PolarDB-X服务器本身的硬件资源。分表数目的选择需要依据对业务数据量的评估，详情请参见如何选择分片数。 4.8 PolarDB-X是否支持分布式JOIN？它是如何支持复杂SQL？PolarDB-X支持大部分的JOIN语法，但对于比较复杂的情况，PolarDB-X做了一些限制。例如大表之间的JOIN，由于执行代价过高，速度过慢容易导致性能或者系统不可用等情况，因此请尽量避免，详情请参见Join与子查询的优化和执行。 4.9 平滑扩容什么是平滑扩容 PolarDB-X 平滑扩容是指通过增加 RDS 的数量以提升整体性能。当 RDS 的 IOPS、CPU、磁盘容量等指标到达瓶颈，并且 SQL 优化、RDS 升配已无法解决瓶颈（例如磁盘已升至顶配）时，可通过 PolarDB-X 水平扩容增加 RDS 数量，提升 PolarDB-X 数据库的容量。 PolarDB-X 平滑扩容通过迁移分库到新 RDS 来降低原 RDS 的压力。例如，扩容前8个库的压力集中在一个 RDS 实例上，扩容后8个库分别部署在两个 RDS 实例上，单个 RDS 实例的压力就明显降低。如下图所示： 说明：平滑扩容多次后，如果出现 RDS 数量和分库数量相等的情况，需要创建另外一个 PolarDB-X 和预期容量 RDS 的数据库，再进行数据迁移以达到更大规模数据容量扩展的目标。此过程较复杂，推荐创建 PolarDB-X 数据库时要考虑未来2-3年数据的增长预期，做好 RDS 数量规划。 第五章 一定要分库分表吗？有没有过渡的方案前面说到分库分表是OLTP的最终形态，但是对于大多数公司来说，分库分表引入的复杂度会大大提升开发成本。所以针对此场景我们设计了一定的过渡方案。 读写分离：查询走只读实例，缓解主实例压力。 查询分析走分析库：业务库只承载事务相关操作，业务库通过同步技术将数据实时同步至分析库（如阿里的ADB），查询分析场景走分析库。 升级为更高性能的OLTP数据库，如PolarDB，单库最大64TB，单集群最大100TB，最大写TPS支持十万，完全兼容Mysql。 第六章 分布式事务解决方案6.1 二阶段提交（2PC）二阶段提交协议（Two-phase Commit Protocol，简称 2PC）是分布式事务的核心协议。在此协议中，一个事务管理器（Transaction Manager，简称 TM）协调 1 个或多个资源管理器（Resource Manager，简称 RM）的活动，所有资源管理器向事务管理器汇报自身活动状态，由事务管理器根据各资源管理器汇报的状态（完成准备或准备失败）来决定各资源管理器是“提交”事务还是进行“回滚”操作。 二阶段提交的具体流程如下： 应用程序向事务管理器提交请求，发起分布式事务； 在第一阶段，事务管理器联络所有资源管理器，通知它们准备提交事务； 各资源管理器返回完成准备（或准备失败）的消息给事务管理器（响应超时算作失败）； 在第二阶段： 如果所有资源管理器均完成准备（如图 1），则事务管理器会通知所有资源管理器执行事务提交； 如果任一资源管理器准备失败（如图 2 中的资源管理器 B），则事务管理器会通知所有资源管理器进行事务回滚。 所有资源管理器完成准备，事务管理器协调各资源管理器提交事务 任一资源管理器准备失败，事务管理器协调各资源管理器回滚事务 6.2 TCC补偿方案Try-Confirm-Cancel（TCC）是初步操作（Try）、确认操作（Confirm）和取消操作（Cancel）三种操作的缩写，这三种操作的业务含义如下： Try 阶段：对业务系统做检测及资源预留； Confirm 阶段：对业务系统做确认提交。默认 Confirm 阶段是不会出错的，只要 Try 成功，Confirm 一定成功； Cancel 阶段：当业务执行出现错误，需要回滚的状态下，执行业务取消，释放预留资源。 TCC 是二阶段提交协议（Two-phase Commit Protocol，简称 2PC）的扩展，Try 操作对应 2PC 中一阶段的准备提交事务（Prepare），Confirm 对应 2PC 中二阶段事务提交（Commit），Cancel 对应 2PC 中二阶段事务回滚（Rollback）。 与 2PC 不同的是，TCC 是一种编程模型，是应用层的 2PC；TCC 的 3 个操作均由编码实现，通过编码实现了 2PC 资源管理器的功能。 TCC 自编码的特性决定 TCC 资源管理器可以跨数据库、跨应用实现资源管理，将对不同的数据库访问、不同的业务操作通过编码方式转换一个原子操作，解决了复杂业务场景下的事务问题。同时 TCC 的每一个操作对于数据库来讲都是一个本地数据库事务，操作结束则本地数据库事务结束，数据库的资源也就被释放；这就规避了数据库层面的 2PC 对资源占用导致的性能低下问题。 基本原理如下图所示： 事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的try接口，完成一阶段准备。之后事务协调器会根据try接口返回情况，决定调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。 TCC方案让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能，比如华为分布式事务中间件DTM性能极高，普通配置服务器可以支持全局事务1万+ TPS，分支事务计算方式为3万+ TPS （阿里分布式事务中间件也是采用后者计算方式）。 当然TCC方案也有不足之处，集中表现在以下两个方面： 对应用的侵入性强。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。 实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。 上述原因导致TCC方案大多被研发实力较强、有迫切需求的大公司所采用。微服务倡导服务的轻量化，而TCC方案中很多事务的处理逻辑需要应用自己编码实现，复杂且开发量大。 6.3 基于消息的最终一致性消息一致性方案是通过消息中间件保证上、下游应用数据操作的[一致性。基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么两者都成功或者都失败。下游应用向消息系统订阅该消息，收到消息后执行相应操作。 消息方案从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性。基于消息的最终一致性方案对应用侵入性也很高，应用需要进行大量业务改造，成本非常高。 入侵代码的方案是基于现有情形“迫不得已”才推出的解决方案，实际上它们实现起来非常不优雅，比如TCC，一个事务的调用通常伴随而来的是对该事务接口增加一系列的反向操作，提交逻辑必然伴随着回滚的逻辑，这样的代码会使得项目非常臃肿，维护成本高。 针对上面所说的分布式事务解决方案的痛点，那很显然，我们理想的分布式事务解决方案肯定是性能要好而且要对业务无入侵，业务层上无需关心分布式事务机制的约束，也就是本文所重点推荐的非侵入事务（全局事务），真正做到事务与业务分离。 下面举例基于阿里云RocketMQ事务消息来保证分布式事务的数据一致性来进行说明。 例如，针对一家互联网电商企业，其业务涉及广泛，如注册、订单、库存、物流等；同时，也会涉及许多业务峰值时刻，如秒杀活动、周年庆、定期特惠等。这些活动都对分布性系统中的各项微服务应用的处理性能带来很大的挑战。 消息队列 RocketMQ 版作为分布式系统中的重要组件，可用于应对这些挑战，例如解决应用的分布式事务的数据一致性问题。 注册系统注册的流程中，用户入口在网页注册系统，通知系统在邮件系统，两个系统之间的数据需要保持最终一致。 普通消息处理 如上所述，注册系统和邮件通知系统之间通过消息队列进行异步处理。注册系统将注册信息写入注册系统之后，发送一条注册成功的消息到消息队列 RocketMQ 版，邮件通知系统订阅消息队列 RocketMQ 版的注册消息，做相应的业务处理，发送注册成功或者失败的邮件。 流程说明如下： 注册系统发起注册。 注册系统向消息队列 RocketMQ 版发送注册消息成功与否的消息。2.1 消息发送成功，进入 3。 2.2 消息发送失败，导致邮件通知系统未收到消息队列 RocketMQ 版发送的注册成功与否的消息，而无法发送邮件，最终邮件通知系统和注册系统之间的状态数据不一致。 邮件通知系统收到消息队列 RocketMQ 版的注册成功消息。 邮件通知系统发送注册成功邮件给用户。 在这样的情况下，虽然实现了系统间的解藕，上游系统不需要关心下游系统的业务处理结果；但是数据一致性不好处理，如何保证邮件通知系统状态与注册系统状态的最终一致。 事务消息处理 此时，需要有利用消息队列 RocketMQ 版所提供的事务消息来实现系统间的状态数据一致性。 流程说明如下： 注册系统向消息队列 RocketMQ 版发送半事务消息。1.1 半事务消息发送成功，进入 2。 1.2 半事务消息发送失败，注册系统不进行注册，流程结束。（最终注册系统与邮件通知系统数据一致） 注册系统开始注册。2.1 注册成功，进入 3.1。 2.2 注册失败，进行 3.2。 注册系统向消息队列 RocketMQ 版发送半消息状态。3.1 提交半事务消息，产生注册成功消息，进入 4。 3.2 回滚半事务消息，未产生注册成功消息，流程结束。（最终注册系统与邮件通知系统数据一致） 邮件通知系统接收消息队列 RocketMQ 版的注册成功消息。 邮件通知系统发送注册成功邮件。（最终注册系统与邮件通知系统数据一致） 半事务消息（Half-transaction message）是消息队列中的一个特性，它常用于实现分布式系统中的最终一致性。消息队列 RocketMQ 版支持半事务消息，这项功能特别适合处理那些既需要执行本地事务，又需要发送消息到消息队列进行后续处理的场景。 在分布式系统中进行事务操作时，可能会涉及到一个系统操作（例如，更新数据库记录）和向其他系统或服务发送消息（例如，进行异步处理或通知）。在这种情况下，半事务消息非常有用，因为它们能够确保本地事务和消息发布之间的一致性。 半事务消息的流程大致分为以下几个步骤： 发送半事务消息：首先，发送一个半事务消息到消息队列。此时消息不会立即被消费者消费，因为它处于“未确定”的状态。 执行本地事务：在发送半事务消息后，执行相关的本地事务逻辑，例如修改数据库的某些记录。 本地事务结果：本地事务的执行结果会有两种：成功或失败。 如果本地事务执行成功，发送确认消息，此时 RocketMQ 会将半事务消息更改为可供消费者消费的状态。 如果本地事务执行失败，发送回滚消息，RocketMQ 会删除该半事务消息或执行回滚操作，确保消息不会被消费。 消息队列回查事务状态：为了处理长时间未确定状态的事务消息，RocketMQ 会定期向消息发送者的系统发起回查，查询本地事务的当前状态，然后基于查询结果确认或回滚半事务消息。 这个机制的好处是，它提供了一种机制来保证跨服务的操作都是成功的，或者在遇到问题时都被回滚，这样可以在不丢失消息的前提下保证各服务间处理的一致性。在分布式系统常见的设计模式，如Saga模式中，半事务消息广泛用于实现服务间复杂事务的一致性。 总而言之，半事务消息是一种特殊类型的消息，在消息和关联的本地事务之间建立了半成品状态，从而保证只有在本地事务成功完成时，消息才会被其他系统消费。这为分布式系统中事务一致性提供了一种相对安全的解决方案。 6.4 全局事务6.4.1 为什么需要全局事务服务 一个完整的业务往往需要调用多个子业务或服务，随着业务的不断增多，涉及的服务及数据也越来越多，越来越复杂。传统的系统难以支撑，出现了应用和数据库等的分布式系统。分布式系统又带来了数据一致性的问题，从而产生了分布式事务。 6.4.2 阿里云全局事务服务GTS全局事务服务GTS（Global Transaction Service）用于实现分布式环境下，特别是微服务架构下的高性能事务一致性。可以与RDS、MySQL、PostgreSQL等数据源，Spring Cloud、Dubbo、HSF及其他RPC框架，MQ消息队列等中间件产品配合使用，轻松实现分布式数据库事务、多库事务、消息事务、服务链路级事务及各种组合。 6.4.3 开源分布式事务解决方案SeataSimple Extensible Autonomous Transaction Architecture（Seata）是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。 2019 年，基于 GTS 的技术积累，阿里巴巴发起了开源项目 Seata 2020 年 2 月，基于 Seata 项目 GA 后的版本，GTS 实现与 Seata 的协议兼容，支持使用 Seata 的应用无缝迁移到云上，基于 GTS 提供的服务高效运行。 GTS 已经全面兼容和支持开源分布式事务 Seata，实现与 Seata 的协议兼容，支持使用 Seata 的应用无缝迁移到云上，基于 GTS 提供的服务高效运行。 6.4.4 分布式事务框架和事务模式GTS 定义了一套事务框架以便描述分布式事务，在框架下支持不同事务模式运行。 核心组件定义 分布式事务包含以下 3 个核心组件： Transaction Coordinator（TC）：事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager（TM）：控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager（RM）：控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 一个典型的事务过程包括： TM 向 TC 申请开启（Begin）一个全局事务，全局事务创建成功并生成一个全局唯一的 XID。 XID 在微服务调用链路的上下文中传播。 RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖。 TM 向 TC 发起针对 XID 的全局提交（Commit）或回滚（Rollback）决议。 TC 调度 XID 下管辖的全部分支事务完成提交（Commit）或回滚（Rollback）请求。 事务框架 基于架构上定义的 3 个核心组件，分布式事务被抽象成如下事务框架。 3个核心组件的功能如下： TM定义全局事务的边界。 RM负责定义分支事务的边界和行为。 TC、TM和RM交互，做全局的协调。交互包括开启（Begin）、提交（Commit）、回滚（Rollback）全局事务；分支注册（Register Branch）、状态上报（Branch Status Report）和分支提交（Branch Commit）、分支回滚（Branch Rollback）。 事务模式 事务模式是这个框架下 RM 驱动的分支事务的不同行为模式，即事务（分支）模式。事务模式包括 AT 模式、TCC 模式、Saga 模式和 XA 模式。 AT 模式 AT 模式 RM 驱动分支事务的行为分为以下两个阶段： 执行阶段： 代理 JDBC 数据源，解析业务 SQL，生成更新前后的镜像数据，形成 UNDO LOG。 向 TC 注册分支。 分支注册成功后，把业务数据的更新和 UNDO LOG 放在同一个本地事务中提交。 完成阶段： 全局提交，收到 TC 的分支提交请求，异步删除相应分支的 UNDO LOG。 全局回滚，收到 TC 的分支回滚请求，查询分支对应的 UNDO LOG 记录，生成补偿回滚的 SQL 语句，执行分支回滚并返回结果给 TC。 TCC模式 TCC 模式 RM 驱动分支事务的行为分为以下两个阶段： 执行阶段： 向 TC 注册分支。 执行业务定义的 Try 方法。 向 TC 上报 Try 方法执行情况：成功或失败。 完成阶段： 全局提交，收到 TC 的分支提交请求，执行业务定义的 Confirm 方法。 全局回滚，收到 TC 的分支回滚请求，执行业务定义的 Cancel 方法。 Saga 模式 Saga 模式 RM 驱动分支事务的行为包含以下两个阶段： 执行阶段： 向 TC 注册分支。 执行业务方法。 向 TC 上报业务方法执行情况：成功或失败。 完成阶段： 全局提交，RM 不需要处理。 全局回滚，收到 TC 的分支回滚请求，执行业务定义的补偿回滚方法。 XA模式 XA 模式 RM 驱动分支事务的行为包含以下两个阶段： 执行阶段： 向 TC 注册分支。 XA Start，执行业务 SQL，XA End。 XA prepare，并向 TC 上报 XA 分支的执行情况：成功或失败。 完成阶段： 收到 TC 的分支提交请求，XA Commit。 收到 TC 的分支回滚请求，XA Rollback。"},{"title":"场景题设计优化文章汇总","path":"/wiki/interview/场景题设计优化文章汇总.html","content":"一、后端存储实战 二、接口性能优化方案"},{"title":"常用命令随记","path":"/wiki/interview/常用命令随记.html","content":"1、代码提交 切换到基础分支： 1git checkout EI63100637_20231102 从基础分支拉取最新的更改： 1git pull origin EI63100637_20231102 这里假设远程仓库的名称是origin，这是默认名称。 创建新的开发分支： 假设你的新开发分支命名为feature&#x2F;my_new_feature，你可以用以下命令创建并切换到这个分支： 1git checkout -b feature/my_new_feature 在新分支上进行更改： 这时候，你可以开始在新分支上进行开发工作。比如创建、编辑、删除文件等。 将更改添加到暂存区： 当你完成了一部分工作并且准备提交时，你需要先将更改添加到暂存区。可以用以下命令添加单个文件： 1git add &lt;filename&gt; 或者添加所有更改的文件： 1git add . 提交更改： 1git commit -m &quot;Add a meaningful commit message describing the changes&quot; 另：不修改commit message 1git commit --amend --no-edit 另：修改后的推送 12# 强制推送修改git push origin my-branch --force # 或使用更安全的 --force-with-lease 将新分支推送到远程仓库： 如果这是第一次推送这个分支到远程仓库，你需要使用–set-upstream（或-u）标志将本地分支与远程分支建立联系： 1git push -u origin feature/my_new_feature 之后的推送可以简化为： 1git push 后续的提交： 对于后续的提交，重复步骤4-6，并使用普通的git push命令将更改推送到远程分支。 合并或Pull Request： 根据你的工作流程，你可能需要将feature&#x2F;my_new_feature分支合并到主分支，或者通过Pull Request（PR）&#x2F; Merge Request（MR）的方式在团队内进行代码审查。 8、拉取远程代码（未合并） 1234567# 查看远程提交git fetch origin# 2a3ee50e4..4860c74b6 jibei_1229 -&gt; origin/jibei_1229# 拉取远程代码到本地分支git pull origin jibei_1229#远程分支跟踪git branch --set-upstream-to=origin/EI63367128_20240315 2、MAC快捷键1、截图 截取全屏: Command (⌘) + Shift + 3: 使用此快捷键会捕捉整个屏幕，并将截图文件保存到桌面。 截取所选区域: Command (⌘) + Shift + 4: 使用此快捷键后，光标将变成一个十字形，你可以拖动以选择屏幕上的特定区域截图。截图完成后，文件会被保存到桌面。 2、强制退出某个应用 Command (⌘) + Option (⌥) + Esc 快捷键 3、复制文件路径 选中文件，然后按住Option + Command + C。这将会复制选中文件的完整路径到剪贴板。 3、kubectl命令1、获取命名空间 12kubectl get namespaceskubectl get namespaces | grep mas 2、获取pods信息 123# mpaasa-mpaas-mas为上一步获取的namespaceskubectl get pods -n mpaasa-mpaas-mas 3、进入容器 123#mpaasa-mpaas-mas-mpaas-masweb-0为pods名kubectl exec -it mpaasa-mpaas-mas-mpaas-masweb-0 bash -n mpaasa-mpaas-mas 3、查看环境变量 12# 查看masweb项目中symbole_query_type的取值env | grep symbole_query_type 4、进入Hbase shell 1234# 进入hbase容器kubectl exec -it mpaasa-anthbase-amaster1-0 bash -n mpaasa-anthbasehbase shell"},{"title":"消息队列MQ","path":"/wiki/interview/消息队列MQ.html","content":"常见面试题1、你们为什么使用mq？具体的使用场景是什么？mq的作用很简单，削峰填谷。以电商交易下单的场景来说，正向交易的过程可能涉及到创建订单、扣减库存、扣减活动预算、扣减积分等等。每个接口的耗时如果是100ms，那么理论上整个下单的链路就需要耗费400ms，这个时间显然是太长了。 如果这些操作全部同步处理的话，首先调用链路太长影响接口性能，其次分布式事务的问题很难处理，这时候像扣减预算和积分这种对实时一致性要求没有那么高的请求，完全就可以通过mq异步的方式去处理了。同时，考虑到异步带来的不一致的问题，我们可以通过job去重试保证接口调用成功，而且一般公司都会有核对的平台，比如下单成功但是未扣减积分的这种问题可以通过核对作为兜底的处理方案。 使用mq之后我们的链路变简单了，同时异步发送消息我们的整个系统的抗压能力也上升了。 2、那你们使用什么mq？基于什么做的选型？我们主要调研了几个主流的mq，kafka、rabbitmq、rocketmq、activemq，选型我们主要基于以下几个点去考虑： 由于我们系统的qps压力比较大，所以性能是首要考虑的要素。 开发语言，由于我们的开发语言是java，主要是为了方便二次开发。 对于高并发的业务场景是必须的，所以需要支持分布式架构的设计。 功能全面，由于不同的业务场景，可能会用到顺序消息、事务消息等。 基于以上几个考虑，我们最终选择了RocketMQ。 3、你上面提到异步发送，那消息可靠性怎么保证？消息丢失可能发生在生产者发送消息、MQ本身丢失消息、消费者丢失消息3个方面。 生产者丢失生产者丢失消息的可能点在于程序发送失败抛异常了没有重试处理，或者发送的过程成功但是过程中网络闪断MQ没收到，消息就丢失了。 由于同步发送的一般不会出现这样使用方式，所以我们就不考虑同步发送的问题，我们基于异步发送的场景来说。 异步发送分为两个方式：异步有回调和异步无回调，无回调的方式，生产者发送完后不管结果可能就会造成消息丢失，而通过异步发送+回调通知+本地消息表的形式我们就可以做出一个解决方案。以下单的场景举例。 下单后先保存本地数据和MQ消息表，这时候消息的状态是发送中，如果本地事务失败，那么下单失败，事务回滚。 下单成功，直接返回客户端成功，异步发送MQ消息 MQ回调通知消息发送结果，对应更新数据库MQ发送状态 JOB轮询超过一定时间（时间根据业务配置）还未发送成功的消息去重试 在监控平台配置或者JOB程序处理超过一定次数一直发送不成功的消息，告警，人工介入。 一般而言，对于大部分场景来说异步回调的形式就可以了，只有那种需要完全保证不能丢失消息的场景我们做一套完整的解决方案。 MQ丢失如果生产者保证消息发送到MQ，而MQ收到消息后还在内存中，这时候宕机了又没来得及同步给从节点，就有可能导致消息丢失。 比如RocketMQ： RocketMQ分为同步刷盘和异步刷盘两种方式，默认的是异步刷盘，就有可能导致消息还未刷到硬盘上就丢失了，可以通过设置为同步刷盘的方式来保证消息可靠性，这样即使MQ挂了，恢复的时候也可以从磁盘中去恢复消息。 比如Kafka也可以通过配置做到： 1234acks=all 只有参与复制的所有节点全部收到消息，才返回生产者成功。这样的话除非所有的节点都挂了，消息才会丢失。replication.factor=N,设置大于1的数，这会要求每个partion至少有2个副本min.insync.replicas=N，设置大于1的数，这会要求leader至少感知到一个follower还保持着连接retries=N，设置一个非常大的值，让生产者发送失败一直重试 虽然我们可以通过配置的方式来达到MQ本身高可用的目的，但是都对性能有损耗，怎样配置需要根据业务做出权衡。 消费者丢失消费者丢失消息的场景：消费者刚收到消息，此时服务器宕机，MQ认为消费者已经消费，不会重复发送消息，消息丢失。 RocketMQ默认是需要消费者回复ack确认，而kafka需要手动开启配置关闭自动offset。 消费方不返回ack确认，重发的机制根据MQ类型的不同发送时间间隔、次数都不尽相同，如果重试超过次数之后会进入死信队列，需要手工来处理了。（Kafka没有这些） 4、你说到消费者消费失败的问题，那么如果一直消费失败导致消息积压怎么处理？因为考虑到时消费者消费一直出错的问题，那么我们可以从以下几个角度来考虑： 消费者出错，肯定是程序或者其他问题导致的，如果容易修复，先把问题修复，让consumer恢复正常消费 如果时间来不及处理很麻烦，做转发处理，写一个临时的consumer消费方案，先把消息消费，然后再转发到一个新的topic和MQ资源，这个新的topic的机器资源单独申请，要能承载住当前积压的消息 处理完积压数据后，修复consumer，去消费新的MQ和现有的MQ数据，新MQ消费完成后恢复原状 5、那如果消息积压达到磁盘上限，消息被删除了怎么办？最初，我们发送的消息记录是落库保存了的，而转发发送的数据也保存了，那么我们就可以通过这部分数据来找到丢失的那部分数据，再单独跑个脚本重发就可以了。如果转发的程序没有落库，那就和消费方的记录去做对比，只是过程会更艰难一点。 6、说了这么多，那你说说RocketMQ实现原理吧？RocketMQ由NameServer注册中心集群、Producer生产者集群、Consumer消费者集群和若干Broker（RocketMQ进程）组成，它的架构原理是这样的： Broker在启动的时候去向所有的NameServer注册，并保持长连接，每30s发送一次心跳 Producer在发送消息的时候从NameServer获取Broker服务器地址，根据负载均衡算法选择一台服务器来发送消息 Conusmer消费消息的时候同样从NameServer获取Broker地址，然后主动拉取消息来消费 7、为什么RocketMQ不使用Zookeeper作为注册中心呢？我认为有以下几个点是不使用zookeeper的原因： 根据CAP理论，同时最多只能满足两个点，而zookeeper满足的是CP，也就是说zookeeper并不能保证服务的可用性，zookeeper在进行选举的时候，整个选举的时间太长，期间整个集群都处于不可用的状态，而这对于一个注册中心来说肯定是不能接受的，作为服务发现来说就应该是为可用性而设计。 基于性能的考虑，NameServer本身的实现非常轻量，而且可以通过增加机器的方式水平扩展，增加集群的抗压能力，而zookeeper的写是不可扩展的，而zookeeper要解决这个问题只能通过划分领域，划分多个zookeeper集群来解决，首先操作起来太复杂，其次这样还是又违反了CAP中的A的设计，导致服务之间是不连通的。 持久化的机制来带的问题，ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，而对于一个简单的服务发现的场景来说，这其实没有太大的必要，这个实现方案太重了。而且本身存储的数据应该是高度定制化的。 消息发送应该弱依赖注册中心，而RocketMQ的设计理念也正是基于此，生产者在第一次发送消息的时候从NameServer获取到Broker地址后缓存到本地，如果NameServer整个集群不可用，短时间内对于生产者和消费者并不会产生太大影响。 8、那Broker是怎么保存数据的呢？RocketMQ主要的存储文件包括commitlog文件、consumequeue文件、indexfile文件。 Broker在收到消息之后，会把消息保存到commitlog的文件当中，而同时在分布式的存储当中，每个broker都会保存一部分topic的数据，同时，每个topic对应的messagequeue下都会生成consumequeue文件用于保存commitlog的物理位置偏移量offset，indexfile中会保存key和offset的对应关系。 CommitLog文件保存于${Rocket_Home}&#x2F;store&#x2F;commitlog目录中，从图中我们可以明显看出来文件名的偏移量，每个文件默认1G，写满后自动生成一个新的文件。 由于同一个topic的消息并不是连续的存储在commitlog中，消费者如果直接从commitlog获取消息效率非常低，所以通过consumequeue保存commitlog中消息的偏移量的物理地址，这样消费者在消费的时候先从consumequeue中根据偏移量定位到具体的commitlog物理文件，然后根据一定的规则（offset和文件大小取模）在commitlog中快速定位。 9、Master和Slave之间是怎么同步数据的呢？而消息在master和slave之间的同步是根据raft协议来进行的： 在broker收到消息后，会被标记为uncommitted状态 然后会把消息发送给所有的slave slave在收到消息之后返回ack响应给master master在收到超过半数的ack之后，把消息标记为committed 发送committed消息给所有slave，slave也修改状态为committed 10、你知道RocketMQ为什么速度快吗？是因为使用了顺序存储、Page Cache和异步刷盘。 我们在写入commitlog的时候是顺序写入的，这样比随机写入的性能就会提高很多 写入commitlog的时候并不是直接写入磁盘，而是先写入操作系统的PageCache 最后由操作系统异步将缓存中的数据刷到磁盘 11、什么是事务、半事务消息？怎么实现的？事务消息就是MQ提供的类似XA的分布式事务能力，通过事务消息可以达到分布式事务的最终一致性。 半事务消息就是MQ收到了生产者的消息，但是没有收到二次确认，不能投递的消息。 实现原理如下： 生产者先发送一条半事务消息到MQ MQ收到消息后返回ack确认 生产者开始执行本地事务 如果事务执行成功发送commit到MQ，失败发送rollback 如果MQ长时间未收到生产者的二次确认commit或者rollback，MQ对生产者发起消息回查 生产者查询事务执行最终状态 根据查询事务状态再次提交二次确认 最终，如果MQ收到二次确认commit，就可以把消息投递给消费者，反之如果是rollback，消息会保存下来并且在3天后被删除。"},{"title":"正则表达式入门","path":"/wiki/interview/正则表达式入门.html","content":"入门手册：正则表达式入门 | MRCODE-BOOK 在线测试：regex101: build, test, and debug regex 1、元字符 2、量词与贪婪 正则中量词默认是贪婪匹配， 如果想要进行非贪婪匹配需要在 量词后面加 上问号（?）。 贪婪和非贪婪匹配都可能会进行回溯，独占模式也是进行贪婪匹配，但不进行回溯，因此在一些场景下，可以提高匹配的效率，具体能不能用独占模式需要看使用的编程语言的类库的支持情况，以及独占模式能不能满足需求。 3、分组与引用 4、匹配模式"},{"title":"计算机网络&操作系统","path":"/wiki/interview/计算机网络&操作系统.html","content":"在线文章计算机网络一、图解网络 二、计算机网络面试题 操作系统一、图解系统 二、操作系统面试题"},{"title":"秒杀技术总结","path":"/wiki/interview/秒杀技术总结.html","content":"什么是秒杀？在电商领域，存在着典型的秒杀业务场景，那何谓秒杀场景呢。简单的来说就是一件商品的购买人数远远大于这件商品的库存，而且这件商品在很短的时间内就会被抢购一空。 比如每年的618、双11大促，小米新品促销等业务场景，就是典型的秒杀业务场景。 秒杀有什么特点？对于秒杀系统来说，我们可以从业务和技术两个角度来阐述其自身存在的一些特点。 业务特点：（1）限时、限量、限价 （2）活动预热 （3）持续时间短 技术特点：（1）瞬时并发量非常高 （2）读多写少 （3）流程简单 秒杀的技术挑战1: 对现有业务冲击秒杀活动具有时间短，并发访问量大的特点，如果和网站原有应用部署在一起，必然会对现有业务造成冲击，稍有不慎可能导致整个网站瘫痪。 解决思路：服务隔离，业务隔离，数据库隔离。秒杀服务单独部署，秒杀涉及的数据库也要单独部署，以防止影响其他业务 2：高并发的应用，对数据库压力大秒杀主流程，查商品详情 -&gt; 查库存 -&gt; 请求扣减库存 -&gt; 下单。 其中查商品详情页的在秒杀开始前的几分钟，用户会疯狂的刷新详情页，导致查询的请求量巨大，如果处理不当，有可能导致秒杀开始前，应用就瘫痪。 解决思路：上层应用缓存商品详情，最好的方法是静态化页面，定时推送到CDN上，商品详情页直接访问CDN。 3：商品超卖秒杀开始时，同时会有很多用户请求秒杀接口，如果查询库存 -&gt; 扣减库存 这两步操作如果不能保证原子操作。一定会导致超卖问题，造成资损。 解决思路：秒杀开始前，初始化库存到缓存中，在缓存中预减库存，下单失败再归还库存。 秒杀架构原则1：尽量把请求拦截在上游，分散用户的并发时间。第一层：客户端拦截1： 输入验证码，图形验证码(12306) 2: 答题 3：客户端限制用户点击秒杀按钮的频率，强制用户等待。 第二层：后端拦截1：通过在缓存中预减库存（秒杀库存远小于请求秒杀的用户数），能拦截掉大量无效请求。 2：防刷控制 3：使用metaq 消峰，把大量的并发请求转换为排队请求。 2：把数据放在离用户最近的地方1：使用CDN静态化秒杀商品数据 2：优先使用本地缓存 秒杀后端架构设计1：全异步方案： 1：用户发起秒杀请求用户发起秒杀请求后，商城服务会经过如下业务流程。 （1）检测验证码是否正确用户发起秒杀请求时，会将验证码一同发送过来，系统会检验验证码是否有效，并且是否正确。 （2）是否限流系统会对用户的请求进行是否限流的判断，这里，我们可以通过判断消息队列的长度来进行判断。因为我们将用户的请求放在了消息队列中，消息队列中堆积的是用户的请求，我们可以根据当前消息队列中存在的待处理的请求数量来判断是否需要对用户的请求进行限流处理。 例如，在秒杀活动中，我们出售1000件商品，此时在消息队列中存在1000个请求，如果后续仍然有用户发起秒杀请求，则后续的请求我们可以不再处理，直接向用户返回商品已售完的提示。 所以，使用限流后，我们可以更快的处理用户的请求和释放连接的资源。这个地方存在一些问题：如果多个秒杀商品共用一个消息队列主题，就无法使用判断队列长度的方式来限流。 （3）发送MQ用户的秒杀请求通过前面的验证后，我们就可以将用户的请求参数等信息发送到MQ中进行异步处理，同时，向用户响应结果信息。在商城服务中，会有专门的异步任务处理模块来消费消息队列中的请求，并处理后续的异步流程。 在用户发起秒杀请求时，异步下单流程比同步下单流程处理的业务操作更少，它将后续的操作通过MQ发送给异步处理模块进行处理，并迅速向用户返回响应结果，释放请求连接。 2：异步处理我们可以将下单流程的如下操作进行异步处理。 （1）判断活动是否已经结束 （2）判断本次请求是否处于系统黑名单，为了防止电商领域同行的恶意竞争可以为系统增加黑名单机制，将恶意的请求放入系统的黑名单中。可以使用拦截器统计访问频次来实现。 （3）扣减缓存中的秒杀商品的库存数量。 （4）生成秒杀Token，这个Token是绑定当前用户和当前秒杀活动的，只有生成了秒杀Token的请求才有资格进行秒杀活动。 这里我们引入了异步处理机制，在异步处理中，系统使用多少资源，分配多少线程来处理相应的任务，是可以进行控制的。 3：轮询结果这里，可以采取客户端短轮询查询是否获得秒杀资格的方案。例如，客户端可以每隔3秒钟轮询请求服务器，查询是否获得秒杀资格，这里，我们在服务器的处理就是判断当前用户是否存在秒杀Token，如果服务器为当前用户生成了秒杀Token，则当前用户存在秒杀资格。否则继续轮询查询，直到超时或者服务器返回商品已售完或者无秒杀资格等信息为止。 采用短轮询查询秒杀结果时，在页面上我们同样可以提示用户排队处理中，但是此时客户端会每隔几秒轮询服务器查询秒杀资格的状态，相比于同步下单流程来说，无需长时间占用请求连接。 此时，可能会有网友会问：采用短轮询查询的方式，会不会存在直到超时也查询不到是否具有秒杀资格的状态呢？答案是：有可能！ 这里我们试想一下秒杀的真实场景，商家参加秒杀活动本质上不是为了赚钱，而是提升商品的销量和商家的知名度，吸引更多的用户来买自己的商品。所以，我们不必保证用户能够100%的查询到是否具有秒杀资格的状态。 4：秒杀结算（1）验证下单Token客户端提交秒杀结算时，会将秒杀Token一同提交到服务器，商城服务会验证当前的秒杀Token是否有效。 （2）加入秒杀购物车商城服务在验证秒杀Token合法并有效后，会将用户秒杀的商品添加到秒杀购物车。 5：提交订单（1）订单入库将用户提交的订单信息保存到数据库中。 （2）删除Token秒杀商品订单入库成功后，删除秒杀Token。 这里大家可以思考一个问题：我们为什么只在异步下单流程的粉色部分采用异步处理，而没有在其他部分采取异步削峰和填谷的措施呢？ 这是因为在异步下单流程的设计中，无论是在产品设计上还是在接口设计上，我们在用户发起秒杀请求阶段对用户的请求进行了限流操作，可以说，系统的限流操作是非常前置的。在用户发起秒杀请求时进行了限流，系统的高峰流量已经被平滑解决了，再往后走，其实系统的并发量和系统流量并不是非常高了。 所以，网上很多的文章和帖子中在介绍秒杀系统时，说是在下单时使用异步削峰来进行一些限流操作，那都是在扯淡！ 因为下单操作在整个秒杀系统的流程中属于比较靠后的操作了，限流操作一定要前置处理，在秒杀业务后面的流程中做限流操作是没啥卵用的。 异步方案缺点：1：token的保存，所有的用户都需要轮询保存token的缓存，如果库存过大，本地缓存可能存不下甚至还需要做本地缓存同步，如果都查询远程缓存(redis)，也会造成热点key问题。 2：没有秒杀成功的用户需要等待轮询超时 2：同步方案 同步方案和异步方案的区别点就是不再使用MQ来做限流降级，而是使用本地缓存实现库存扣减，本地缓存扣减成功才扣减远程缓存(redis) 只有两个缓存都扣减成功才算真正扣减库存成功，才给用户返回秒杀成功的token。 具体的扣库存原理如下（拿买火车票举例）：本地扣库存。我们把一定的库存量分配到本地机器，直接在内存中减库存，然后按照之前的逻辑异步创建订单。改进过之后的单机系统是这样的: 这样就避免了对数据库频繁的IO操作，只在内存中做运算，极大的提高了单机抗并发的能力。但是百万的用户请求量单机是无论如何也抗不住的，虽然nginx处理网络请求使用epoll模型，c10k的问题在业界早已得到了解决。但是linux系统下，一切资源皆文件，网络请求也是这样，大量的文件描述符会使操作系统瞬间失去响应。我们可以把秒杀服务集群化，比如增加100台服务器，这样单机所承受的并发量就小了很多。然后我们每台机器本地库存100张火车票，100台服务器上的总库存还是1万，这样保证了库存订单不超卖,下面是我们描述的集群架构: 问题接踵而至，在高并发情况下，现在我们还无法保证系统的高可用，假如这100台服务器上有两三台机器因为扛不住并发的流量或者其他的原因宕机了。那么这些服务器上的订单就卖不出去了，这就造成了订单的少卖。要解决这个问题，我们需要对总订单量做统一的管理，这就是接下来的容错方案。服务器不仅要在本地减库存，另外要远程统一减库存。有了远程统一减库存的操作，我们就可以根据机器负载情况，为每台机器分配一些多余的“buffer库存”用来防止机器中有机器宕机的情况。我们结合下面架构图具体分析一下: 我们采用Redis存储统一库存，因为Redis的性能非常高，号称单机QPS能抗10W的并发。在本地减库存以后，如果本地有订单，我们再去请求redis远程减库存，本地减库存和远程减库存都成功了，才返回给用户抢票成功的提示,这样也能有效的保证订单不会超卖。当机器中有机器宕机时，因为每个机器上有预留的buffer余票，所以宕机机器上的余票依然能够在其他机器上得到弥补，保证了不少卖。buffer余票设置多少合适呢，理论上buffer设置的越多，系统容忍宕机的机器数量就越多，但是buffer设置的太大也会对redis造成一定的影响。虽然redis内存数据库抗并发能力非常高，请求依然会走一次网络IO,其实抢票过程中对redis的请求次数是本地库存和buffer库存的总量，因为当本地库存不足时，系统直接返回用户“已售罄”的信息提示，就不会再走统一扣库存的逻辑，这在一定程度上也避免了巨大的网络请求量把redis压跨，所以buffer值设置多少，需要架构师对系统的负载能力做认真的考量。 扣库存的优化方案：无论是同步方案还是异步方案，都存在库存过大时，远程缓存热点key的问题。 如何解决热点key的问题，利用redis的集群功能，进一步提高秒杀的并发能力。 分桶：如果库存过大，可以把大库存分成多个小份放在不同的redis桶中。 例如，原来的秒杀商品的id为10001，库存为1000件，在Redis中的存储为(10001, 1000)，我们将原有的库存分割为5份，则每份的库存为200件，此时，我们在Redia中存储的信息为(10001_0, 200)，(10001_1, 200)，(10001_2, 200)，(10001_3, 200)，(10001_4, 200)。 此时，我们将库存进行分割后，每个分割后的库存使用商品id加上一个数字标识来存储，这样，在对存储商品库存的每个Key进行Hash运算时，得出的Hash结果是不同的，这就说明，存储商品库存的Key有很大概率不在Redis的同一个槽位中，这就能够提升Redis处理请求的性能和并发量。 分割库存后，我们还需要在Redis中存储一份商品id和分割库存后的Key的映射关系，此时映射关系的Key为商品的id，也就是10001，Value为分割库存后存储库存信息的Key，也就是10001_0，10001_1，10001_2，10001_3，10001_4。在Redis中我们可以使用List来存储这些值。 优酷体育能量商城的秒杀技术方案优酷体育的方案采用了全异步 + 库存分桶方案，具体方案如下： 缓存模块商品详情1s本地缓存。 预计 数据库 QPS 200 5ms 商城秒杀列表1s本地缓存 （倒计时不缓存）。 数据库 QPS 100 10ms 商城商品列表本地缓存5秒。 限流模块： 库存控制模块：redis exincrBy 支持最大值限制： 每个桶最大能发放10000 ， 回滚库存时： 增加能发放的最大值。 支持秒杀过程中，运营新增库存，支持本地桶上线下线控制 桶的上线下线逻辑： 整体链路流程秒杀初始化链路流程： 秒杀流程："},{"title":"面试题知识手册","path":"/wiki/interview/面试题知识手册.html","content":"在线知识库一、《服务端开发与面试知识手册》 二、小林coding (图解计算机基础系列) 三、面渣逆袭 | 二哥的Java进阶之路 四、Java后端面试题大全-开发者客栈 五、Java 全栈知识体系 站内汇总分类：Java面试题精选"}]